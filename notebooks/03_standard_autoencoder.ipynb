{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kate-simonova/cancer-subtyping/blob/main/03_standard_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIDb772jundM"
      },
      "source": [
        "# Autoencoder implemetation\n",
        "\n",
        "The goal of this notebook is to implement a standard autoencoder and apply clustering on this latent space.\n",
        "\n",
        "**Three metrics were selected for measuring the quality of the model:**\n",
        "\n",
        "* Sihouette score - the better, the value is closer to 1\n",
        "* Davins-Bouldin score - the better, the value is closer to 0\n",
        "* Adjusted Rand Index - the better, the value is closer to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LbvVJcd1SV"
      },
      "source": [
        "## Data Loading and installation of packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyxZeQF-B4I-"
      },
      "outputs": [],
      "source": [
        "!pip install -U kaleido\n",
        "!pip install umap-learn\n",
        "#!pip install keras==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVJ81ITZtoC5",
        "outputId": "d4998388-e90c-4f3e-aa0c-1110c1b8d4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# importing necessary packages\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "path = \"/content/drive/MyDrive/MY_DATA/merged/\"\n",
        "\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import plotly.express as px\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Reshape, Lambda, Dropout, BatchNormalization, Dense, Input, concatenate\n",
        "from keras import regularizers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "import gc\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
        "\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras import initializers\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.metrics import binary_crossentropy\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.optimizers import SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6grQ4u-NoFl",
        "outputId": "eae55eea-9311-4ad0-afa2-350006e8f247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "print(cores)\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdQLFoEhFi5k"
      },
      "source": [
        "# Loading and preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAZPe96J5pQr",
        "outputId": "6512a795-8106-4e24-9b4c-9b6b4edf46c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2156, 22596)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "test_df = pd.read_csv(f\"{path}/split_circle/X_test_A_merged_CRC_BRCA.csv.tar.gz\", compression = \"gzip\", index_col=0).T\n",
        "test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yqazWaT5svk",
        "outputId": "1636b3a6-bd65-4a2a-b674-97bb139e68ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2156, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "y_test = pd.read_csv(f\"{path}/split_circle/y_test_A_merged_CRC_BRCA.csv.tar.gz\", compression = \"gzip\", index_col=0)\n",
        "#y_test = y_test.reset_index(drop=True)\n",
        "#y_test.index = test_df.index\n",
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCV-kkjl-uTn",
        "outputId": "1615e573-7a78-43b1-ca65-dd60b0595939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11544, 22596)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_df = pd.read_csv(f\"{path}/split_circle/X_train_A_merged_CRC_BRCA.csv.tar.gz\", compression=\"gzip\", index_col=0).T\n",
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmFoBWFPFo3j",
        "outputId": "bffa00bf-9db3-406c-808b-2224b9f4e30f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11544, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "y_train = pd.read_csv(f\"{path}/split_circle/y_train_A_merged_CRC_BRCA.csv.tar.gz\", compression = \"gzip\", index_col=0)\n",
        "#y_train = y_train.reset_index(drop=True)\n",
        "#y_train.index = train_df.index\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9QDYS2N9k3t"
      },
      "source": [
        "# Dimensionality reduction with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bJGk-WSG7bj",
        "outputId": "a5ca6e1f-aedd-426e-e3cd-83e60896cc5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11544, 4557)\n",
            "(2156, 4557)\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(0.95).fit(train_df)\n",
        "x_train = pca.transform(train_df)\n",
        "x_test = pca.transform(test_df)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWRX-5srEdV6"
      },
      "outputs": [],
      "source": [
        "x_test = pd.DataFrame(x_test, index=test_df.index)\n",
        "x_train = pd.DataFrame(x_train, index=train_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBbx7V6zMKmt",
        "outputId": "6c60a2b5-35f5-4012-fa4a-07ff60d46d28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2156, 4557)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEki3ZViL6es"
      },
      "outputs": [],
      "source": [
        "x_test.columns = [\"PC\"+ str(i+1) for i in range(4557)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMjyG8N61haq"
      },
      "outputs": [],
      "source": [
        "X_tr, X_val, y_tr, y_val = train_test_split(x_train, y_train[\"Label\"], test_size=0.2, random_state=42, stratify=y_train[\"Label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlyhIq47F3-0",
        "outputId": "fa8264c3-a72a-482f-bcdd-9cc9f0706866"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9235, 4557)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_tr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk8KHHqU_OLE"
      },
      "source": [
        "## Oversampling and synthetic data generation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj8ztQiSFmK6",
        "outputId": "13902489-47c7-451c-deff-7baae8c715f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Counter({'Luminal A': 3405, 'Luminal B': 1789, 'Basal-like': 1323, 'Her2': 861, 'CMS2': 620, 'Normal-like': 393, 'CMS4': 301, 'CMS1': 229, 'CMS3': 185, 'Claudin-low': 129})\n",
            "After Counter({9: 3446, 3: 3417, 5: 3416, 1: 3410, 7: 3405, 2: 3394, 4: 3390, 6: 3386, 0: 3341, 8: 3314})\n"
          ]
        }
      ],
      "source": [
        "mapp = LabelEncoder()\n",
        "\n",
        "y_train_adasyn = mapp.fit_transform(y_tr).ravel()\n",
        "\n",
        "before_counter = Counter(y_tr)\n",
        "print('Before', before_counter)\n",
        "\n",
        "ada = ADASYN(random_state=0, sampling_strategy='all')\n",
        "x_train_ada, y_train_ada = ada.fit_resample(X_tr, y_train_adasyn)\n",
        "\n",
        "after_counter = Counter(y_train_ada)\n",
        "print('After', after_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUU5P76sGWuL",
        "outputId": "cdf53a98-4c0f-4cf7-8a19-313c4642af1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Basal-like': 0, 'CMS1': 1, 'CMS2': 2, 'CMS3': 3, 'CMS4': 4, 'Claudin-low': 5, 'Her2': 6, 'Luminal A': 7, 'Luminal B': 8, 'Normal-like': 9}\n"
          ]
        }
      ],
      "source": [
        "mapping = dict(zip(mapp.classes_, mapp.transform(mapp.classes_)))\n",
        "print(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjxB0z5pXzdk"
      },
      "outputs": [],
      "source": [
        "x_train_ada = pd.DataFrame(x_train_ada)\n",
        "y_train_ada = pd.DataFrame(y_train_ada)\n",
        "y_train_ada.columns = [\"Label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY-hKMA6WkGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "4d8ca5b0-dc85-4eba-b0d7-b7fb281777c7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6b630a4d61c9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# temporary saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train_ada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path}/pickle/x_train_ada_circle.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train_ada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path}/pickle/y_train_ada_circle.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path}/pickle/X_val_circle.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train_ada' is not defined"
          ]
        }
      ],
      "source": [
        "# temporary saving\n",
        "x_train_ada.to_pickle(f\"{path}/pickle/x_train_ada_circle.pkl\")\n",
        "y_train_ada.to_pickle(f\"{path}/pickle/y_train_ada_circle.pkl\")\n",
        "\n",
        "X_val.to_pickle(f\"{path}/pickle/X_val_circle.pkl\")\n",
        "y_val.to_pickle(f\"{path}/pickle/y_val_circle.pkl\")\n",
        "\n",
        "x_test.to_pickle(f\"{path}/pickle/x_test_circle.pkl\")\n",
        "y_test.to_pickle(f\"{path}/pickle/y_test_circle.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCiEKz4egAkA"
      },
      "outputs": [],
      "source": [
        "x_train.columns = [\"PC\" + str(i+1) for i in range(4557)]\n",
        "x_train_ada.columns = [\"PC\" + str(i+1) for i in range(4557)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmPbcqSKfpdo"
      },
      "outputs": [],
      "source": [
        "y_train_ada[\"New\"] = [1 if x in range(12203) else 0 for x in y_train_ada.index]\n",
        "\n",
        "y_train_adasyn = pd.DataFrame(y_train_adasyn)\n",
        "y_train_adasyn.columns = [\"Label\"]\n",
        "y_train_adasyn[\"New\"] = 1\n",
        "x_train.index = y_train_adasyn.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6pX-5N_2HkA"
      },
      "outputs": [],
      "source": [
        "# Implementation of graph before/after oversampling \n",
        "f, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "ax1.scatter(x_train[\"PC1\"], x_train[\"PC2\"], label=\"Original points\",\n",
        "                 alpha=0.5, s=2)\n",
        "ax1.set_title('Original dataset')\n",
        "\n",
        "c0 = ax2.scatter(x_train_ada[y_train_ada[\"New\"] == 1][\"PC1\"], x_train_ada[y_train_ada[\"New\"] == 1][\"PC2\"],\n",
        "            label=\"New points\", alpha=.5, s=2)\n",
        "c1 = ax2.scatter(x_train_ada[y_train_ada[\"New\"] == 0][\"PC1\"], x_train_ada[y_train_ada[\"New\"] == 0][\"PC2\"],\n",
        "            label=\"Original points\", alpha=.5, s=2)\n",
        "\n",
        "ax2.set_title('Oversampled dataset')\n",
        "\n",
        "# make nice plotting\n",
        "for ax in (ax1, ax2):\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.get_xaxis().tick_bottom()\n",
        "    ax.get_yaxis().tick_left()\n",
        "    ax.spines['left'].set_position(('outward', 10))\n",
        "    ax.spines['bottom'].set_position(('outward', 10))\n",
        "    ax.set_xlim([-4, 6])\n",
        "    ax.set_ylim([-4, 4])\n",
        "    ax.set_ylabel(\"PC2\")\n",
        "    ax.set_xlabel(\"PC1\")\n",
        "\n",
        "plt.figlegend((c0, c1), ('Original points', 'New points'), loc='lower center',\n",
        "              ncol=2, labelspacing=0., fontsize=8)\n",
        "plt.tight_layout(pad=2)\n",
        "plt.savefig(f\"{path}/img/before_after_oversampling_split_circle_new_old.pdf\", format=\"pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLbapQ9fuKm8"
      },
      "source": [
        "## Autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is used for explaration of the model performance in one iteration."
      ],
      "metadata": {
        "id": "QXlZ91m2N4XB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMmdBDsjSgaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1b7227-69fa-48f5-c4ae-8e7bff7b957d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33919, 4557)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "y_train_ada = pd.read_pickle(f\"{path}/pickle/y_train_ada_circle.pkl\")\n",
        "x_train_ada = pd.read_pickle(f\"{path}/pickle/x_train_ada_circle.pkl\")\n",
        "x_test = pd.read_pickle(f\"{path}/pickle/x_test_circle.pkl\")\n",
        "y_test = pd.read_pickle(f\"{path}/pickle/y_test_circle.pkl\")\n",
        "X_val = pd.read_pickle(f\"{path}/pickle/X_val_circle.pkl\")\n",
        "y_val = pd.read_pickle(f\"{path}/pickle/y_val_circle.pkl\")\n",
        "x_train_ada.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u7SmqgYHTXY"
      },
      "outputs": [],
      "source": [
        "from keras.layers.activation.leaky_relu import LeakyReLU\n",
        "\n",
        "def autoencoder(dims, act='tanh', init='glorot_uniform'):\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    input_data = Input(shape=(dims[0],), name='input')\n",
        "    x = input_data\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        x = Dense(dims[i + 1], kernel_initializer=init, name='encoder_%d' % i)(x) # Dense act and kernel_initializer\n",
        "        x = BatchNormalization(name='batch_norm_%d' % i)(x) # Add batch normalization layer after dense layer\n",
        "        #x = LeakyReLU()(x)\n",
        "   \n",
        "    # hidden layer\n",
        "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
        "\n",
        "    x = encoded\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        x = Dense(dims[i], kernel_initializer=init, name='decoder_%d' % i)(x) # activation=act, kernel_initializer=init Dense\n",
        "        x = BatchNormalization(name='batch_norm_%d' % (n_stacks-i))(x) # Add batch normalization layer after dense layer\n",
        "        #x = LeakyReLU()(x)\n",
        "\n",
        "    # output\n",
        "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
        "    decoded = x\n",
        "    return Model(inputs=input_data, outputs=decoded, name='AE'), Model(inputs=input_data, outputs=encoded, name='encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWhA0KhKHgex"
      },
      "outputs": [],
      "source": [
        "dims = [4557, 1024, 64]\n",
        "\n",
        "init = VarianceScaling(scale=1. / 2., mode='fan_avg',\n",
        "                      distribution='uniform',\n",
        "                      seed=0)\n",
        "\n",
        "epochs = 1000\n",
        "batch_size = 1024\n",
        "\n",
        "optimizer = SGD(learning_rate=0.2, decay=1e-5, momentum=0.9, nesterov=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xja5k0Uy1bSs",
        "outputId": "c31dc221-e43b-4f0d-d5bb-dc9a4920ff18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "34/34 [==============================] - 2s 46ms/step - loss: 0.5058 - val_loss: 0.5433\n",
            "Epoch 2/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.3650 - val_loss: 0.4802\n",
            "Epoch 3/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.3381 - val_loss: 0.4598\n",
            "Epoch 4/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.3229 - val_loss: 0.4464\n",
            "Epoch 5/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.3103 - val_loss: 0.4358\n",
            "Epoch 6/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.3006 - val_loss: 0.4275\n",
            "Epoch 7/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2926 - val_loss: 0.4203\n",
            "Epoch 8/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2854 - val_loss: 0.4136\n",
            "Epoch 9/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2789 - val_loss: 0.4070\n",
            "Epoch 10/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2735 - val_loss: 0.4015\n",
            "Epoch 11/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2688 - val_loss: 0.3959\n",
            "Epoch 12/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2647 - val_loss: 0.3914\n",
            "Epoch 13/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2612 - val_loss: 0.3871\n",
            "Epoch 14/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2580 - val_loss: 0.3836\n",
            "Epoch 15/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2551 - val_loss: 0.3801\n",
            "Epoch 16/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2524 - val_loss: 0.3770\n",
            "Epoch 17/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2500 - val_loss: 0.3742\n",
            "Epoch 18/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2477 - val_loss: 0.3714\n",
            "Epoch 19/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2456 - val_loss: 0.3689\n",
            "Epoch 20/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2436 - val_loss: 0.3666\n",
            "Epoch 21/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2419 - val_loss: 0.3644\n",
            "Epoch 22/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2403 - val_loss: 0.3627\n",
            "Epoch 23/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2389 - val_loss: 0.3610\n",
            "Epoch 24/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2374 - val_loss: 0.3594\n",
            "Epoch 25/1000\n",
            "34/34 [==============================] - 1s 35ms/step - loss: 0.2362 - val_loss: 0.3579\n",
            "Epoch 26/1000\n",
            "34/34 [==============================] - 2s 45ms/step - loss: 0.2351 - val_loss: 0.3567\n",
            "Epoch 27/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2341 - val_loss: 0.3555\n",
            "Epoch 28/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2331 - val_loss: 0.3544\n",
            "Epoch 29/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2323 - val_loss: 0.3534\n",
            "Epoch 30/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2315 - val_loss: 0.3525\n",
            "Epoch 31/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2307 - val_loss: 0.3517\n",
            "Epoch 32/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2301 - val_loss: 0.3509\n",
            "Epoch 33/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2295 - val_loss: 0.3502\n",
            "Epoch 34/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2290 - val_loss: 0.3496\n",
            "Epoch 35/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2284 - val_loss: 0.3490\n",
            "Epoch 36/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2280 - val_loss: 0.3485\n",
            "Epoch 37/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2275 - val_loss: 0.3480\n",
            "Epoch 38/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2272 - val_loss: 0.3476\n",
            "Epoch 39/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2269 - val_loss: 0.3472\n",
            "Epoch 40/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2266 - val_loss: 0.3468\n",
            "Epoch 41/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2263 - val_loss: 0.3465\n",
            "Epoch 42/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2262 - val_loss: 0.3462\n",
            "Epoch 43/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2259 - val_loss: 0.3460\n",
            "Epoch 44/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2257 - val_loss: 0.3457\n",
            "Epoch 45/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2255 - val_loss: 0.3454\n",
            "Epoch 46/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2253 - val_loss: 0.3453\n",
            "Epoch 47/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2252 - val_loss: 0.3450\n",
            "Epoch 48/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2252 - val_loss: 0.3449\n",
            "Epoch 49/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2249 - val_loss: 0.3448\n",
            "Epoch 50/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2249 - val_loss: 0.3446\n",
            "Epoch 51/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2248 - val_loss: 0.3444\n",
            "Epoch 52/1000\n",
            "34/34 [==============================] - 2s 49ms/step - loss: 0.2246 - val_loss: 0.3443\n",
            "Epoch 53/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2246 - val_loss: 0.3441\n",
            "Epoch 54/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2245 - val_loss: 0.3440\n",
            "Epoch 55/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2244 - val_loss: 0.3440\n",
            "Epoch 56/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2243 - val_loss: 0.3438\n",
            "Epoch 57/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2243 - val_loss: 0.3437\n",
            "Epoch 58/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2243 - val_loss: 0.3436\n",
            "Epoch 59/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2242 - val_loss: 0.3435\n",
            "Epoch 60/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2241 - val_loss: 0.3434\n",
            "Epoch 61/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2241 - val_loss: 0.3433\n",
            "Epoch 62/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2240 - val_loss: 0.3433\n",
            "Epoch 63/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2240 - val_loss: 0.3432\n",
            "Epoch 64/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2240 - val_loss: 0.3431\n",
            "Epoch 65/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2239 - val_loss: 0.3430\n",
            "Epoch 66/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2239 - val_loss: 0.3429\n",
            "Epoch 67/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2238 - val_loss: 0.3429\n",
            "Epoch 68/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2238 - val_loss: 0.3428\n",
            "Epoch 69/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2237 - val_loss: 0.3428\n",
            "Epoch 70/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2237 - val_loss: 0.3427\n",
            "Epoch 71/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2237 - val_loss: 0.3427\n",
            "Epoch 72/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2236 - val_loss: 0.3427\n",
            "Epoch 73/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2236 - val_loss: 0.3426\n",
            "Epoch 74/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2236 - val_loss: 0.3425\n",
            "Epoch 75/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2235 - val_loss: 0.3425\n",
            "Epoch 76/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2235 - val_loss: 0.3424\n",
            "Epoch 77/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2235 - val_loss: 0.3424\n",
            "Epoch 78/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2235 - val_loss: 0.3423\n",
            "Epoch 79/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2235 - val_loss: 0.3422\n",
            "Epoch 80/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2235 - val_loss: 0.3422\n",
            "Epoch 81/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2235 - val_loss: 0.3422\n",
            "Epoch 82/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2234 - val_loss: 0.3422\n",
            "Epoch 83/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2233 - val_loss: 0.3421\n",
            "Epoch 84/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2234 - val_loss: 0.3421\n",
            "Epoch 85/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2233 - val_loss: 0.3421\n",
            "Epoch 86/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2233 - val_loss: 0.3420\n",
            "Epoch 87/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2234 - val_loss: 0.3420\n",
            "Epoch 88/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2234 - val_loss: 0.3420\n",
            "Epoch 89/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2233 - val_loss: 0.3419\n",
            "Epoch 90/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2232 - val_loss: 0.3419\n",
            "Epoch 91/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2233 - val_loss: 0.3418\n",
            "Epoch 92/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2232 - val_loss: 0.3418\n",
            "Epoch 93/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2233 - val_loss: 0.3418\n",
            "Epoch 94/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2232 - val_loss: 0.3418\n",
            "Epoch 95/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2232 - val_loss: 0.3417\n",
            "Epoch 96/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2232 - val_loss: 0.3417\n",
            "Epoch 97/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2232 - val_loss: 0.3417\n",
            "Epoch 98/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2232 - val_loss: 0.3416\n",
            "Epoch 99/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2232 - val_loss: 0.3416\n",
            "Epoch 100/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2232 - val_loss: 0.3416\n",
            "Epoch 101/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2232 - val_loss: 0.3416\n",
            "Epoch 102/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2231 - val_loss: 0.3416\n",
            "Epoch 103/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2231 - val_loss: 0.3416\n",
            "Epoch 104/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2232 - val_loss: 0.3416\n",
            "Epoch 105/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2231 - val_loss: 0.3415\n",
            "Epoch 106/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2231 - val_loss: 0.3415\n",
            "Epoch 107/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2230 - val_loss: 0.3415\n",
            "Epoch 108/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2230 - val_loss: 0.3415\n",
            "Epoch 109/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2230 - val_loss: 0.3415\n",
            "Epoch 110/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2231 - val_loss: 0.3414\n",
            "Epoch 111/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2231 - val_loss: 0.3414\n",
            "Epoch 112/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2230 - val_loss: 0.3414\n",
            "Epoch 113/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2231 - val_loss: 0.3414\n",
            "Epoch 114/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2230 - val_loss: 0.3414\n",
            "Epoch 115/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2230 - val_loss: 0.3413\n",
            "Epoch 116/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3413\n",
            "Epoch 117/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2230 - val_loss: 0.3413\n",
            "Epoch 118/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2230 - val_loss: 0.3413\n",
            "Epoch 119/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2230 - val_loss: 0.3413\n",
            "Epoch 120/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2229 - val_loss: 0.3413\n",
            "Epoch 121/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2230 - val_loss: 0.3413\n",
            "Epoch 122/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2230 - val_loss: 0.3413\n",
            "Epoch 123/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2230 - val_loss: 0.3412\n",
            "Epoch 124/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3412\n",
            "Epoch 125/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3412\n",
            "Epoch 126/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2230 - val_loss: 0.3412\n",
            "Epoch 127/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2229 - val_loss: 0.3412\n",
            "Epoch 128/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 129/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 130/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 131/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 132/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2230 - val_loss: 0.3411\n",
            "Epoch 133/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 134/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 135/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 136/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3410\n",
            "Epoch 137/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3410\n",
            "Epoch 138/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2229 - val_loss: 0.3411\n",
            "Epoch 139/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2228 - val_loss: 0.3411\n",
            "Epoch 140/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2229 - val_loss: 0.3410\n",
            "Epoch 141/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3410\n",
            "Epoch 142/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3410\n",
            "Epoch 143/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3410\n",
            "Epoch 144/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2229 - val_loss: 0.3410\n",
            "Epoch 145/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 146/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2229 - val_loss: 0.3410\n",
            "Epoch 147/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3409\n",
            "Epoch 148/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3409\n",
            "Epoch 149/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 150/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 151/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 152/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 153/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 154/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3409\n",
            "Epoch 155/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 156/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2229 - val_loss: 0.3409\n",
            "Epoch 157/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2228 - val_loss: 0.3409\n",
            "Epoch 158/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 159/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2227 - val_loss: 0.3409\n",
            "Epoch 160/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 161/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 162/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2229 - val_loss: 0.3408\n",
            "Epoch 163/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 164/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 165/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 166/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 167/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 168/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 169/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 170/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2227 - val_loss: 0.3408\n",
            "Epoch 171/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 172/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2228 - val_loss: 0.3407\n",
            "Epoch 173/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 174/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 175/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 176/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 177/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3407\n",
            "Epoch 178/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 179/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 180/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2228 - val_loss: 0.3408\n",
            "Epoch 181/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 182/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3407\n",
            "Epoch 183/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 184/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 185/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 186/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3407\n",
            "Epoch 187/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 188/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 189/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2228 - val_loss: 0.3407\n",
            "Epoch 190/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 191/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 192/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3406\n",
            "Epoch 193/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2227 - val_loss: 0.3407\n",
            "Epoch 194/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 195/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 196/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 197/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 198/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2228 - val_loss: 0.3406\n",
            "Epoch 199/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3406\n",
            "Epoch 200/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 201/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 202/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 203/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 204/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 205/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 206/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 207/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 208/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3406\n",
            "Epoch 209/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 210/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 211/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3406\n",
            "Epoch 212/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 213/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 214/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 215/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 216/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 217/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 218/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 219/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 220/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 221/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 222/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 223/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3406\n",
            "Epoch 224/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 225/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 226/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 227/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 228/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2228 - val_loss: 0.3405\n",
            "Epoch 229/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 230/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 231/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 232/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 233/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 234/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 235/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 236/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 237/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 238/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 239/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 240/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 241/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 242/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 243/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 244/1000\n",
            "34/34 [==============================] - 1s 35ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 245/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 246/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 247/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 248/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 249/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 250/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 251/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3405\n",
            "Epoch 252/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 253/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3405\n",
            "Epoch 254/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 255/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2228 - val_loss: 0.3404\n",
            "Epoch 256/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 257/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 258/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 259/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 260/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 261/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 262/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 263/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 264/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 265/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 266/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 267/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3404\n",
            "Epoch 268/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 269/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 270/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 271/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 272/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 273/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 274/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 275/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 276/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 277/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 278/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 279/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 280/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 281/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 282/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 283/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3404\n",
            "Epoch 284/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 285/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 286/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 287/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 288/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3403\n",
            "Epoch 289/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 290/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 291/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 292/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 293/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 294/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 295/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 296/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 297/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 298/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 299/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 300/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 301/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 302/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2227 - val_loss: 0.3403\n",
            "Epoch 303/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 304/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 305/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 306/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 307/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 308/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 309/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 310/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 311/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 312/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 313/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 314/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2227 - val_loss: 0.3402\n",
            "Epoch 315/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 316/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 317/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 318/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2227 - val_loss: 0.3402\n",
            "Epoch 319/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 320/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 321/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 322/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 323/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 324/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 325/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2226 - val_loss: 0.3403\n",
            "Epoch 326/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3403\n",
            "Epoch 327/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 328/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 329/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 330/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2225 - val_loss: 0.3403\n",
            "Epoch 331/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 332/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 333/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 334/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 335/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 336/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 337/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 338/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 339/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 340/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 341/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 342/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 343/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 344/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 345/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 346/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 347/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 348/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 349/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 350/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 351/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 352/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 353/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 354/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 355/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 356/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 357/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 358/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 359/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 360/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 361/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 362/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 363/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 364/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 365/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 366/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 367/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3402\n",
            "Epoch 368/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 369/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 370/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 371/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 372/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 373/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 374/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 375/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 376/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 377/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 378/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 379/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 380/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 381/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 382/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 383/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 384/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 385/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3402\n",
            "Epoch 386/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 387/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 388/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 389/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 390/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 391/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 392/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 393/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 394/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 395/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 396/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 397/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 398/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 399/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 400/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 401/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 402/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 403/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 404/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 405/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 406/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 407/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 408/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 409/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 410/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 411/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 412/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 413/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 414/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 415/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 416/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 417/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 418/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 419/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 420/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 421/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 422/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 423/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 424/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 425/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 426/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 427/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 428/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 429/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 430/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 431/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 432/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 433/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 434/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 435/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 436/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 437/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 438/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 439/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 440/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3401\n",
            "Epoch 441/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 442/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 443/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 444/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 445/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3401\n",
            "Epoch 446/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 447/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 448/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 449/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 450/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 451/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 452/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 453/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 454/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 455/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 456/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 457/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 458/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 459/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 460/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 461/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 462/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 463/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 464/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 465/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 466/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 467/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 468/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 469/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3401\n",
            "Epoch 470/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 471/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 472/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 473/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3400\n",
            "Epoch 474/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 475/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 476/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 477/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 478/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 479/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3400\n",
            "Epoch 480/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 481/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 482/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 483/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 484/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 485/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 486/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 487/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 488/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 489/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 490/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 491/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 492/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3400\n",
            "Epoch 493/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 494/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 495/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 496/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3399\n",
            "Epoch 497/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 498/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 499/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 500/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 501/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3400\n",
            "Epoch 502/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 503/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 504/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 505/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 506/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 507/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 508/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 509/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 510/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 511/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 512/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 513/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2226 - val_loss: 0.3400\n",
            "Epoch 514/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 515/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3400\n",
            "Epoch 516/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 517/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 518/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 519/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 520/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 521/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 522/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 523/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 524/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 525/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 526/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 527/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 528/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 529/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 530/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 531/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 532/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 533/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 534/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 535/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 536/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 537/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2226 - val_loss: 0.3399\n",
            "Epoch 538/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 539/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 540/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 541/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 542/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 543/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 544/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 545/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 546/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 547/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 548/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 549/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 550/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 551/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 552/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3400\n",
            "Epoch 553/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 554/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 555/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 556/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 557/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 558/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 559/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 560/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 561/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 562/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 563/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 564/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 565/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 566/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 567/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 568/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 569/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2226 - val_loss: 0.3399\n",
            "Epoch 570/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 571/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 572/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 573/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 574/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 575/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 576/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 577/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 578/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 579/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 580/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 581/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 582/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 583/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 584/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 585/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 586/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 587/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 588/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 589/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 590/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 591/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 592/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 593/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 594/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 595/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 596/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 597/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 598/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 599/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 600/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 601/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 602/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 603/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 604/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 605/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 606/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 607/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 608/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 609/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 610/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 611/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 612/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 613/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 614/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 615/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 616/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 617/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 618/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 619/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 620/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 621/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 622/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 623/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 624/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 625/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 626/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 627/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 628/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 629/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 630/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 631/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 632/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 633/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 634/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 635/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 636/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 637/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 638/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 639/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 640/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 641/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 642/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 643/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 644/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 645/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 646/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3399\n",
            "Epoch 647/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 648/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 649/1000\n",
            "34/34 [==============================] - 2s 47ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 650/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 651/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 652/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 653/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 654/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 655/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 656/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 657/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 658/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 659/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 660/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 661/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 662/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 663/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 664/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 665/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 666/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 667/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 668/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 669/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 670/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 671/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 672/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 673/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 674/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 675/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 676/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 677/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 678/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 679/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 680/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 681/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 682/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 683/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 684/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 685/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 686/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 687/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 688/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 689/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 690/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 691/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 692/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 693/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 694/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 695/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 696/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 697/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 698/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2224 - val_loss: 0.3399\n",
            "Epoch 699/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 700/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 701/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 702/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 703/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 704/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 705/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 706/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 707/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 708/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 709/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 710/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 711/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 712/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 713/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 714/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 715/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 716/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 717/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 718/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 719/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 720/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 721/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 722/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 723/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 724/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 725/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 726/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 727/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 728/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 729/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 730/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 731/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 732/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 733/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 734/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 735/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 736/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 737/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 738/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 739/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 740/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 741/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 742/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 743/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 744/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 745/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 746/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 747/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 748/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 749/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 750/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 751/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 752/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 753/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 754/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 755/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 756/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 757/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 758/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 759/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 760/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 761/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 762/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 763/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 764/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 765/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 766/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 767/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 768/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 769/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 770/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 771/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 772/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 773/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 774/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 775/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 776/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 777/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 778/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 779/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 780/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 781/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 782/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 783/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 784/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 785/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 786/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 787/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 788/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 789/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 790/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 791/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 792/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 793/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 794/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 795/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 796/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 797/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 798/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 799/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 800/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 801/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 802/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 803/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 804/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 805/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 806/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 807/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 808/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 809/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 810/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 811/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 812/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 813/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 814/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 815/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 816/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 817/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 818/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 819/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 820/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 821/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 822/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 823/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 824/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 825/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 826/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 827/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 828/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 829/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 830/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 831/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3398\n",
            "Epoch 832/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 833/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 834/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 835/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 836/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 837/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 838/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 839/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 840/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 841/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 842/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 843/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 844/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 845/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 846/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 847/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 848/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 849/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 850/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 851/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 852/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 853/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 854/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 855/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 856/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 857/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 858/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 859/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 860/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 861/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 862/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 863/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 864/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 865/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 866/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 867/1000\n",
            "34/34 [==============================] - 2s 46ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 868/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 869/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 870/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 871/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 872/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 873/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 874/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 875/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 876/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 877/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 878/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 879/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 880/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 881/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 882/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 883/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 884/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 885/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 886/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 887/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 888/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 889/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 890/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 891/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 892/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 893/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 894/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 895/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 896/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 897/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 898/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 899/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 900/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 901/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 902/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 903/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 904/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 905/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 906/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 907/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 908/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 909/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 910/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 911/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 912/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 913/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 914/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 915/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 916/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 917/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 918/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 919/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 920/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 921/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 922/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2223 - val_loss: 0.3397\n",
            "Epoch 923/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 924/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 925/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3398\n",
            "Epoch 926/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 927/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 928/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 929/1000\n",
            "34/34 [==============================] - 1s 36ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 930/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 931/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 932/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 933/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 934/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 935/1000\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 936/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 937/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2223 - val_loss: 0.3397\n",
            "Epoch 938/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 939/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 940/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 941/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 942/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 943/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 944/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 945/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 946/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 947/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 948/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 949/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 950/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 951/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 952/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 953/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 954/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 955/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 956/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 957/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 958/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 959/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 960/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 961/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 962/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 963/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 964/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 965/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 966/1000\n",
            "34/34 [==============================] - 2s 46ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 967/1000\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 968/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 969/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 970/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 971/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 972/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 973/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2225 - val_loss: 0.3396\n",
            "Epoch 974/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 975/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 976/1000\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 977/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 978/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 979/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 980/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 981/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 982/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 983/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 984/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 985/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 986/1000\n",
            "34/34 [==============================] - 2s 45ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 987/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 988/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 989/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 990/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 991/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 992/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 993/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 994/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3396\n",
            "Epoch 995/1000\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 996/1000\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 997/1000\n",
            "34/34 [==============================] - 1s 39ms/step - loss: 0.2223 - val_loss: 0.3397\n",
            "Epoch 998/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2225 - val_loss: 0.3397\n",
            "Epoch 999/1000\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 0.2224 - val_loss: 0.3397\n",
            "Epoch 1000/1000\n",
            "34/34 [==============================] - 1s 37ms/step - loss: 0.2224 - val_loss: 0.3397\n"
          ]
        }
      ],
      "source": [
        "model, encoder = autoencoder(dims, init=init)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "csv_logger = CSVLogger(f'{path}/log_ae.csv', append=True, separator=';')\n",
        "\n",
        "history = model.fit(x_train_ada, x_train_ada, batch_size=batch_size, epochs=epochs, validation_data=(X_val, X_val), callbacks=[csv_logger])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ued7fyYTPRA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504be109-f752-48b8-9f3a-f0b88196f554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "encoder.save(f\"{path}/ae_encoder.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLsR2KdUKEvH"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "WlpA9SAoE0Em",
        "outputId": "5430f70c-6fa1-4f90-f31f-5a92f0ebb703"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch [-]\")\n",
        "plt.ylabel(\"Loss [-]\")\n",
        "plt.savefig(f'{path}/figure1.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yggX-Ydj0Pit",
        "outputId": "7593cc42-aa51-44b7-c1f4-dfc0052ae082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22234980762004852 0.3397122919559479\n"
          ]
        }
      ],
      "source": [
        "print(history.history[\"loss\"][-1],history.history[\"val_loss\"][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S0DcQIliKxz"
      },
      "source": [
        "### Applying pretrained autoencoder on testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUCJw7uqKGxI",
        "outputId": "311e33bd-bcca-42de-a92d-16da219e4881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhuette: -0.03799999877810478 ARI 0.192 Davies-Bouldin 3.836\n"
          ]
        }
      ],
      "source": [
        "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "km = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = km.fit_predict(x_test_encoded)\n",
        "ari = adjusted_rand_score(y_pred, y_test[\"Label\"])\n",
        "\n",
        "silhouetteScore = silhouette_score(x_test_encoded, y_test[\"Label\"], metric=\"euclidean\")\n",
        "davies_bouldinScore = davies_bouldin_score(x_test_encoded, y_test[\"Label\"])\n",
        "\n",
        "print(f\"Silhuette: {round(silhouetteScore,3)}\", f\"ARI {round(ari,3)}\", f\"Davies-Bouldin {round(davies_bouldinScore,3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Clearing metrics below for the new round of calculations."
      ],
      "metadata": {
        "id": "aoZgPNGFYm3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del encoder, model\n",
        "K.clear_session()\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "vuAx4WnyYkZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c7632f-1949-4afe-fd1c-71396ab2edc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4981"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL-kw6xp1EGX"
      },
      "source": [
        "## Manual autoencoder tuning - Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation below contains all steps above in a loop for preselected hyperparamenters and CV=5."
      ],
      "metadata": {
        "id": "99qPIjAuX4Ye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3TLMNSu1LMT"
      },
      "outputs": [],
      "source": [
        "# hyperarameters\n",
        "batch_sizes = [64, 128, 256, 512, 1024]\n",
        "epochs = [200, 500, 1000]\n",
        "learning_rates = [0.1, 0.2]\n",
        "\n",
        "\n",
        "# constant values\n",
        "#dims = [4557, 1024, 64]\n",
        "init = VarianceScaling(scale=1. / 2., mode='fan_avg',\n",
        "                      distribution='uniform',\n",
        "                      seed=0)\n",
        "csv_logger = CSVLogger(f'{path}/logs_hyperparameter_tuning.csv', append=True, separator=';')\n",
        "\n",
        "def autoencoder(dims, act='tanh', init='glorot_uniform', batch_norm=True):\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    input_data = Input(shape=(dims[0],), name='input')\n",
        "    x = input_data\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x) # Dense act and kernel_initializer\n",
        "        if batch_norm:\n",
        "            x = BatchNormalization(name='batch_norm_%d' % i)(x) # Add batch normalization layer after dense layer\n",
        "\n",
        "    # hidden layer\n",
        "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
        "\n",
        "    x = encoded\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x) # activation=act, kernel_initializer=init Dense\n",
        "        if batch_norm:\n",
        "            x = BatchNormalization(name='batch_norm_%d' % (n_stacks-i))(x) # Add batch normalization layer after dense layer\n",
        "    # output\n",
        "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
        "    decoded = x\n",
        "    return Model(inputs=input_data, outputs=decoded, name='AE'), Model(inputs=input_data, outputs=encoded, name='encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HWCM2bAi2XLK",
        "outputId": "bb006315-c8da-46fb-e75f-b2a4e2cce6c5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "523/523 [==============================] - 7s 5ms/step - loss: 0.3794 - val_loss: 0.3456\n",
            "Epoch 2/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2968 - val_loss: 0.3096\n",
            "Epoch 3/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2769 - val_loss: 0.2956\n",
            "Epoch 4/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2690 - val_loss: 0.2902\n",
            "Epoch 5/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2658 - val_loss: 0.2883\n",
            "Epoch 6/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2643 - val_loss: 0.2871\n",
            "Epoch 7/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2625 - val_loss: 0.2863\n",
            "Epoch 8/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2613 - val_loss: 0.2855\n",
            "Epoch 9/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2596 - val_loss: 0.2849\n",
            "Epoch 10/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2581 - val_loss: 0.2843\n",
            "Epoch 11/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2559 - val_loss: 0.2833\n",
            "Epoch 12/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2542 - val_loss: 0.2827\n",
            "Epoch 13/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2523 - val_loss: 0.2818\n",
            "Epoch 14/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2496 - val_loss: 0.2811\n",
            "Epoch 15/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2475 - val_loss: 0.2804\n",
            "Epoch 16/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2452 - val_loss: 0.2795\n",
            "Epoch 17/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2427 - val_loss: 0.2791\n",
            "Epoch 18/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2409 - val_loss: 0.2784\n",
            "Epoch 19/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2386 - val_loss: 0.2777\n",
            "Epoch 20/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2366 - val_loss: 0.2771\n",
            "Epoch 21/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2345 - val_loss: 0.2765\n",
            "Epoch 22/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2328 - val_loss: 0.2760\n",
            "Epoch 23/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2307 - val_loss: 0.2758\n",
            "Epoch 24/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2284 - val_loss: 0.2753\n",
            "Epoch 25/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2271 - val_loss: 0.2748\n",
            "Epoch 26/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2251 - val_loss: 0.2745\n",
            "Epoch 27/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2238 - val_loss: 0.2742\n",
            "Epoch 28/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2220 - val_loss: 0.2738\n",
            "Epoch 29/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2202 - val_loss: 0.2737\n",
            "Epoch 30/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2191 - val_loss: 0.2734\n",
            "Epoch 31/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2177 - val_loss: 0.2733\n",
            "Epoch 32/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2162 - val_loss: 0.2732\n",
            "Epoch 33/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2149 - val_loss: 0.2729\n",
            "Epoch 34/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2137 - val_loss: 0.2728\n",
            "Epoch 35/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2126 - val_loss: 0.2727\n",
            "Epoch 36/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2112 - val_loss: 0.2725\n",
            "Epoch 37/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2099 - val_loss: 0.2725\n",
            "Epoch 38/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2090 - val_loss: 0.2724\n",
            "Epoch 39/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2080 - val_loss: 0.2724\n",
            "Epoch 40/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2071 - val_loss: 0.2724\n",
            "Epoch 41/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2061 - val_loss: 0.2723\n",
            "Epoch 42/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2052 - val_loss: 0.2723\n",
            "Epoch 43/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2045 - val_loss: 0.2721\n",
            "Epoch 44/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2034 - val_loss: 0.2721\n",
            "Epoch 45/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.2026 - val_loss: 0.2722\n",
            "Epoch 46/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2019 - val_loss: 0.2722\n",
            "Epoch 47/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2009 - val_loss: 0.2722\n",
            "Epoch 48/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2005 - val_loss: 0.2722\n",
            "Epoch 49/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1992 - val_loss: 0.2721\n",
            "Epoch 50/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1985 - val_loss: 0.2722\n",
            "Epoch 51/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1980 - val_loss: 0.2724\n",
            "Epoch 52/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1973 - val_loss: 0.2724\n",
            "Epoch 53/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1967 - val_loss: 0.2723\n",
            "Epoch 54/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1959 - val_loss: 0.2723\n",
            "Epoch 55/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1953 - val_loss: 0.2723\n",
            "Epoch 56/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1947 - val_loss: 0.2725\n",
            "Epoch 57/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1943 - val_loss: 0.2724\n",
            "Epoch 58/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1936 - val_loss: 0.2726\n",
            "Epoch 59/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1928 - val_loss: 0.2727\n",
            "Epoch 60/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1929 - val_loss: 0.2727\n",
            "Epoch 61/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1924 - val_loss: 0.2727\n",
            "Epoch 62/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1912 - val_loss: 0.2727\n",
            "Epoch 63/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1912 - val_loss: 0.2728\n",
            "Epoch 64/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1906 - val_loss: 0.2729\n",
            "Epoch 65/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1900 - val_loss: 0.2730\n",
            "Epoch 66/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1896 - val_loss: 0.2731\n",
            "Epoch 67/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1895 - val_loss: 0.2731\n",
            "Epoch 68/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1889 - val_loss: 0.2731\n",
            "Epoch 69/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1887 - val_loss: 0.2731\n",
            "Epoch 70/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1883 - val_loss: 0.2732\n",
            "Epoch 71/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1876 - val_loss: 0.2734\n",
            "Epoch 72/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1873 - val_loss: 0.2733\n",
            "Epoch 73/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1871 - val_loss: 0.2735\n",
            "Epoch 74/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1866 - val_loss: 0.2735\n",
            "Epoch 75/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1862 - val_loss: 0.2736\n",
            "Epoch 76/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1861 - val_loss: 0.2738\n",
            "Epoch 77/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1857 - val_loss: 0.2738\n",
            "Epoch 78/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1849 - val_loss: 0.2740\n",
            "Epoch 79/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1853 - val_loss: 0.2739\n",
            "Epoch 80/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1846 - val_loss: 0.2739\n",
            "Epoch 81/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1846 - val_loss: 0.2741\n",
            "Epoch 82/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1841 - val_loss: 0.2741\n",
            "Epoch 83/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1838 - val_loss: 0.2742\n",
            "Epoch 84/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1834 - val_loss: 0.2743\n",
            "Epoch 85/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1832 - val_loss: 0.2745\n",
            "Epoch 86/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1829 - val_loss: 0.2745\n",
            "Epoch 87/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1828 - val_loss: 0.2745\n",
            "Epoch 88/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1823 - val_loss: 0.2747\n",
            "Epoch 89/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1823 - val_loss: 0.2747\n",
            "Epoch 90/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1821 - val_loss: 0.2748\n",
            "Epoch 91/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1815 - val_loss: 0.2748\n",
            "Epoch 92/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1817 - val_loss: 0.2749\n",
            "Epoch 93/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2749\n",
            "Epoch 94/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1813 - val_loss: 0.2750\n",
            "Epoch 95/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1811 - val_loss: 0.2751\n",
            "Epoch 96/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1805 - val_loss: 0.2752\n",
            "Epoch 97/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1801 - val_loss: 0.2752\n",
            "Epoch 98/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1803 - val_loss: 0.2753\n",
            "Epoch 99/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1797 - val_loss: 0.2753\n",
            "Epoch 100/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1799 - val_loss: 0.2754\n",
            "Epoch 101/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1790 - val_loss: 0.2755\n",
            "Epoch 102/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2755\n",
            "Epoch 103/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1790 - val_loss: 0.2756\n",
            "Epoch 104/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1790 - val_loss: 0.2757\n",
            "Epoch 105/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1792 - val_loss: 0.2757\n",
            "Epoch 106/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1787 - val_loss: 0.2758\n",
            "Epoch 107/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1785 - val_loss: 0.2759\n",
            "Epoch 108/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2760\n",
            "Epoch 109/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1783 - val_loss: 0.2762\n",
            "Epoch 110/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1780 - val_loss: 0.2762\n",
            "Epoch 111/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1781 - val_loss: 0.2762\n",
            "Epoch 112/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1776 - val_loss: 0.2763\n",
            "Epoch 113/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2763\n",
            "Epoch 114/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1774 - val_loss: 0.2763\n",
            "Epoch 115/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1772 - val_loss: 0.2764\n",
            "Epoch 116/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1769 - val_loss: 0.2765\n",
            "Epoch 117/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2766\n",
            "Epoch 118/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2766\n",
            "Epoch 119/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1767 - val_loss: 0.2768\n",
            "Epoch 120/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1770 - val_loss: 0.2768\n",
            "Epoch 121/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1760 - val_loss: 0.2768\n",
            "Epoch 122/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2768\n",
            "Epoch 123/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1763 - val_loss: 0.2769\n",
            "Epoch 124/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1761 - val_loss: 0.2769\n",
            "Epoch 125/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1757 - val_loss: 0.2769\n",
            "Epoch 126/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1759 - val_loss: 0.2771\n",
            "Epoch 127/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1755 - val_loss: 0.2771\n",
            "Epoch 128/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1756 - val_loss: 0.2773\n",
            "Epoch 129/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2773\n",
            "Epoch 130/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2773\n",
            "Epoch 131/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1755 - val_loss: 0.2774\n",
            "Epoch 132/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2774\n",
            "Epoch 133/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2776\n",
            "Epoch 134/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1747 - val_loss: 0.2776\n",
            "Epoch 135/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1744 - val_loss: 0.2776\n",
            "Epoch 136/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1748 - val_loss: 0.2777\n",
            "Epoch 137/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1743 - val_loss: 0.2777\n",
            "Epoch 138/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2778\n",
            "Epoch 139/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2779\n",
            "Epoch 140/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1746 - val_loss: 0.2779\n",
            "Epoch 141/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2779\n",
            "Epoch 142/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1743 - val_loss: 0.2780\n",
            "Epoch 143/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1741 - val_loss: 0.2781\n",
            "Epoch 144/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1741 - val_loss: 0.2781\n",
            "Epoch 145/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1737 - val_loss: 0.2782\n",
            "Epoch 146/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2782\n",
            "Epoch 147/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2783\n",
            "Epoch 148/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2782\n",
            "Epoch 149/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1734 - val_loss: 0.2783\n",
            "Epoch 150/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1736 - val_loss: 0.2784\n",
            "Epoch 151/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1734 - val_loss: 0.2785\n",
            "Epoch 152/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2786\n",
            "Epoch 153/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2786\n",
            "Epoch 154/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1729 - val_loss: 0.2786\n",
            "Epoch 155/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2787\n",
            "Epoch 156/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2786\n",
            "Epoch 157/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2788\n",
            "Epoch 158/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2788\n",
            "Epoch 159/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2788\n",
            "Epoch 160/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1722 - val_loss: 0.2788\n",
            "Epoch 161/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1722 - val_loss: 0.2790\n",
            "Epoch 162/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2789\n",
            "Epoch 163/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1722 - val_loss: 0.2790\n",
            "Epoch 164/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1722 - val_loss: 0.2791\n",
            "Epoch 165/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2791\n",
            "Epoch 166/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1719 - val_loss: 0.2792\n",
            "Epoch 167/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2792\n",
            "Epoch 168/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2794\n",
            "Epoch 169/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2796\n",
            "Epoch 170/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1717 - val_loss: 0.2793\n",
            "Epoch 171/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1715 - val_loss: 0.2795\n",
            "Epoch 172/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2794\n",
            "Epoch 173/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2795\n",
            "Epoch 174/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1714 - val_loss: 0.2795\n",
            "Epoch 175/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2795\n",
            "Epoch 176/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2795\n",
            "Epoch 177/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2797\n",
            "Epoch 178/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2796\n",
            "Epoch 179/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2798\n",
            "Epoch 180/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2799\n",
            "Epoch 181/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2799\n",
            "Epoch 182/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2798\n",
            "Epoch 183/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2800\n",
            "Epoch 184/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1712 - val_loss: 0.2800\n",
            "Epoch 185/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2800\n",
            "Epoch 186/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1708 - val_loss: 0.2800\n",
            "Epoch 187/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2802\n",
            "Epoch 188/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2801\n",
            "Epoch 189/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1707 - val_loss: 0.2801\n",
            "Epoch 190/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1702 - val_loss: 0.2802\n",
            "Epoch 191/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1705 - val_loss: 0.2802\n",
            "Epoch 192/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2803\n",
            "Epoch 193/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2803\n",
            "Epoch 194/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1703 - val_loss: 0.2803\n",
            "Epoch 195/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2804\n",
            "Epoch 196/200\n",
            "523/523 [==============================] - 2s 5ms/step - loss: 0.1705 - val_loss: 0.2803\n",
            "Epoch 197/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2805\n",
            "Epoch 198/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2804\n",
            "Epoch 199/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2804\n",
            "Epoch 200/200\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2806\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 64, 0.1703566461801529, 0.2805897891521454, 0.02977198, 3.92818215301005, 0.1844479829446896)\n",
            "Epoch 1/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.3776 - val_loss: 0.3409\n",
            "Epoch 2/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2959 - val_loss: 0.3061\n",
            "Epoch 3/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2757 - val_loss: 0.2920\n",
            "Epoch 4/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2678 - val_loss: 0.2869\n",
            "Epoch 5/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2647 - val_loss: 0.2849\n",
            "Epoch 6/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2629 - val_loss: 0.2838\n",
            "Epoch 7/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2616 - val_loss: 0.2830\n",
            "Epoch 8/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2598 - val_loss: 0.2822\n",
            "Epoch 9/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2583 - val_loss: 0.2813\n",
            "Epoch 10/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2565 - val_loss: 0.2807\n",
            "Epoch 11/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2545 - val_loss: 0.2801\n",
            "Epoch 12/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2525 - val_loss: 0.2793\n",
            "Epoch 13/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2505 - val_loss: 0.2786\n",
            "Epoch 14/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2476 - val_loss: 0.2778\n",
            "Epoch 15/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2456 - val_loss: 0.2771\n",
            "Epoch 16/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2431 - val_loss: 0.2765\n",
            "Epoch 17/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2411 - val_loss: 0.2758\n",
            "Epoch 18/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2386 - val_loss: 0.2752\n",
            "Epoch 19/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2363 - val_loss: 0.2746\n",
            "Epoch 20/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2344 - val_loss: 0.2740\n",
            "Epoch 21/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2326 - val_loss: 0.2737\n",
            "Epoch 22/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2304 - val_loss: 0.2731\n",
            "Epoch 23/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2281 - val_loss: 0.2728\n",
            "Epoch 24/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2267 - val_loss: 0.2723\n",
            "Epoch 25/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2249 - val_loss: 0.2720\n",
            "Epoch 26/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2229 - val_loss: 0.2718\n",
            "Epoch 27/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2213 - val_loss: 0.2715\n",
            "Epoch 28/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2199 - val_loss: 0.2711\n",
            "Epoch 29/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2184 - val_loss: 0.2710\n",
            "Epoch 30/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2169 - val_loss: 0.2708\n",
            "Epoch 31/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2153 - val_loss: 0.2706\n",
            "Epoch 32/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2142 - val_loss: 0.2706\n",
            "Epoch 33/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2127 - val_loss: 0.2704\n",
            "Epoch 34/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2116 - val_loss: 0.2703\n",
            "Epoch 35/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2102 - val_loss: 0.2701\n",
            "Epoch 36/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2096 - val_loss: 0.2700\n",
            "Epoch 37/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2083 - val_loss: 0.2701\n",
            "Epoch 38/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2068 - val_loss: 0.2699\n",
            "Epoch 39/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2061 - val_loss: 0.2699\n",
            "Epoch 40/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2047 - val_loss: 0.2697\n",
            "Epoch 41/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2045 - val_loss: 0.2697\n",
            "Epoch 42/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2033 - val_loss: 0.2698\n",
            "Epoch 43/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2023 - val_loss: 0.2698\n",
            "Epoch 44/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2013 - val_loss: 0.2698\n",
            "Epoch 45/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.2005 - val_loss: 0.2697\n",
            "Epoch 46/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1997 - val_loss: 0.2698\n",
            "Epoch 47/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1994 - val_loss: 0.2698\n",
            "Epoch 48/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1984 - val_loss: 0.2698\n",
            "Epoch 49/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1973 - val_loss: 0.2698\n",
            "Epoch 50/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1969 - val_loss: 0.2698\n",
            "Epoch 51/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1959 - val_loss: 0.2700\n",
            "Epoch 52/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1954 - val_loss: 0.2702\n",
            "Epoch 53/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1946 - val_loss: 0.2700\n",
            "Epoch 54/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1943 - val_loss: 0.2701\n",
            "Epoch 55/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1937 - val_loss: 0.2701\n",
            "Epoch 56/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1931 - val_loss: 0.2702\n",
            "Epoch 57/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1927 - val_loss: 0.2702\n",
            "Epoch 58/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1917 - val_loss: 0.2703\n",
            "Epoch 59/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1914 - val_loss: 0.2704\n",
            "Epoch 60/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1905 - val_loss: 0.2704\n",
            "Epoch 61/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1904 - val_loss: 0.2705\n",
            "Epoch 62/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1897 - val_loss: 0.2704\n",
            "Epoch 63/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1895 - val_loss: 0.2706\n",
            "Epoch 64/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1890 - val_loss: 0.2706\n",
            "Epoch 65/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1882 - val_loss: 0.2708\n",
            "Epoch 66/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1881 - val_loss: 0.2708\n",
            "Epoch 67/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1875 - val_loss: 0.2708\n",
            "Epoch 68/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1870 - val_loss: 0.2710\n",
            "Epoch 69/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1870 - val_loss: 0.2710\n",
            "Epoch 70/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1864 - val_loss: 0.2712\n",
            "Epoch 71/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1861 - val_loss: 0.2711\n",
            "Epoch 72/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1857 - val_loss: 0.2712\n",
            "Epoch 73/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1854 - val_loss: 0.2712\n",
            "Epoch 74/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1851 - val_loss: 0.2713\n",
            "Epoch 75/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1847 - val_loss: 0.2714\n",
            "Epoch 76/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1844 - val_loss: 0.2715\n",
            "Epoch 77/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1841 - val_loss: 0.2716\n",
            "Epoch 78/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1834 - val_loss: 0.2717\n",
            "Epoch 79/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1832 - val_loss: 0.2717\n",
            "Epoch 80/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1830 - val_loss: 0.2718\n",
            "Epoch 81/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1827 - val_loss: 0.2720\n",
            "Epoch 82/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1824 - val_loss: 0.2719\n",
            "Epoch 83/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1820 - val_loss: 0.2719\n",
            "Epoch 84/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1822 - val_loss: 0.2722\n",
            "Epoch 85/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1822 - val_loss: 0.2721\n",
            "Epoch 86/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1812 - val_loss: 0.2722\n",
            "Epoch 87/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1814 - val_loss: 0.2723\n",
            "Epoch 88/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1806 - val_loss: 0.2724\n",
            "Epoch 89/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1806 - val_loss: 0.2724\n",
            "Epoch 90/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1805 - val_loss: 0.2724\n",
            "Epoch 91/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1804 - val_loss: 0.2725\n",
            "Epoch 92/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1796 - val_loss: 0.2727\n",
            "Epoch 93/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1793 - val_loss: 0.2727\n",
            "Epoch 94/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1797 - val_loss: 0.2728\n",
            "Epoch 95/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1793 - val_loss: 0.2728\n",
            "Epoch 96/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1793 - val_loss: 0.2729\n",
            "Epoch 97/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1788 - val_loss: 0.2730\n",
            "Epoch 98/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1788 - val_loss: 0.2730\n",
            "Epoch 99/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1784 - val_loss: 0.2731\n",
            "Epoch 100/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1781 - val_loss: 0.2732\n",
            "Epoch 101/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1782 - val_loss: 0.2732\n",
            "Epoch 102/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1777 - val_loss: 0.2733\n",
            "Epoch 103/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1782 - val_loss: 0.2734\n",
            "Epoch 104/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1772 - val_loss: 0.2734\n",
            "Epoch 105/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1775 - val_loss: 0.2736\n",
            "Epoch 106/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1771 - val_loss: 0.2736\n",
            "Epoch 107/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1772 - val_loss: 0.2737\n",
            "Epoch 108/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2736\n",
            "Epoch 109/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1768 - val_loss: 0.2738\n",
            "Epoch 110/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1768 - val_loss: 0.2738\n",
            "Epoch 111/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1765 - val_loss: 0.2739\n",
            "Epoch 112/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2739\n",
            "Epoch 113/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1761 - val_loss: 0.2740\n",
            "Epoch 114/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1760 - val_loss: 0.2740\n",
            "Epoch 115/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1762 - val_loss: 0.2742\n",
            "Epoch 116/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1758 - val_loss: 0.2742\n",
            "Epoch 117/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1755 - val_loss: 0.2742\n",
            "Epoch 118/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1755 - val_loss: 0.2743\n",
            "Epoch 119/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2743\n",
            "Epoch 120/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1753 - val_loss: 0.2744\n",
            "Epoch 121/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1748 - val_loss: 0.2745\n",
            "Epoch 122/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2746\n",
            "Epoch 123/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2746\n",
            "Epoch 124/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2748\n",
            "Epoch 125/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1746 - val_loss: 0.2747\n",
            "Epoch 126/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2748\n",
            "Epoch 127/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1741 - val_loss: 0.2749\n",
            "Epoch 128/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2749\n",
            "Epoch 129/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2749\n",
            "Epoch 130/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1738 - val_loss: 0.2750\n",
            "Epoch 131/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2750\n",
            "Epoch 132/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2751\n",
            "Epoch 133/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2753\n",
            "Epoch 134/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2753\n",
            "Epoch 135/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1731 - val_loss: 0.2754\n",
            "Epoch 136/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1731 - val_loss: 0.2755\n",
            "Epoch 137/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2754\n",
            "Epoch 138/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2756\n",
            "Epoch 139/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1730 - val_loss: 0.2755\n",
            "Epoch 140/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1725 - val_loss: 0.2756\n",
            "Epoch 141/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1727 - val_loss: 0.2757\n",
            "Epoch 142/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2755\n",
            "Epoch 143/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2756\n",
            "Epoch 144/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2759\n",
            "Epoch 145/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1720 - val_loss: 0.2758\n",
            "Epoch 146/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1722 - val_loss: 0.2759\n",
            "Epoch 147/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2760\n",
            "Epoch 148/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2759\n",
            "Epoch 149/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2759\n",
            "Epoch 150/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1715 - val_loss: 0.2759\n",
            "Epoch 151/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2761\n",
            "Epoch 152/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2762\n",
            "Epoch 153/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2762\n",
            "Epoch 154/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1718 - val_loss: 0.2763\n",
            "Epoch 155/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2763\n",
            "Epoch 156/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2764\n",
            "Epoch 157/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2764\n",
            "Epoch 158/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2765\n",
            "Epoch 159/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2764\n",
            "Epoch 160/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2764\n",
            "Epoch 161/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1714 - val_loss: 0.2765\n",
            "Epoch 162/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2766\n",
            "Epoch 163/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2767\n",
            "Epoch 164/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1710 - val_loss: 0.2767\n",
            "Epoch 165/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2767\n",
            "Epoch 166/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2768\n",
            "Epoch 167/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2768\n",
            "Epoch 168/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2768\n",
            "Epoch 169/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2768\n",
            "Epoch 170/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1708 - val_loss: 0.2770\n",
            "Epoch 171/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1704 - val_loss: 0.2769\n",
            "Epoch 172/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2770\n",
            "Epoch 173/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2771\n",
            "Epoch 174/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1704 - val_loss: 0.2771\n",
            "Epoch 175/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.2772\n",
            "Epoch 176/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2772\n",
            "Epoch 177/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2771\n",
            "Epoch 178/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.2772\n",
            "Epoch 179/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1701 - val_loss: 0.2773\n",
            "Epoch 180/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1701 - val_loss: 0.2774\n",
            "Epoch 181/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1698 - val_loss: 0.2773\n",
            "Epoch 182/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2774\n",
            "Epoch 183/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2774\n",
            "Epoch 184/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1698 - val_loss: 0.2775\n",
            "Epoch 185/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2775\n",
            "Epoch 186/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2775\n",
            "Epoch 187/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2776\n",
            "Epoch 188/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1691 - val_loss: 0.2777\n",
            "Epoch 189/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1692 - val_loss: 0.2776\n",
            "Epoch 190/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2778\n",
            "Epoch 191/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2778\n",
            "Epoch 192/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2778\n",
            "Epoch 193/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2778\n",
            "Epoch 194/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2779\n",
            "Epoch 195/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2779\n",
            "Epoch 196/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2779\n",
            "Epoch 197/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2779\n",
            "Epoch 198/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1690 - val_loss: 0.2781\n",
            "Epoch 199/200\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1691 - val_loss: 0.2780\n",
            "Epoch 200/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1687 - val_loss: 0.2780\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 64, 0.16867779195308685, 0.27804461121559143, 0.028582005, 4.008982571006784, 0.2394030921175509)\n",
            "Epoch 1/200\n",
            "530/530 [==============================] - 4s 5ms/step - loss: 0.3788 - val_loss: 0.3414\n",
            "Epoch 2/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2963 - val_loss: 0.3043\n",
            "Epoch 3/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2764 - val_loss: 0.2909\n",
            "Epoch 4/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2687 - val_loss: 0.2863\n",
            "Epoch 5/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2654 - val_loss: 0.2842\n",
            "Epoch 6/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2637 - val_loss: 0.2830\n",
            "Epoch 7/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2626 - val_loss: 0.2824\n",
            "Epoch 8/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2608 - val_loss: 0.2815\n",
            "Epoch 9/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2595 - val_loss: 0.2807\n",
            "Epoch 10/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2578 - val_loss: 0.2800\n",
            "Epoch 11/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2556 - val_loss: 0.2794\n",
            "Epoch 12/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2534 - val_loss: 0.2786\n",
            "Epoch 13/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2512 - val_loss: 0.2778\n",
            "Epoch 14/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2486 - val_loss: 0.2773\n",
            "Epoch 15/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2463 - val_loss: 0.2765\n",
            "Epoch 16/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2440 - val_loss: 0.2760\n",
            "Epoch 17/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2414 - val_loss: 0.2752\n",
            "Epoch 18/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2394 - val_loss: 0.2747\n",
            "Epoch 19/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2371 - val_loss: 0.2742\n",
            "Epoch 20/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2347 - val_loss: 0.2736\n",
            "Epoch 21/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2331 - val_loss: 0.2732\n",
            "Epoch 22/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2308 - val_loss: 0.2727\n",
            "Epoch 23/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2290 - val_loss: 0.2725\n",
            "Epoch 24/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2271 - val_loss: 0.2719\n",
            "Epoch 25/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2251 - val_loss: 0.2716\n",
            "Epoch 26/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2234 - val_loss: 0.2715\n",
            "Epoch 27/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2216 - val_loss: 0.2711\n",
            "Epoch 28/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2202 - val_loss: 0.2709\n",
            "Epoch 29/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2189 - val_loss: 0.2707\n",
            "Epoch 30/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2173 - val_loss: 0.2704\n",
            "Epoch 31/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2159 - val_loss: 0.2702\n",
            "Epoch 32/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2144 - val_loss: 0.2701\n",
            "Epoch 33/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2129 - val_loss: 0.2699\n",
            "Epoch 34/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2123 - val_loss: 0.2700\n",
            "Epoch 35/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2106 - val_loss: 0.2699\n",
            "Epoch 36/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2092 - val_loss: 0.2697\n",
            "Epoch 37/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2085 - val_loss: 0.2697\n",
            "Epoch 38/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2071 - val_loss: 0.2695\n",
            "Epoch 39/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2065 - val_loss: 0.2695\n",
            "Epoch 40/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2054 - val_loss: 0.2695\n",
            "Epoch 41/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2043 - val_loss: 0.2695\n",
            "Epoch 42/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2033 - val_loss: 0.2696\n",
            "Epoch 43/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2024 - val_loss: 0.2695\n",
            "Epoch 44/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2016 - val_loss: 0.2696\n",
            "Epoch 45/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2006 - val_loss: 0.2695\n",
            "Epoch 46/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1997 - val_loss: 0.2695\n",
            "Epoch 47/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1989 - val_loss: 0.2695\n",
            "Epoch 48/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1986 - val_loss: 0.2695\n",
            "Epoch 49/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1976 - val_loss: 0.2695\n",
            "Epoch 50/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1970 - val_loss: 0.2695\n",
            "Epoch 51/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1964 - val_loss: 0.2696\n",
            "Epoch 52/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1955 - val_loss: 0.2698\n",
            "Epoch 53/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1947 - val_loss: 0.2697\n",
            "Epoch 54/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1941 - val_loss: 0.2697\n",
            "Epoch 55/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1935 - val_loss: 0.2698\n",
            "Epoch 56/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1928 - val_loss: 0.2699\n",
            "Epoch 57/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1925 - val_loss: 0.2700\n",
            "Epoch 58/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1919 - val_loss: 0.2700\n",
            "Epoch 59/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1915 - val_loss: 0.2699\n",
            "Epoch 60/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1910 - val_loss: 0.2702\n",
            "Epoch 61/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1907 - val_loss: 0.2702\n",
            "Epoch 62/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1898 - val_loss: 0.2704\n",
            "Epoch 63/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1894 - val_loss: 0.2703\n",
            "Epoch 64/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1889 - val_loss: 0.2703\n",
            "Epoch 65/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1883 - val_loss: 0.2706\n",
            "Epoch 66/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1882 - val_loss: 0.2705\n",
            "Epoch 67/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1873 - val_loss: 0.2706\n",
            "Epoch 68/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1871 - val_loss: 0.2706\n",
            "Epoch 69/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1867 - val_loss: 0.2707\n",
            "Epoch 70/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1864 - val_loss: 0.2709\n",
            "Epoch 71/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1860 - val_loss: 0.2710\n",
            "Epoch 72/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1856 - val_loss: 0.2709\n",
            "Epoch 73/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1851 - val_loss: 0.2711\n",
            "Epoch 74/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1851 - val_loss: 0.2712\n",
            "Epoch 75/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1845 - val_loss: 0.2713\n",
            "Epoch 76/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1843 - val_loss: 0.2713\n",
            "Epoch 77/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1840 - val_loss: 0.2713\n",
            "Epoch 78/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1838 - val_loss: 0.2715\n",
            "Epoch 79/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1843 - val_loss: 0.2715\n",
            "Epoch 80/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1834 - val_loss: 0.2716\n",
            "Epoch 81/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1826 - val_loss: 0.2716\n",
            "Epoch 82/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1823 - val_loss: 0.2717\n",
            "Epoch 83/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1820 - val_loss: 0.2718\n",
            "Epoch 84/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1822 - val_loss: 0.2719\n",
            "Epoch 85/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1816 - val_loss: 0.2720\n",
            "Epoch 86/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2719\n",
            "Epoch 87/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2720\n",
            "Epoch 88/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1808 - val_loss: 0.2721\n",
            "Epoch 89/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1807 - val_loss: 0.2724\n",
            "Epoch 90/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2723\n",
            "Epoch 91/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1801 - val_loss: 0.2724\n",
            "Epoch 92/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2724\n",
            "Epoch 93/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2726\n",
            "Epoch 94/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2725\n",
            "Epoch 95/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1790 - val_loss: 0.2727\n",
            "Epoch 96/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1792 - val_loss: 0.2729\n",
            "Epoch 97/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1785 - val_loss: 0.2728\n",
            "Epoch 98/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1789 - val_loss: 0.2729\n",
            "Epoch 99/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2729\n",
            "Epoch 100/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1786 - val_loss: 0.2730\n",
            "Epoch 101/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1778 - val_loss: 0.2731\n",
            "Epoch 102/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1776 - val_loss: 0.2732\n",
            "Epoch 103/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1778 - val_loss: 0.2732\n",
            "Epoch 104/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1774 - val_loss: 0.2733\n",
            "Epoch 105/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1772 - val_loss: 0.2735\n",
            "Epoch 106/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1774 - val_loss: 0.2736\n",
            "Epoch 107/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2735\n",
            "Epoch 108/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1770 - val_loss: 0.2736\n",
            "Epoch 109/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2736\n",
            "Epoch 110/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1766 - val_loss: 0.2738\n",
            "Epoch 111/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1763 - val_loss: 0.2739\n",
            "Epoch 112/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2738\n",
            "Epoch 113/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2738\n",
            "Epoch 114/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2740\n",
            "Epoch 115/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2742\n",
            "Epoch 116/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2742\n",
            "Epoch 117/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1755 - val_loss: 0.2741\n",
            "Epoch 118/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2742\n",
            "Epoch 119/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2743\n",
            "Epoch 120/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2743\n",
            "Epoch 121/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2745\n",
            "Epoch 122/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2745\n",
            "Epoch 123/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2746\n",
            "Epoch 124/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2747\n",
            "Epoch 125/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2746\n",
            "Epoch 126/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2748\n",
            "Epoch 127/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2748\n",
            "Epoch 128/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1741 - val_loss: 0.2750\n",
            "Epoch 129/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2749\n",
            "Epoch 130/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2748\n",
            "Epoch 131/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1738 - val_loss: 0.2750\n",
            "Epoch 132/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2749\n",
            "Epoch 133/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2751\n",
            "Epoch 134/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2753\n",
            "Epoch 135/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2753\n",
            "Epoch 136/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2752\n",
            "Epoch 137/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2754\n",
            "Epoch 138/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2754\n",
            "Epoch 139/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2754\n",
            "Epoch 140/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2755\n",
            "Epoch 141/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2755\n",
            "Epoch 142/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2757\n",
            "Epoch 143/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2756\n",
            "Epoch 144/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2757\n",
            "Epoch 145/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2759\n",
            "Epoch 146/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2758\n",
            "Epoch 147/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2758\n",
            "Epoch 148/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2759\n",
            "Epoch 149/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2761\n",
            "Epoch 150/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2760\n",
            "Epoch 151/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2760\n",
            "Epoch 152/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2760\n",
            "Epoch 153/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2761\n",
            "Epoch 154/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2761\n",
            "Epoch 155/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2763\n",
            "Epoch 156/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2764\n",
            "Epoch 157/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2765\n",
            "Epoch 158/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2763\n",
            "Epoch 159/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2764\n",
            "Epoch 160/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2764\n",
            "Epoch 161/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2764\n",
            "Epoch 162/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2765\n",
            "Epoch 163/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2766\n",
            "Epoch 164/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2767\n",
            "Epoch 165/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2767\n",
            "Epoch 166/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2766\n",
            "Epoch 167/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2766\n",
            "Epoch 168/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2768\n",
            "Epoch 169/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2769\n",
            "Epoch 170/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2768\n",
            "Epoch 171/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2771\n",
            "Epoch 172/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2770\n",
            "Epoch 173/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2770\n",
            "Epoch 174/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2771\n",
            "Epoch 175/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2771\n",
            "Epoch 176/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2771\n",
            "Epoch 177/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2772\n",
            "Epoch 178/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2773\n",
            "Epoch 179/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2773\n",
            "Epoch 180/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2773\n",
            "Epoch 181/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2775\n",
            "Epoch 182/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2775\n",
            "Epoch 183/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2775\n",
            "Epoch 184/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2775\n",
            "Epoch 185/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2776\n",
            "Epoch 186/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2776\n",
            "Epoch 187/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2776\n",
            "Epoch 188/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2776\n",
            "Epoch 189/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2777\n",
            "Epoch 190/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2777\n",
            "Epoch 191/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2777\n",
            "Epoch 192/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2778\n",
            "Epoch 193/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2778\n",
            "Epoch 194/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2778\n",
            "Epoch 195/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2778\n",
            "Epoch 196/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2780\n",
            "Epoch 197/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2779\n",
            "Epoch 198/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2780\n",
            "Epoch 199/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2781\n",
            "Epoch 200/200\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2780\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 64, 0.1690361350774765, 0.27798065543174744, 0.02678057, 3.9189183686114646, 0.22460513568087837)\n",
            "Epoch 1/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.3768 - val_loss: 0.3425\n",
            "Epoch 2/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2949 - val_loss: 0.3073\n",
            "Epoch 3/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2747 - val_loss: 0.2935\n",
            "Epoch 4/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2674 - val_loss: 0.2887\n",
            "Epoch 5/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2644 - val_loss: 0.2865\n",
            "Epoch 6/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2625 - val_loss: 0.2853\n",
            "Epoch 7/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2612 - val_loss: 0.2843\n",
            "Epoch 8/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2596 - val_loss: 0.2836\n",
            "Epoch 9/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2579 - val_loss: 0.2829\n",
            "Epoch 10/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2563 - val_loss: 0.2819\n",
            "Epoch 11/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2544 - val_loss: 0.2812\n",
            "Epoch 12/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2520 - val_loss: 0.2803\n",
            "Epoch 13/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2496 - val_loss: 0.2796\n",
            "Epoch 14/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2473 - val_loss: 0.2788\n",
            "Epoch 15/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2457 - val_loss: 0.2782\n",
            "Epoch 16/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2428 - val_loss: 0.2774\n",
            "Epoch 17/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2408 - val_loss: 0.2769\n",
            "Epoch 18/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2384 - val_loss: 0.2763\n",
            "Epoch 19/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2365 - val_loss: 0.2757\n",
            "Epoch 20/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2342 - val_loss: 0.2752\n",
            "Epoch 21/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2323 - val_loss: 0.2749\n",
            "Epoch 22/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2304 - val_loss: 0.2743\n",
            "Epoch 23/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2285 - val_loss: 0.2738\n",
            "Epoch 24/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2267 - val_loss: 0.2735\n",
            "Epoch 25/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2249 - val_loss: 0.2731\n",
            "Epoch 26/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2230 - val_loss: 0.2729\n",
            "Epoch 27/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2218 - val_loss: 0.2726\n",
            "Epoch 28/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2198 - val_loss: 0.2723\n",
            "Epoch 29/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2183 - val_loss: 0.2721\n",
            "Epoch 30/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2173 - val_loss: 0.2720\n",
            "Epoch 31/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2157 - val_loss: 0.2717\n",
            "Epoch 32/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2144 - val_loss: 0.2716\n",
            "Epoch 33/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2132 - val_loss: 0.2717\n",
            "Epoch 34/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2118 - val_loss: 0.2714\n",
            "Epoch 35/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2111 - val_loss: 0.2712\n",
            "Epoch 36/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2096 - val_loss: 0.2712\n",
            "Epoch 37/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2085 - val_loss: 0.2711\n",
            "Epoch 38/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2074 - val_loss: 0.2710\n",
            "Epoch 39/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2063 - val_loss: 0.2710\n",
            "Epoch 40/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2054 - val_loss: 0.2709\n",
            "Epoch 41/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2047 - val_loss: 0.2710\n",
            "Epoch 42/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2035 - val_loss: 0.2709\n",
            "Epoch 43/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2026 - val_loss: 0.2709\n",
            "Epoch 44/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2017 - val_loss: 0.2709\n",
            "Epoch 45/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2005 - val_loss: 0.2710\n",
            "Epoch 46/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1999 - val_loss: 0.2711\n",
            "Epoch 47/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1994 - val_loss: 0.2710\n",
            "Epoch 48/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1986 - val_loss: 0.2711\n",
            "Epoch 49/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1978 - val_loss: 0.2710\n",
            "Epoch 50/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1973 - val_loss: 0.2710\n",
            "Epoch 51/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1964 - val_loss: 0.2711\n",
            "Epoch 52/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1956 - val_loss: 0.2711\n",
            "Epoch 53/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1953 - val_loss: 0.2712\n",
            "Epoch 54/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1944 - val_loss: 0.2712\n",
            "Epoch 55/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1941 - val_loss: 0.2713\n",
            "Epoch 56/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1935 - val_loss: 0.2713\n",
            "Epoch 57/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1931 - val_loss: 0.2713\n",
            "Epoch 58/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1927 - val_loss: 0.2714\n",
            "Epoch 59/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1919 - val_loss: 0.2716\n",
            "Epoch 60/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1909 - val_loss: 0.2715\n",
            "Epoch 61/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1909 - val_loss: 0.2716\n",
            "Epoch 62/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1903 - val_loss: 0.2717\n",
            "Epoch 63/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1899 - val_loss: 0.2717\n",
            "Epoch 64/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1892 - val_loss: 0.2717\n",
            "Epoch 65/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1890 - val_loss: 0.2718\n",
            "Epoch 66/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1885 - val_loss: 0.2718\n",
            "Epoch 67/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1879 - val_loss: 0.2721\n",
            "Epoch 68/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1876 - val_loss: 0.2722\n",
            "Epoch 69/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1870 - val_loss: 0.2720\n",
            "Epoch 70/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1869 - val_loss: 0.2721\n",
            "Epoch 71/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1866 - val_loss: 0.2724\n",
            "Epoch 72/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1866 - val_loss: 0.2723\n",
            "Epoch 73/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1857 - val_loss: 0.2723\n",
            "Epoch 74/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1854 - val_loss: 0.2724\n",
            "Epoch 75/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1852 - val_loss: 0.2726\n",
            "Epoch 76/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1846 - val_loss: 0.2726\n",
            "Epoch 77/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1845 - val_loss: 0.2726\n",
            "Epoch 78/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1840 - val_loss: 0.2728\n",
            "Epoch 79/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1838 - val_loss: 0.2729\n",
            "Epoch 80/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1832 - val_loss: 0.2729\n",
            "Epoch 81/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1830 - val_loss: 0.2732\n",
            "Epoch 82/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1829 - val_loss: 0.2731\n",
            "Epoch 83/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1827 - val_loss: 0.2731\n",
            "Epoch 84/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1824 - val_loss: 0.2732\n",
            "Epoch 85/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1822 - val_loss: 0.2733\n",
            "Epoch 86/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1824 - val_loss: 0.2733\n",
            "Epoch 87/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1817 - val_loss: 0.2733\n",
            "Epoch 88/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1812 - val_loss: 0.2736\n",
            "Epoch 89/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1807 - val_loss: 0.2735\n",
            "Epoch 90/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1809 - val_loss: 0.2736\n",
            "Epoch 91/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2738\n",
            "Epoch 92/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1808 - val_loss: 0.2737\n",
            "Epoch 93/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1806 - val_loss: 0.2738\n",
            "Epoch 94/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2739\n",
            "Epoch 95/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1797 - val_loss: 0.2740\n",
            "Epoch 96/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2739\n",
            "Epoch 97/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1795 - val_loss: 0.2743\n",
            "Epoch 98/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1792 - val_loss: 0.2742\n",
            "Epoch 99/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1790 - val_loss: 0.2744\n",
            "Epoch 100/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1786 - val_loss: 0.2743\n",
            "Epoch 101/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1786 - val_loss: 0.2743\n",
            "Epoch 102/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1785 - val_loss: 0.2744\n",
            "Epoch 103/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1782 - val_loss: 0.2747\n",
            "Epoch 104/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2745\n",
            "Epoch 105/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1779 - val_loss: 0.2747\n",
            "Epoch 106/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.2747\n",
            "Epoch 107/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1774 - val_loss: 0.2749\n",
            "Epoch 108/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1772 - val_loss: 0.2748\n",
            "Epoch 109/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1774 - val_loss: 0.2749\n",
            "Epoch 110/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2750\n",
            "Epoch 111/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2749\n",
            "Epoch 112/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2751\n",
            "Epoch 113/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1766 - val_loss: 0.2752\n",
            "Epoch 114/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1763 - val_loss: 0.2752\n",
            "Epoch 115/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2753\n",
            "Epoch 116/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1764 - val_loss: 0.2752\n",
            "Epoch 117/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1761 - val_loss: 0.2754\n",
            "Epoch 118/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2754\n",
            "Epoch 119/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2754\n",
            "Epoch 120/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2755\n",
            "Epoch 121/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1753 - val_loss: 0.2756\n",
            "Epoch 122/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2757\n",
            "Epoch 123/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2758\n",
            "Epoch 124/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2758\n",
            "Epoch 125/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2758\n",
            "Epoch 126/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1747 - val_loss: 0.2758\n",
            "Epoch 127/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1748 - val_loss: 0.2760\n",
            "Epoch 128/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2759\n",
            "Epoch 129/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2760\n",
            "Epoch 130/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2762\n",
            "Epoch 131/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2762\n",
            "Epoch 132/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2761\n",
            "Epoch 133/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2763\n",
            "Epoch 134/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2762\n",
            "Epoch 135/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2765\n",
            "Epoch 136/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2765\n",
            "Epoch 137/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2765\n",
            "Epoch 138/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2765\n",
            "Epoch 139/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2766\n",
            "Epoch 140/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2765\n",
            "Epoch 141/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2767\n",
            "Epoch 142/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2768\n",
            "Epoch 143/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2768\n",
            "Epoch 144/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2768\n",
            "Epoch 145/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2768\n",
            "Epoch 146/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2769\n",
            "Epoch 147/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2770\n",
            "Epoch 148/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2770\n",
            "Epoch 149/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2771\n",
            "Epoch 150/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2771\n",
            "Epoch 151/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2771\n",
            "Epoch 152/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2771\n",
            "Epoch 153/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2772\n",
            "Epoch 154/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2772\n",
            "Epoch 155/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2774\n",
            "Epoch 156/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2773\n",
            "Epoch 157/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2776\n",
            "Epoch 158/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2775\n",
            "Epoch 159/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2776\n",
            "Epoch 160/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1718 - val_loss: 0.2775\n",
            "Epoch 161/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2776\n",
            "Epoch 162/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2777\n",
            "Epoch 163/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2777\n",
            "Epoch 164/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2778\n",
            "Epoch 165/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2778\n",
            "Epoch 166/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2778\n",
            "Epoch 167/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2779\n",
            "Epoch 168/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2778\n",
            "Epoch 169/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2779\n",
            "Epoch 170/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2780\n",
            "Epoch 171/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2780\n",
            "Epoch 172/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2782\n",
            "Epoch 173/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2782\n",
            "Epoch 174/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2782\n",
            "Epoch 175/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2782\n",
            "Epoch 176/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2782\n",
            "Epoch 177/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2782\n",
            "Epoch 178/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2783\n",
            "Epoch 179/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2784\n",
            "Epoch 180/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2785\n",
            "Epoch 181/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2783\n",
            "Epoch 182/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.2785\n",
            "Epoch 183/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.2785\n",
            "Epoch 184/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2785\n",
            "Epoch 185/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2785\n",
            "Epoch 186/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2787\n",
            "Epoch 187/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.2786\n",
            "Epoch 188/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2787\n",
            "Epoch 189/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2787\n",
            "Epoch 190/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2788\n",
            "Epoch 191/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2787\n",
            "Epoch 192/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2789\n",
            "Epoch 193/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2788\n",
            "Epoch 194/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2789\n",
            "Epoch 195/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2789\n",
            "Epoch 196/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2789\n",
            "Epoch 197/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2789\n",
            "Epoch 198/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2790\n",
            "Epoch 199/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2791\n",
            "Epoch 200/200\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2792\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 64, 0.16905750334262848, 0.2791891098022461, 0.0303595, 3.953265475941371, 0.20828979791717725)\n",
            "Epoch 1/200\n",
            "531/531 [==============================] - 4s 6ms/step - loss: 0.3760 - val_loss: 0.3442\n",
            "Epoch 2/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2952 - val_loss: 0.3082\n",
            "Epoch 3/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2751 - val_loss: 0.2939\n",
            "Epoch 4/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2675 - val_loss: 0.2887\n",
            "Epoch 5/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2641 - val_loss: 0.2867\n",
            "Epoch 6/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2624 - val_loss: 0.2852\n",
            "Epoch 7/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2609 - val_loss: 0.2845\n",
            "Epoch 8/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2595 - val_loss: 0.2837\n",
            "Epoch 9/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2581 - val_loss: 0.2830\n",
            "Epoch 10/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2560 - val_loss: 0.2822\n",
            "Epoch 11/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2545 - val_loss: 0.2813\n",
            "Epoch 12/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2523 - val_loss: 0.2807\n",
            "Epoch 13/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2497 - val_loss: 0.2799\n",
            "Epoch 14/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2474 - val_loss: 0.2791\n",
            "Epoch 15/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2453 - val_loss: 0.2784\n",
            "Epoch 16/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2429 - val_loss: 0.2777\n",
            "Epoch 17/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2409 - val_loss: 0.2771\n",
            "Epoch 18/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2386 - val_loss: 0.2764\n",
            "Epoch 19/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2362 - val_loss: 0.2760\n",
            "Epoch 20/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2340 - val_loss: 0.2752\n",
            "Epoch 21/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2324 - val_loss: 0.2745\n",
            "Epoch 22/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2301 - val_loss: 0.2743\n",
            "Epoch 23/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2287 - val_loss: 0.2737\n",
            "Epoch 24/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2268 - val_loss: 0.2734\n",
            "Epoch 25/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2249 - val_loss: 0.2729\n",
            "Epoch 26/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2233 - val_loss: 0.2726\n",
            "Epoch 27/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2218 - val_loss: 0.2722\n",
            "Epoch 28/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2202 - val_loss: 0.2721\n",
            "Epoch 29/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2187 - val_loss: 0.2719\n",
            "Epoch 30/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2171 - val_loss: 0.2719\n",
            "Epoch 31/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2161 - val_loss: 0.2715\n",
            "Epoch 32/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2145 - val_loss: 0.2713\n",
            "Epoch 33/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2131 - val_loss: 0.2713\n",
            "Epoch 34/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2116 - val_loss: 0.2710\n",
            "Epoch 35/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2108 - val_loss: 0.2710\n",
            "Epoch 36/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2097 - val_loss: 0.2711\n",
            "Epoch 37/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2086 - val_loss: 0.2707\n",
            "Epoch 38/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2076 - val_loss: 0.2707\n",
            "Epoch 39/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2062 - val_loss: 0.2705\n",
            "Epoch 40/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2057 - val_loss: 0.2705\n",
            "Epoch 41/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2042 - val_loss: 0.2708\n",
            "Epoch 42/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2038 - val_loss: 0.2705\n",
            "Epoch 43/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2028 - val_loss: 0.2705\n",
            "Epoch 44/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2018 - val_loss: 0.2704\n",
            "Epoch 45/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2013 - val_loss: 0.2705\n",
            "Epoch 46/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2004 - val_loss: 0.2706\n",
            "Epoch 47/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1996 - val_loss: 0.2704\n",
            "Epoch 48/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1986 - val_loss: 0.2704\n",
            "Epoch 49/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1980 - val_loss: 0.2705\n",
            "Epoch 50/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1974 - val_loss: 0.2704\n",
            "Epoch 51/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1970 - val_loss: 0.2704\n",
            "Epoch 52/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1960 - val_loss: 0.2705\n",
            "Epoch 53/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1953 - val_loss: 0.2707\n",
            "Epoch 54/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1951 - val_loss: 0.2705\n",
            "Epoch 55/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1940 - val_loss: 0.2706\n",
            "Epoch 56/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1937 - val_loss: 0.2707\n",
            "Epoch 57/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1926 - val_loss: 0.2706\n",
            "Epoch 58/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1926 - val_loss: 0.2708\n",
            "Epoch 59/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1918 - val_loss: 0.2708\n",
            "Epoch 60/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1918 - val_loss: 0.2709\n",
            "Epoch 61/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1908 - val_loss: 0.2709\n",
            "Epoch 62/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1905 - val_loss: 0.2709\n",
            "Epoch 63/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1900 - val_loss: 0.2711\n",
            "Epoch 64/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1900 - val_loss: 0.2714\n",
            "Epoch 65/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1890 - val_loss: 0.2712\n",
            "Epoch 66/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1884 - val_loss: 0.2712\n",
            "Epoch 67/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1884 - val_loss: 0.2712\n",
            "Epoch 68/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1879 - val_loss: 0.2712\n",
            "Epoch 69/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1872 - val_loss: 0.2712\n",
            "Epoch 70/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1873 - val_loss: 0.2713\n",
            "Epoch 71/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1866 - val_loss: 0.2715\n",
            "Epoch 72/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1862 - val_loss: 0.2716\n",
            "Epoch 73/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1860 - val_loss: 0.2715\n",
            "Epoch 74/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1853 - val_loss: 0.2717\n",
            "Epoch 75/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1856 - val_loss: 0.2718\n",
            "Epoch 76/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1849 - val_loss: 0.2718\n",
            "Epoch 77/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1844 - val_loss: 0.2718\n",
            "Epoch 78/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1844 - val_loss: 0.2718\n",
            "Epoch 79/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1840 - val_loss: 0.2719\n",
            "Epoch 80/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1838 - val_loss: 0.2720\n",
            "Epoch 81/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1833 - val_loss: 0.2722\n",
            "Epoch 82/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1830 - val_loss: 0.2720\n",
            "Epoch 83/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1829 - val_loss: 0.2725\n",
            "Epoch 84/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1827 - val_loss: 0.2722\n",
            "Epoch 85/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1822 - val_loss: 0.2724\n",
            "Epoch 86/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1818 - val_loss: 0.2724\n",
            "Epoch 87/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1817 - val_loss: 0.2725\n",
            "Epoch 88/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1815 - val_loss: 0.2726\n",
            "Epoch 89/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1814 - val_loss: 0.2726\n",
            "Epoch 90/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1808 - val_loss: 0.2728\n",
            "Epoch 91/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1805 - val_loss: 0.2729\n",
            "Epoch 92/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1805 - val_loss: 0.2729\n",
            "Epoch 93/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1799 - val_loss: 0.2728\n",
            "Epoch 94/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1800 - val_loss: 0.2730\n",
            "Epoch 95/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2729\n",
            "Epoch 96/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2731\n",
            "Epoch 97/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1791 - val_loss: 0.2731\n",
            "Epoch 98/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1793 - val_loss: 0.2733\n",
            "Epoch 99/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1790 - val_loss: 0.2732\n",
            "Epoch 100/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1788 - val_loss: 0.2734\n",
            "Epoch 101/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1788 - val_loss: 0.2733\n",
            "Epoch 102/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1788 - val_loss: 0.2734\n",
            "Epoch 103/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2735\n",
            "Epoch 104/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1785 - val_loss: 0.2736\n",
            "Epoch 105/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1781 - val_loss: 0.2736\n",
            "Epoch 106/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1774 - val_loss: 0.2737\n",
            "Epoch 107/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1777 - val_loss: 0.2739\n",
            "Epoch 108/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.2740\n",
            "Epoch 109/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2738\n",
            "Epoch 110/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.2739\n",
            "Epoch 111/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2740\n",
            "Epoch 112/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1766 - val_loss: 0.2740\n",
            "Epoch 113/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1763 - val_loss: 0.2743\n",
            "Epoch 114/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2742\n",
            "Epoch 115/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2743\n",
            "Epoch 116/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2742\n",
            "Epoch 117/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1766 - val_loss: 0.2743\n",
            "Epoch 118/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2744\n",
            "Epoch 119/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1758 - val_loss: 0.2744\n",
            "Epoch 120/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1759 - val_loss: 0.2747\n",
            "Epoch 121/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2744\n",
            "Epoch 122/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2747\n",
            "Epoch 123/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2747\n",
            "Epoch 124/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2748\n",
            "Epoch 125/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2748\n",
            "Epoch 126/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2749\n",
            "Epoch 127/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2749\n",
            "Epoch 128/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2750\n",
            "Epoch 129/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2751\n",
            "Epoch 130/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2751\n",
            "Epoch 131/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2752\n",
            "Epoch 132/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2752\n",
            "Epoch 133/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1743 - val_loss: 0.2751\n",
            "Epoch 134/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2753\n",
            "Epoch 135/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2753\n",
            "Epoch 136/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2753\n",
            "Epoch 137/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2754\n",
            "Epoch 138/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2756\n",
            "Epoch 139/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2756\n",
            "Epoch 140/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2756\n",
            "Epoch 141/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2756\n",
            "Epoch 142/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1734 - val_loss: 0.2756\n",
            "Epoch 143/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2757\n",
            "Epoch 144/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2757\n",
            "Epoch 145/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2758\n",
            "Epoch 146/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2758\n",
            "Epoch 147/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2758\n",
            "Epoch 148/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2760\n",
            "Epoch 149/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2759\n",
            "Epoch 150/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1722 - val_loss: 0.2761\n",
            "Epoch 151/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1722 - val_loss: 0.2761\n",
            "Epoch 152/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2761\n",
            "Epoch 153/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2763\n",
            "Epoch 154/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2763\n",
            "Epoch 155/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2762\n",
            "Epoch 156/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2764\n",
            "Epoch 157/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2764\n",
            "Epoch 158/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2763\n",
            "Epoch 159/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2765\n",
            "Epoch 160/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2766\n",
            "Epoch 161/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2765\n",
            "Epoch 162/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2767\n",
            "Epoch 163/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2767\n",
            "Epoch 164/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2768\n",
            "Epoch 165/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2767\n",
            "Epoch 166/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2768\n",
            "Epoch 167/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2768\n",
            "Epoch 168/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2768\n",
            "Epoch 169/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2768\n",
            "Epoch 170/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2771\n",
            "Epoch 171/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2769\n",
            "Epoch 172/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2769\n",
            "Epoch 173/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2771\n",
            "Epoch 174/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2772\n",
            "Epoch 175/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2771\n",
            "Epoch 176/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2772\n",
            "Epoch 177/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2773\n",
            "Epoch 178/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2774\n",
            "Epoch 179/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2773\n",
            "Epoch 180/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2773\n",
            "Epoch 181/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.2774\n",
            "Epoch 182/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2773\n",
            "Epoch 183/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2776\n",
            "Epoch 184/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2776\n",
            "Epoch 185/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2773\n",
            "Epoch 186/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2774\n",
            "Epoch 187/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2776\n",
            "Epoch 188/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2777\n",
            "Epoch 189/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2777\n",
            "Epoch 190/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2778\n",
            "Epoch 191/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2777\n",
            "Epoch 192/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2777\n",
            "Epoch 193/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2779\n",
            "Epoch 194/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2779\n",
            "Epoch 195/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2779\n",
            "Epoch 196/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2779\n",
            "Epoch 197/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2778\n",
            "Epoch 198/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2779\n",
            "Epoch 199/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2781\n",
            "Epoch 200/200\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2781\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 64, 0.16934463381767273, 0.2780674993991852, 0.030649493, 3.9102417848418143, 0.19721044563299456)\n",
            "Epoch 1/200\n",
            "262/262 [==============================] - 2s 7ms/step - loss: 0.4209 - val_loss: 0.3988\n",
            "Epoch 2/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.3261 - val_loss: 0.3440\n",
            "Epoch 3/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2981 - val_loss: 0.3213\n",
            "Epoch 4/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2829 - val_loss: 0.3077\n",
            "Epoch 5/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2735 - val_loss: 0.2994\n",
            "Epoch 6/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2677 - val_loss: 0.2944\n",
            "Epoch 7/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2641 - val_loss: 0.2911\n",
            "Epoch 8/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2620 - val_loss: 0.2893\n",
            "Epoch 9/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2607 - val_loss: 0.2880\n",
            "Epoch 10/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2595 - val_loss: 0.2873\n",
            "Epoch 11/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2589 - val_loss: 0.2867\n",
            "Epoch 12/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2581 - val_loss: 0.2861\n",
            "Epoch 13/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2575 - val_loss: 0.2856\n",
            "Epoch 14/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2570 - val_loss: 0.2853\n",
            "Epoch 15/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2562 - val_loss: 0.2849\n",
            "Epoch 16/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2552 - val_loss: 0.2844\n",
            "Epoch 17/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2545 - val_loss: 0.2843\n",
            "Epoch 18/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2540 - val_loss: 0.2839\n",
            "Epoch 19/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2533 - val_loss: 0.2834\n",
            "Epoch 20/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2524 - val_loss: 0.2831\n",
            "Epoch 21/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2515 - val_loss: 0.2827\n",
            "Epoch 22/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2501 - val_loss: 0.2824\n",
            "Epoch 23/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2491 - val_loss: 0.2820\n",
            "Epoch 24/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2480 - val_loss: 0.2818\n",
            "Epoch 25/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2468 - val_loss: 0.2812\n",
            "Epoch 26/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2460 - val_loss: 0.2808\n",
            "Epoch 27/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2448 - val_loss: 0.2805\n",
            "Epoch 28/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2435 - val_loss: 0.2801\n",
            "Epoch 29/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2422 - val_loss: 0.2796\n",
            "Epoch 30/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2411 - val_loss: 0.2793\n",
            "Epoch 31/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2400 - val_loss: 0.2789\n",
            "Epoch 32/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2388 - val_loss: 0.2786\n",
            "Epoch 33/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2376 - val_loss: 0.2783\n",
            "Epoch 34/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2366 - val_loss: 0.2781\n",
            "Epoch 35/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2352 - val_loss: 0.2777\n",
            "Epoch 36/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2341 - val_loss: 0.2773\n",
            "Epoch 37/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2330 - val_loss: 0.2770\n",
            "Epoch 38/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2318 - val_loss: 0.2769\n",
            "Epoch 39/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2308 - val_loss: 0.2766\n",
            "Epoch 40/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2297 - val_loss: 0.2762\n",
            "Epoch 41/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2289 - val_loss: 0.2760\n",
            "Epoch 42/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2278 - val_loss: 0.2757\n",
            "Epoch 43/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2268 - val_loss: 0.2755\n",
            "Epoch 44/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2256 - val_loss: 0.2752\n",
            "Epoch 45/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2248 - val_loss: 0.2750\n",
            "Epoch 46/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2237 - val_loss: 0.2749\n",
            "Epoch 47/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2228 - val_loss: 0.2747\n",
            "Epoch 48/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2218 - val_loss: 0.2745\n",
            "Epoch 49/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2208 - val_loss: 0.2743\n",
            "Epoch 50/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2200 - val_loss: 0.2743\n",
            "Epoch 51/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2191 - val_loss: 0.2740\n",
            "Epoch 52/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2181 - val_loss: 0.2739\n",
            "Epoch 53/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2176 - val_loss: 0.2737\n",
            "Epoch 54/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2167 - val_loss: 0.2736\n",
            "Epoch 55/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2154 - val_loss: 0.2735\n",
            "Epoch 56/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2151 - val_loss: 0.2734\n",
            "Epoch 57/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2143 - val_loss: 0.2733\n",
            "Epoch 58/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2135 - val_loss: 0.2733\n",
            "Epoch 59/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2127 - val_loss: 0.2730\n",
            "Epoch 60/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2119 - val_loss: 0.2730\n",
            "Epoch 61/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2112 - val_loss: 0.2729\n",
            "Epoch 62/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2105 - val_loss: 0.2729\n",
            "Epoch 63/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2099 - val_loss: 0.2728\n",
            "Epoch 64/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2091 - val_loss: 0.2727\n",
            "Epoch 65/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2081 - val_loss: 0.2727\n",
            "Epoch 66/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2078 - val_loss: 0.2726\n",
            "Epoch 67/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2068 - val_loss: 0.2724\n",
            "Epoch 68/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2063 - val_loss: 0.2725\n",
            "Epoch 69/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2056 - val_loss: 0.2724\n",
            "Epoch 70/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2049 - val_loss: 0.2723\n",
            "Epoch 71/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2045 - val_loss: 0.2725\n",
            "Epoch 72/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2039 - val_loss: 0.2723\n",
            "Epoch 73/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2032 - val_loss: 0.2723\n",
            "Epoch 74/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2027 - val_loss: 0.2722\n",
            "Epoch 75/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2018 - val_loss: 0.2723\n",
            "Epoch 76/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.2014 - val_loss: 0.2721\n",
            "Epoch 77/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2012 - val_loss: 0.2722\n",
            "Epoch 78/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2006 - val_loss: 0.2722\n",
            "Epoch 79/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.2001 - val_loss: 0.2721\n",
            "Epoch 80/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2722\n",
            "Epoch 81/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1987 - val_loss: 0.2723\n",
            "Epoch 82/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1983 - val_loss: 0.2722\n",
            "Epoch 83/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1978 - val_loss: 0.2721\n",
            "Epoch 84/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1975 - val_loss: 0.2721\n",
            "Epoch 85/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1970 - val_loss: 0.2721\n",
            "Epoch 86/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1965 - val_loss: 0.2721\n",
            "Epoch 87/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1960 - val_loss: 0.2720\n",
            "Epoch 88/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1953 - val_loss: 0.2723\n",
            "Epoch 89/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1951 - val_loss: 0.2721\n",
            "Epoch 90/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1946 - val_loss: 0.2722\n",
            "Epoch 91/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1944 - val_loss: 0.2721\n",
            "Epoch 92/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1938 - val_loss: 0.2722\n",
            "Epoch 93/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1932 - val_loss: 0.2723\n",
            "Epoch 94/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1928 - val_loss: 0.2726\n",
            "Epoch 95/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1928 - val_loss: 0.2722\n",
            "Epoch 96/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1921 - val_loss: 0.2722\n",
            "Epoch 97/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1917 - val_loss: 0.2722\n",
            "Epoch 98/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1914 - val_loss: 0.2723\n",
            "Epoch 99/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1910 - val_loss: 0.2723\n",
            "Epoch 100/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1905 - val_loss: 0.2723\n",
            "Epoch 101/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1905 - val_loss: 0.2724\n",
            "Epoch 102/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1896 - val_loss: 0.2726\n",
            "Epoch 103/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1894 - val_loss: 0.2725\n",
            "Epoch 104/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1890 - val_loss: 0.2724\n",
            "Epoch 105/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1889 - val_loss: 0.2725\n",
            "Epoch 106/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1884 - val_loss: 0.2725\n",
            "Epoch 107/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1881 - val_loss: 0.2726\n",
            "Epoch 108/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1878 - val_loss: 0.2726\n",
            "Epoch 109/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1874 - val_loss: 0.2726\n",
            "Epoch 110/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1871 - val_loss: 0.2727\n",
            "Epoch 111/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1867 - val_loss: 0.2727\n",
            "Epoch 112/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1864 - val_loss: 0.2727\n",
            "Epoch 113/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1863 - val_loss: 0.2728\n",
            "Epoch 114/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1859 - val_loss: 0.2728\n",
            "Epoch 115/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1856 - val_loss: 0.2730\n",
            "Epoch 116/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1853 - val_loss: 0.2729\n",
            "Epoch 117/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1853 - val_loss: 0.2730\n",
            "Epoch 118/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1845 - val_loss: 0.2731\n",
            "Epoch 119/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1844 - val_loss: 0.2732\n",
            "Epoch 120/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1841 - val_loss: 0.2730\n",
            "Epoch 121/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1840 - val_loss: 0.2731\n",
            "Epoch 122/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1838 - val_loss: 0.2732\n",
            "Epoch 123/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1833 - val_loss: 0.2731\n",
            "Epoch 124/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1830 - val_loss: 0.2735\n",
            "Epoch 125/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1828 - val_loss: 0.2733\n",
            "Epoch 126/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1825 - val_loss: 0.2733\n",
            "Epoch 127/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1822 - val_loss: 0.2734\n",
            "Epoch 128/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1820 - val_loss: 0.2735\n",
            "Epoch 129/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1819 - val_loss: 0.2735\n",
            "Epoch 130/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1815 - val_loss: 0.2736\n",
            "Epoch 131/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1813 - val_loss: 0.2737\n",
            "Epoch 132/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1811 - val_loss: 0.2738\n",
            "Epoch 133/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1808 - val_loss: 0.2737\n",
            "Epoch 134/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1806 - val_loss: 0.2737\n",
            "Epoch 135/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2737\n",
            "Epoch 136/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2739\n",
            "Epoch 137/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1799 - val_loss: 0.2738\n",
            "Epoch 138/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1798 - val_loss: 0.2740\n",
            "Epoch 139/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1794 - val_loss: 0.2742\n",
            "Epoch 140/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1792 - val_loss: 0.2740\n",
            "Epoch 141/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1792 - val_loss: 0.2740\n",
            "Epoch 142/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1788 - val_loss: 0.2742\n",
            "Epoch 143/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1788 - val_loss: 0.2742\n",
            "Epoch 144/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1785 - val_loss: 0.2743\n",
            "Epoch 145/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1782 - val_loss: 0.2743\n",
            "Epoch 146/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1782 - val_loss: 0.2743\n",
            "Epoch 147/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1780 - val_loss: 0.2744\n",
            "Epoch 148/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1777 - val_loss: 0.2745\n",
            "Epoch 149/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1774 - val_loss: 0.2745\n",
            "Epoch 150/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1773 - val_loss: 0.2747\n",
            "Epoch 151/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1773 - val_loss: 0.2745\n",
            "Epoch 152/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1771 - val_loss: 0.2747\n",
            "Epoch 153/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1771 - val_loss: 0.2748\n",
            "Epoch 154/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1765 - val_loss: 0.2748\n",
            "Epoch 155/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1764 - val_loss: 0.2747\n",
            "Epoch 156/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1762 - val_loss: 0.2748\n",
            "Epoch 157/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1761 - val_loss: 0.2751\n",
            "Epoch 158/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1761 - val_loss: 0.2749\n",
            "Epoch 159/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1757 - val_loss: 0.2750\n",
            "Epoch 160/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1758 - val_loss: 0.2751\n",
            "Epoch 161/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1756 - val_loss: 0.2751\n",
            "Epoch 162/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1752 - val_loss: 0.2751\n",
            "Epoch 163/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1752 - val_loss: 0.2752\n",
            "Epoch 164/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1749 - val_loss: 0.2753\n",
            "Epoch 165/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1748 - val_loss: 0.2753\n",
            "Epoch 166/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1749 - val_loss: 0.2754\n",
            "Epoch 167/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1746 - val_loss: 0.2754\n",
            "Epoch 168/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1744 - val_loss: 0.2755\n",
            "Epoch 169/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1743 - val_loss: 0.2755\n",
            "Epoch 170/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1740 - val_loss: 0.2755\n",
            "Epoch 171/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1741 - val_loss: 0.2756\n",
            "Epoch 172/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1737 - val_loss: 0.2757\n",
            "Epoch 173/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1737 - val_loss: 0.2757\n",
            "Epoch 174/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1736 - val_loss: 0.2756\n",
            "Epoch 175/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1736 - val_loss: 0.2758\n",
            "Epoch 176/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1732 - val_loss: 0.2759\n",
            "Epoch 177/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1732 - val_loss: 0.2759\n",
            "Epoch 178/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1729 - val_loss: 0.2759\n",
            "Epoch 179/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1730 - val_loss: 0.2759\n",
            "Epoch 180/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1728 - val_loss: 0.2761\n",
            "Epoch 181/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1726 - val_loss: 0.2761\n",
            "Epoch 182/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1725 - val_loss: 0.2762\n",
            "Epoch 183/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1725 - val_loss: 0.2762\n",
            "Epoch 184/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1723 - val_loss: 0.2762\n",
            "Epoch 185/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1720 - val_loss: 0.2763\n",
            "Epoch 186/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1719 - val_loss: 0.2763\n",
            "Epoch 187/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1719 - val_loss: 0.2764\n",
            "Epoch 188/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1718 - val_loss: 0.2765\n",
            "Epoch 189/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1718 - val_loss: 0.2765\n",
            "Epoch 190/200\n",
            "262/262 [==============================] - 1s 6ms/step - loss: 0.1715 - val_loss: 0.2766\n",
            "Epoch 191/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1714 - val_loss: 0.2767\n",
            "Epoch 192/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1716 - val_loss: 0.2766\n",
            "Epoch 193/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1713 - val_loss: 0.2767\n",
            "Epoch 194/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1709 - val_loss: 0.2767\n",
            "Epoch 195/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1709 - val_loss: 0.2768\n",
            "Epoch 196/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1709 - val_loss: 0.2768\n",
            "Epoch 197/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2769\n",
            "Epoch 198/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2769\n",
            "Epoch 199/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1704 - val_loss: 0.2770\n",
            "Epoch 200/200\n",
            "262/262 [==============================] - 2s 6ms/step - loss: 0.1704 - val_loss: 0.2771\n",
            "19/19 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 128, 0.17043772339820862, 0.27712562680244446, 0.03255251, 3.805178763158916, 0.23614560928255107)\n",
            "Epoch 1/200\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.4180 - val_loss: 0.3921\n",
            "Epoch 2/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3241 - val_loss: 0.3388\n",
            "Epoch 3/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2966 - val_loss: 0.3177\n",
            "Epoch 4/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2815 - val_loss: 0.3043\n",
            "Epoch 5/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2720 - val_loss: 0.2957\n",
            "Epoch 6/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2663 - val_loss: 0.2908\n",
            "Epoch 7/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2631 - val_loss: 0.2878\n",
            "Epoch 8/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2609 - val_loss: 0.2860\n",
            "Epoch 9/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2596 - val_loss: 0.2849\n",
            "Epoch 10/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2584 - val_loss: 0.2840\n",
            "Epoch 11/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2576 - val_loss: 0.2835\n",
            "Epoch 12/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2570 - val_loss: 0.2830\n",
            "Epoch 13/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2563 - val_loss: 0.2825\n",
            "Epoch 14/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2555 - val_loss: 0.2821\n",
            "Epoch 15/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2549 - val_loss: 0.2818\n",
            "Epoch 16/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2540 - val_loss: 0.2815\n",
            "Epoch 17/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2534 - val_loss: 0.2810\n",
            "Epoch 18/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2526 - val_loss: 0.2807\n",
            "Epoch 19/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2519 - val_loss: 0.2803\n",
            "Epoch 20/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2507 - val_loss: 0.2799\n",
            "Epoch 21/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2495 - val_loss: 0.2797\n",
            "Epoch 22/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2485 - val_loss: 0.2793\n",
            "Epoch 23/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2474 - val_loss: 0.2789\n",
            "Epoch 24/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2462 - val_loss: 0.2785\n",
            "Epoch 25/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2452 - val_loss: 0.2782\n",
            "Epoch 26/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2439 - val_loss: 0.2778\n",
            "Epoch 27/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2426 - val_loss: 0.2774\n",
            "Epoch 28/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2415 - val_loss: 0.2770\n",
            "Epoch 29/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2404 - val_loss: 0.2766\n",
            "Epoch 30/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2395 - val_loss: 0.2764\n",
            "Epoch 31/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2377 - val_loss: 0.2759\n",
            "Epoch 32/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2368 - val_loss: 0.2755\n",
            "Epoch 33/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2355 - val_loss: 0.2753\n",
            "Epoch 34/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2346 - val_loss: 0.2750\n",
            "Epoch 35/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2334 - val_loss: 0.2746\n",
            "Epoch 36/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2322 - val_loss: 0.2744\n",
            "Epoch 37/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2310 - val_loss: 0.2741\n",
            "Epoch 38/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2301 - val_loss: 0.2738\n",
            "Epoch 39/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2287 - val_loss: 0.2734\n",
            "Epoch 40/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2280 - val_loss: 0.2734\n",
            "Epoch 41/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2268 - val_loss: 0.2730\n",
            "Epoch 42/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2256 - val_loss: 0.2728\n",
            "Epoch 43/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2245 - val_loss: 0.2725\n",
            "Epoch 44/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2236 - val_loss: 0.2723\n",
            "Epoch 45/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2226 - val_loss: 0.2722\n",
            "Epoch 46/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2215 - val_loss: 0.2719\n",
            "Epoch 47/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2208 - val_loss: 0.2716\n",
            "Epoch 48/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2197 - val_loss: 0.2715\n",
            "Epoch 49/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2189 - val_loss: 0.2714\n",
            "Epoch 50/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2179 - val_loss: 0.2714\n",
            "Epoch 51/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2170 - val_loss: 0.2711\n",
            "Epoch 52/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2161 - val_loss: 0.2709\n",
            "Epoch 53/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2151 - val_loss: 0.2708\n",
            "Epoch 54/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2145 - val_loss: 0.2707\n",
            "Epoch 55/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2135 - val_loss: 0.2706\n",
            "Epoch 56/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2127 - val_loss: 0.2706\n",
            "Epoch 57/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2118 - val_loss: 0.2705\n",
            "Epoch 58/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2112 - val_loss: 0.2703\n",
            "Epoch 59/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2104 - val_loss: 0.2703\n",
            "Epoch 60/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2096 - val_loss: 0.2701\n",
            "Epoch 61/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2089 - val_loss: 0.2701\n",
            "Epoch 62/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2082 - val_loss: 0.2699\n",
            "Epoch 63/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2075 - val_loss: 0.2699\n",
            "Epoch 64/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2068 - val_loss: 0.2698\n",
            "Epoch 65/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2060 - val_loss: 0.2698\n",
            "Epoch 66/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2054 - val_loss: 0.2697\n",
            "Epoch 67/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2047 - val_loss: 0.2698\n",
            "Epoch 68/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2039 - val_loss: 0.2698\n",
            "Epoch 69/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2035 - val_loss: 0.2697\n",
            "Epoch 70/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2027 - val_loss: 0.2696\n",
            "Epoch 71/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2022 - val_loss: 0.2695\n",
            "Epoch 72/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2015 - val_loss: 0.2696\n",
            "Epoch 73/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2011 - val_loss: 0.2696\n",
            "Epoch 74/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2005 - val_loss: 0.2695\n",
            "Epoch 75/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1998 - val_loss: 0.2697\n",
            "Epoch 76/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1994 - val_loss: 0.2695\n",
            "Epoch 77/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1988 - val_loss: 0.2695\n",
            "Epoch 78/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1983 - val_loss: 0.2696\n",
            "Epoch 79/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1977 - val_loss: 0.2694\n",
            "Epoch 80/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1971 - val_loss: 0.2697\n",
            "Epoch 81/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1965 - val_loss: 0.2696\n",
            "Epoch 82/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1961 - val_loss: 0.2694\n",
            "Epoch 83/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1956 - val_loss: 0.2694\n",
            "Epoch 84/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1953 - val_loss: 0.2695\n",
            "Epoch 85/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1947 - val_loss: 0.2695\n",
            "Epoch 86/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1945 - val_loss: 0.2695\n",
            "Epoch 87/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1936 - val_loss: 0.2698\n",
            "Epoch 88/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1934 - val_loss: 0.2697\n",
            "Epoch 89/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1929 - val_loss: 0.2697\n",
            "Epoch 90/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1925 - val_loss: 0.2697\n",
            "Epoch 91/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1918 - val_loss: 0.2695\n",
            "Epoch 92/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1917 - val_loss: 0.2698\n",
            "Epoch 93/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1913 - val_loss: 0.2696\n",
            "Epoch 94/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1906 - val_loss: 0.2696\n",
            "Epoch 95/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1906 - val_loss: 0.2697\n",
            "Epoch 96/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1899 - val_loss: 0.2699\n",
            "Epoch 97/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1898 - val_loss: 0.2698\n",
            "Epoch 98/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1889 - val_loss: 0.2698\n",
            "Epoch 99/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1888 - val_loss: 0.2700\n",
            "Epoch 100/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1883 - val_loss: 0.2699\n",
            "Epoch 101/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1881 - val_loss: 0.2703\n",
            "Epoch 102/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1880 - val_loss: 0.2699\n",
            "Epoch 103/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1871 - val_loss: 0.2700\n",
            "Epoch 104/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1872 - val_loss: 0.2701\n",
            "Epoch 105/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1866 - val_loss: 0.2702\n",
            "Epoch 106/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1864 - val_loss: 0.2702\n",
            "Epoch 107/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1861 - val_loss: 0.2701\n",
            "Epoch 108/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1857 - val_loss: 0.2702\n",
            "Epoch 109/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1855 - val_loss: 0.2704\n",
            "Epoch 110/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1850 - val_loss: 0.2703\n",
            "Epoch 111/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1848 - val_loss: 0.2703\n",
            "Epoch 112/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1845 - val_loss: 0.2704\n",
            "Epoch 113/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1840 - val_loss: 0.2704\n",
            "Epoch 114/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1838 - val_loss: 0.2705\n",
            "Epoch 115/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1834 - val_loss: 0.2705\n",
            "Epoch 116/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1835 - val_loss: 0.2706\n",
            "Epoch 117/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1828 - val_loss: 0.2706\n",
            "Epoch 118/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1830 - val_loss: 0.2707\n",
            "Epoch 119/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1826 - val_loss: 0.2707\n",
            "Epoch 120/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1823 - val_loss: 0.2708\n",
            "Epoch 121/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1819 - val_loss: 0.2709\n",
            "Epoch 122/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1816 - val_loss: 0.2709\n",
            "Epoch 123/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1812 - val_loss: 0.2708\n",
            "Epoch 124/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1811 - val_loss: 0.2709\n",
            "Epoch 125/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1807 - val_loss: 0.2711\n",
            "Epoch 126/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1809 - val_loss: 0.2711\n",
            "Epoch 127/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1806 - val_loss: 0.2712\n",
            "Epoch 128/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1799 - val_loss: 0.2712\n",
            "Epoch 129/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2714\n",
            "Epoch 130/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2712\n",
            "Epoch 131/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1794 - val_loss: 0.2713\n",
            "Epoch 132/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1794 - val_loss: 0.2713\n",
            "Epoch 133/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1790 - val_loss: 0.2715\n",
            "Epoch 134/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1787 - val_loss: 0.2715\n",
            "Epoch 135/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1789 - val_loss: 0.2716\n",
            "Epoch 136/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1781 - val_loss: 0.2717\n",
            "Epoch 137/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1782 - val_loss: 0.2717\n",
            "Epoch 138/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1778 - val_loss: 0.2716\n",
            "Epoch 139/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1777 - val_loss: 0.2717\n",
            "Epoch 140/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1774 - val_loss: 0.2718\n",
            "Epoch 141/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1773 - val_loss: 0.2718\n",
            "Epoch 142/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1773 - val_loss: 0.2718\n",
            "Epoch 143/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1769 - val_loss: 0.2719\n",
            "Epoch 144/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1767 - val_loss: 0.2720\n",
            "Epoch 145/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1766 - val_loss: 0.2721\n",
            "Epoch 146/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1763 - val_loss: 0.2721\n",
            "Epoch 147/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1761 - val_loss: 0.2722\n",
            "Epoch 148/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1762 - val_loss: 0.2723\n",
            "Epoch 149/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1758 - val_loss: 0.2723\n",
            "Epoch 150/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1756 - val_loss: 0.2723\n",
            "Epoch 151/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1752 - val_loss: 0.2724\n",
            "Epoch 152/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1751 - val_loss: 0.2724\n",
            "Epoch 153/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1751 - val_loss: 0.2725\n",
            "Epoch 154/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1749 - val_loss: 0.2725\n",
            "Epoch 155/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1747 - val_loss: 0.2726\n",
            "Epoch 156/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1746 - val_loss: 0.2726\n",
            "Epoch 157/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1745 - val_loss: 0.2726\n",
            "Epoch 158/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1742 - val_loss: 0.2728\n",
            "Epoch 159/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1743 - val_loss: 0.2728\n",
            "Epoch 160/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1740 - val_loss: 0.2728\n",
            "Epoch 161/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1739 - val_loss: 0.2730\n",
            "Epoch 162/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1736 - val_loss: 0.2729\n",
            "Epoch 163/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1733 - val_loss: 0.2730\n",
            "Epoch 164/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1733 - val_loss: 0.2730\n",
            "Epoch 165/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1733 - val_loss: 0.2731\n",
            "Epoch 166/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1731 - val_loss: 0.2730\n",
            "Epoch 167/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1726 - val_loss: 0.2731\n",
            "Epoch 168/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1728 - val_loss: 0.2733\n",
            "Epoch 169/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1725 - val_loss: 0.2732\n",
            "Epoch 170/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2735\n",
            "Epoch 171/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2735\n",
            "Epoch 172/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1722 - val_loss: 0.2734\n",
            "Epoch 173/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1721 - val_loss: 0.2736\n",
            "Epoch 174/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1719 - val_loss: 0.2735\n",
            "Epoch 175/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1719 - val_loss: 0.2737\n",
            "Epoch 176/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1717 - val_loss: 0.2738\n",
            "Epoch 177/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1712 - val_loss: 0.2736\n",
            "Epoch 178/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1714 - val_loss: 0.2737\n",
            "Epoch 179/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1711 - val_loss: 0.2738\n",
            "Epoch 180/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1711 - val_loss: 0.2738\n",
            "Epoch 181/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1711 - val_loss: 0.2742\n",
            "Epoch 182/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2740\n",
            "Epoch 183/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2740\n",
            "Epoch 184/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2740\n",
            "Epoch 185/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1706 - val_loss: 0.2741\n",
            "Epoch 186/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1705 - val_loss: 0.2741\n",
            "Epoch 187/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1703 - val_loss: 0.2742\n",
            "Epoch 188/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1702 - val_loss: 0.2743\n",
            "Epoch 189/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1703 - val_loss: 0.2743\n",
            "Epoch 190/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1698 - val_loss: 0.2743\n",
            "Epoch 191/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1700 - val_loss: 0.2744\n",
            "Epoch 192/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1699 - val_loss: 0.2744\n",
            "Epoch 193/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1694 - val_loss: 0.2745\n",
            "Epoch 194/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1692 - val_loss: 0.2745\n",
            "Epoch 195/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1693 - val_loss: 0.2745\n",
            "Epoch 196/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1691 - val_loss: 0.2747\n",
            "Epoch 197/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1692 - val_loss: 0.2746\n",
            "Epoch 198/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1695 - val_loss: 0.2748\n",
            "Epoch 199/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1689 - val_loss: 0.2747\n",
            "Epoch 200/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1686 - val_loss: 0.2748\n",
            "19/19 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 128, 0.16864146292209625, 0.2748028337955475, 0.031207059, 3.8619313631744796, 0.24470661453712303)\n",
            "Epoch 1/200\n",
            "265/265 [==============================] - 2s 7ms/step - loss: 0.4210 - val_loss: 0.3913\n",
            "Epoch 2/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.3266 - val_loss: 0.3389\n",
            "Epoch 3/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2979 - val_loss: 0.3159\n",
            "Epoch 4/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2827 - val_loss: 0.3029\n",
            "Epoch 5/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2732 - val_loss: 0.2948\n",
            "Epoch 6/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2675 - val_loss: 0.2898\n",
            "Epoch 7/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2642 - val_loss: 0.2869\n",
            "Epoch 8/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2617 - val_loss: 0.2851\n",
            "Epoch 9/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2606 - val_loss: 0.2839\n",
            "Epoch 10/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2596 - val_loss: 0.2832\n",
            "Epoch 11/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2588 - val_loss: 0.2826\n",
            "Epoch 12/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2581 - val_loss: 0.2820\n",
            "Epoch 13/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2576 - val_loss: 0.2817\n",
            "Epoch 14/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2567 - val_loss: 0.2812\n",
            "Epoch 15/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2561 - val_loss: 0.2809\n",
            "Epoch 16/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2555 - val_loss: 0.2805\n",
            "Epoch 17/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2545 - val_loss: 0.2803\n",
            "Epoch 18/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2538 - val_loss: 0.2798\n",
            "Epoch 19/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2526 - val_loss: 0.2796\n",
            "Epoch 20/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2517 - val_loss: 0.2792\n",
            "Epoch 21/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2507 - val_loss: 0.2788\n",
            "Epoch 22/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2496 - val_loss: 0.2785\n",
            "Epoch 23/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2486 - val_loss: 0.2782\n",
            "Epoch 24/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2474 - val_loss: 0.2778\n",
            "Epoch 25/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2461 - val_loss: 0.2774\n",
            "Epoch 26/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2451 - val_loss: 0.2770\n",
            "Epoch 27/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2439 - val_loss: 0.2767\n",
            "Epoch 28/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2424 - val_loss: 0.2763\n",
            "Epoch 29/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2411 - val_loss: 0.2759\n",
            "Epoch 30/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2401 - val_loss: 0.2756\n",
            "Epoch 31/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2388 - val_loss: 0.2753\n",
            "Epoch 32/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2376 - val_loss: 0.2749\n",
            "Epoch 33/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2365 - val_loss: 0.2747\n",
            "Epoch 34/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2353 - val_loss: 0.2744\n",
            "Epoch 35/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2342 - val_loss: 0.2741\n",
            "Epoch 36/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2330 - val_loss: 0.2737\n",
            "Epoch 37/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2317 - val_loss: 0.2734\n",
            "Epoch 38/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2306 - val_loss: 0.2732\n",
            "Epoch 39/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2293 - val_loss: 0.2730\n",
            "Epoch 40/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2285 - val_loss: 0.2727\n",
            "Epoch 41/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2271 - val_loss: 0.2725\n",
            "Epoch 42/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2262 - val_loss: 0.2722\n",
            "Epoch 43/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2252 - val_loss: 0.2720\n",
            "Epoch 44/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2244 - val_loss: 0.2718\n",
            "Epoch 45/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2232 - val_loss: 0.2717\n",
            "Epoch 46/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2221 - val_loss: 0.2715\n",
            "Epoch 47/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2213 - val_loss: 0.2713\n",
            "Epoch 48/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2203 - val_loss: 0.2711\n",
            "Epoch 49/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2194 - val_loss: 0.2710\n",
            "Epoch 50/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2182 - val_loss: 0.2709\n",
            "Epoch 51/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2175 - val_loss: 0.2707\n",
            "Epoch 52/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2164 - val_loss: 0.2706\n",
            "Epoch 53/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2154 - val_loss: 0.2705\n",
            "Epoch 54/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2146 - val_loss: 0.2703\n",
            "Epoch 55/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2141 - val_loss: 0.2702\n",
            "Epoch 56/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2132 - val_loss: 0.2701\n",
            "Epoch 57/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2127 - val_loss: 0.2701\n",
            "Epoch 58/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2117 - val_loss: 0.2700\n",
            "Epoch 59/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2108 - val_loss: 0.2698\n",
            "Epoch 60/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2098 - val_loss: 0.2698\n",
            "Epoch 61/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2095 - val_loss: 0.2697\n",
            "Epoch 62/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2085 - val_loss: 0.2696\n",
            "Epoch 63/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2076 - val_loss: 0.2695\n",
            "Epoch 64/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2070 - val_loss: 0.2695\n",
            "Epoch 65/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2065 - val_loss: 0.2694\n",
            "Epoch 66/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2058 - val_loss: 0.2694\n",
            "Epoch 67/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2052 - val_loss: 0.2694\n",
            "Epoch 68/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2043 - val_loss: 0.2693\n",
            "Epoch 69/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2036 - val_loss: 0.2694\n",
            "Epoch 70/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2032 - val_loss: 0.2692\n",
            "Epoch 71/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2030 - val_loss: 0.2692\n",
            "Epoch 72/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2018 - val_loss: 0.2692\n",
            "Epoch 73/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2015 - val_loss: 0.2692\n",
            "Epoch 74/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2008 - val_loss: 0.2693\n",
            "Epoch 75/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.2003 - val_loss: 0.2691\n",
            "Epoch 76/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1998 - val_loss: 0.2691\n",
            "Epoch 77/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1990 - val_loss: 0.2691\n",
            "Epoch 78/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1985 - val_loss: 0.2691\n",
            "Epoch 79/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1980 - val_loss: 0.2691\n",
            "Epoch 80/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1974 - val_loss: 0.2691\n",
            "Epoch 81/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1970 - val_loss: 0.2691\n",
            "Epoch 82/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1962 - val_loss: 0.2691\n",
            "Epoch 83/200\n",
            "265/265 [==============================] - 2s 7ms/step - loss: 0.1960 - val_loss: 0.2691\n",
            "Epoch 84/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1952 - val_loss: 0.2691\n",
            "Epoch 85/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1950 - val_loss: 0.2691\n",
            "Epoch 86/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1948 - val_loss: 0.2692\n",
            "Epoch 87/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1938 - val_loss: 0.2692\n",
            "Epoch 88/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1935 - val_loss: 0.2692\n",
            "Epoch 89/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1931 - val_loss: 0.2692\n",
            "Epoch 90/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1929 - val_loss: 0.2692\n",
            "Epoch 91/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1924 - val_loss: 0.2693\n",
            "Epoch 92/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1921 - val_loss: 0.2692\n",
            "Epoch 93/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1914 - val_loss: 0.2693\n",
            "Epoch 94/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1910 - val_loss: 0.2693\n",
            "Epoch 95/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1905 - val_loss: 0.2694\n",
            "Epoch 96/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1904 - val_loss: 0.2693\n",
            "Epoch 97/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1898 - val_loss: 0.2694\n",
            "Epoch 98/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1895 - val_loss: 0.2694\n",
            "Epoch 99/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1890 - val_loss: 0.2695\n",
            "Epoch 100/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1891 - val_loss: 0.2695\n",
            "Epoch 101/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1885 - val_loss: 0.2695\n",
            "Epoch 102/200\n",
            "265/265 [==============================] - 2s 7ms/step - loss: 0.1879 - val_loss: 0.2695\n",
            "Epoch 103/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1875 - val_loss: 0.2695\n",
            "Epoch 104/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1873 - val_loss: 0.2697\n",
            "Epoch 105/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1870 - val_loss: 0.2698\n",
            "Epoch 106/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1865 - val_loss: 0.2698\n",
            "Epoch 107/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1863 - val_loss: 0.2697\n",
            "Epoch 108/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1858 - val_loss: 0.2697\n",
            "Epoch 109/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1855 - val_loss: 0.2698\n",
            "Epoch 110/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1854 - val_loss: 0.2698\n",
            "Epoch 111/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1849 - val_loss: 0.2699\n",
            "Epoch 112/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1847 - val_loss: 0.2700\n",
            "Epoch 113/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1843 - val_loss: 0.2701\n",
            "Epoch 114/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1841 - val_loss: 0.2700\n",
            "Epoch 115/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1838 - val_loss: 0.2701\n",
            "Epoch 116/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1833 - val_loss: 0.2701\n",
            "Epoch 117/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1834 - val_loss: 0.2702\n",
            "Epoch 118/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1830 - val_loss: 0.2702\n",
            "Epoch 119/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1825 - val_loss: 0.2703\n",
            "Epoch 120/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1825 - val_loss: 0.2703\n",
            "Epoch 121/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1819 - val_loss: 0.2704\n",
            "Epoch 122/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1817 - val_loss: 0.2704\n",
            "Epoch 123/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1812 - val_loss: 0.2704\n",
            "Epoch 124/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1814 - val_loss: 0.2706\n",
            "Epoch 125/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1811 - val_loss: 0.2705\n",
            "Epoch 126/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1811 - val_loss: 0.2706\n",
            "Epoch 127/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1803 - val_loss: 0.2706\n",
            "Epoch 128/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1804 - val_loss: 0.2707\n",
            "Epoch 129/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2710\n",
            "Epoch 130/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1802 - val_loss: 0.2711\n",
            "Epoch 131/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1800 - val_loss: 0.2708\n",
            "Epoch 132/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1791 - val_loss: 0.2709\n",
            "Epoch 133/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1790 - val_loss: 0.2710\n",
            "Epoch 134/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1791 - val_loss: 0.2710\n",
            "Epoch 135/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1787 - val_loss: 0.2711\n",
            "Epoch 136/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1783 - val_loss: 0.2711\n",
            "Epoch 137/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1783 - val_loss: 0.2713\n",
            "Epoch 138/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1781 - val_loss: 0.2712\n",
            "Epoch 139/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1778 - val_loss: 0.2714\n",
            "Epoch 140/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1775 - val_loss: 0.2713\n",
            "Epoch 141/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1775 - val_loss: 0.2715\n",
            "Epoch 142/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1773 - val_loss: 0.2714\n",
            "Epoch 143/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1768 - val_loss: 0.2715\n",
            "Epoch 144/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1766 - val_loss: 0.2716\n",
            "Epoch 145/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1767 - val_loss: 0.2716\n",
            "Epoch 146/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1766 - val_loss: 0.2716\n",
            "Epoch 147/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1761 - val_loss: 0.2717\n",
            "Epoch 148/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1761 - val_loss: 0.2718\n",
            "Epoch 149/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1758 - val_loss: 0.2718\n",
            "Epoch 150/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1757 - val_loss: 0.2718\n",
            "Epoch 151/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1757 - val_loss: 0.2719\n",
            "Epoch 152/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1754 - val_loss: 0.2719\n",
            "Epoch 153/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1751 - val_loss: 0.2720\n",
            "Epoch 154/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1746 - val_loss: 0.2720\n",
            "Epoch 155/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1746 - val_loss: 0.2721\n",
            "Epoch 156/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1745 - val_loss: 0.2722\n",
            "Epoch 157/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1747 - val_loss: 0.2723\n",
            "Epoch 158/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1741 - val_loss: 0.2723\n",
            "Epoch 159/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1742 - val_loss: 0.2724\n",
            "Epoch 160/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1740 - val_loss: 0.2723\n",
            "Epoch 161/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1737 - val_loss: 0.2724\n",
            "Epoch 162/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1736 - val_loss: 0.2725\n",
            "Epoch 163/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1735 - val_loss: 0.2725\n",
            "Epoch 164/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1734 - val_loss: 0.2726\n",
            "Epoch 165/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1733 - val_loss: 0.2727\n",
            "Epoch 166/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1734 - val_loss: 0.2727\n",
            "Epoch 167/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1729 - val_loss: 0.2728\n",
            "Epoch 168/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1726 - val_loss: 0.2728\n",
            "Epoch 169/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1727 - val_loss: 0.2729\n",
            "Epoch 170/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2730\n",
            "Epoch 171/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1721 - val_loss: 0.2730\n",
            "Epoch 172/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1721 - val_loss: 0.2730\n",
            "Epoch 173/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1720 - val_loss: 0.2730\n",
            "Epoch 174/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1715 - val_loss: 0.2732\n",
            "Epoch 175/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1718 - val_loss: 0.2732\n",
            "Epoch 176/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1714 - val_loss: 0.2733\n",
            "Epoch 177/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1712 - val_loss: 0.2733\n",
            "Epoch 178/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1713 - val_loss: 0.2734\n",
            "Epoch 179/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1710 - val_loss: 0.2734\n",
            "Epoch 180/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1710 - val_loss: 0.2734\n",
            "Epoch 181/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2735\n",
            "Epoch 182/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2735\n",
            "Epoch 183/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2736\n",
            "Epoch 184/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1709 - val_loss: 0.2737\n",
            "Epoch 185/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2737\n",
            "Epoch 186/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2738\n",
            "Epoch 187/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1702 - val_loss: 0.2738\n",
            "Epoch 188/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1701 - val_loss: 0.2739\n",
            "Epoch 189/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1697 - val_loss: 0.2739\n",
            "Epoch 190/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1701 - val_loss: 0.2740\n",
            "Epoch 191/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1698 - val_loss: 0.2740\n",
            "Epoch 192/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1698 - val_loss: 0.2741\n",
            "Epoch 193/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1694 - val_loss: 0.2741\n",
            "Epoch 194/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1693 - val_loss: 0.2742\n",
            "Epoch 195/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1691 - val_loss: 0.2742\n",
            "Epoch 196/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1692 - val_loss: 0.2743\n",
            "Epoch 197/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1692 - val_loss: 0.2743\n",
            "Epoch 198/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1689 - val_loss: 0.2744\n",
            "Epoch 199/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1688 - val_loss: 0.2744\n",
            "Epoch 200/200\n",
            "265/265 [==============================] - 2s 6ms/step - loss: 0.1689 - val_loss: 0.2744\n",
            "19/19 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 128, 0.16889573633670807, 0.27438896894454956, 0.029705096, 3.7976828570403187, 0.22783449581094445)\n",
            "Epoch 1/200\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.4188 - val_loss: 0.3945\n",
            "Epoch 2/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3240 - val_loss: 0.3409\n",
            "Epoch 3/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2960 - val_loss: 0.3186\n",
            "Epoch 4/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2809 - val_loss: 0.3055\n",
            "Epoch 5/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2715 - val_loss: 0.2973\n",
            "Epoch 6/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2659 - val_loss: 0.2923\n",
            "Epoch 7/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2625 - val_loss: 0.2895\n",
            "Epoch 8/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2604 - val_loss: 0.2878\n",
            "Epoch 9/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2593 - val_loss: 0.2865\n",
            "Epoch 10/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2583 - val_loss: 0.2857\n",
            "Epoch 11/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2577 - val_loss: 0.2851\n",
            "Epoch 12/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2568 - val_loss: 0.2847\n",
            "Epoch 13/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2562 - val_loss: 0.2842\n",
            "Epoch 14/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2555 - val_loss: 0.2837\n",
            "Epoch 15/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2548 - val_loss: 0.2833\n",
            "Epoch 16/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2541 - val_loss: 0.2829\n",
            "Epoch 17/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2531 - val_loss: 0.2825\n",
            "Epoch 18/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2523 - val_loss: 0.2822\n",
            "Epoch 19/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2513 - val_loss: 0.2818\n",
            "Epoch 20/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2505 - val_loss: 0.2814\n",
            "Epoch 21/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2495 - val_loss: 0.2810\n",
            "Epoch 22/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2482 - val_loss: 0.2806\n",
            "Epoch 23/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2472 - val_loss: 0.2802\n",
            "Epoch 24/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2463 - val_loss: 0.2799\n",
            "Epoch 25/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2451 - val_loss: 0.2794\n",
            "Epoch 26/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2440 - val_loss: 0.2792\n",
            "Epoch 27/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2428 - val_loss: 0.2788\n",
            "Epoch 28/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2415 - val_loss: 0.2784\n",
            "Epoch 29/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2404 - val_loss: 0.2779\n",
            "Epoch 30/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2391 - val_loss: 0.2777\n",
            "Epoch 31/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2381 - val_loss: 0.2773\n",
            "Epoch 32/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2371 - val_loss: 0.2769\n",
            "Epoch 33/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2358 - val_loss: 0.2766\n",
            "Epoch 34/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2346 - val_loss: 0.2763\n",
            "Epoch 35/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2336 - val_loss: 0.2760\n",
            "Epoch 36/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2323 - val_loss: 0.2757\n",
            "Epoch 37/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2313 - val_loss: 0.2754\n",
            "Epoch 38/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2300 - val_loss: 0.2752\n",
            "Epoch 39/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2293 - val_loss: 0.2749\n",
            "Epoch 40/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2280 - val_loss: 0.2747\n",
            "Epoch 41/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2270 - val_loss: 0.2743\n",
            "Epoch 42/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2259 - val_loss: 0.2741\n",
            "Epoch 43/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2248 - val_loss: 0.2739\n",
            "Epoch 44/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2237 - val_loss: 0.2737\n",
            "Epoch 45/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2229 - val_loss: 0.2735\n",
            "Epoch 46/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2220 - val_loss: 0.2733\n",
            "Epoch 47/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2211 - val_loss: 0.2731\n",
            "Epoch 48/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2199 - val_loss: 0.2730\n",
            "Epoch 49/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2189 - val_loss: 0.2728\n",
            "Epoch 50/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2183 - val_loss: 0.2726\n",
            "Epoch 51/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2172 - val_loss: 0.2724\n",
            "Epoch 52/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2167 - val_loss: 0.2724\n",
            "Epoch 53/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2156 - val_loss: 0.2722\n",
            "Epoch 54/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2146 - val_loss: 0.2721\n",
            "Epoch 55/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2141 - val_loss: 0.2719\n",
            "Epoch 56/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2132 - val_loss: 0.2719\n",
            "Epoch 57/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2123 - val_loss: 0.2717\n",
            "Epoch 58/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2115 - val_loss: 0.2716\n",
            "Epoch 59/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2108 - val_loss: 0.2716\n",
            "Epoch 60/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2101 - val_loss: 0.2715\n",
            "Epoch 61/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2092 - val_loss: 0.2714\n",
            "Epoch 62/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2086 - val_loss: 0.2713\n",
            "Epoch 63/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2079 - val_loss: 0.2713\n",
            "Epoch 64/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2069 - val_loss: 0.2713\n",
            "Epoch 65/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2066 - val_loss: 0.2712\n",
            "Epoch 66/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2058 - val_loss: 0.2712\n",
            "Epoch 67/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2053 - val_loss: 0.2711\n",
            "Epoch 68/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2044 - val_loss: 0.2711\n",
            "Epoch 69/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2040 - val_loss: 0.2710\n",
            "Epoch 70/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2034 - val_loss: 0.2710\n",
            "Epoch 71/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2025 - val_loss: 0.2709\n",
            "Epoch 72/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2020 - val_loss: 0.2709\n",
            "Epoch 73/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2014 - val_loss: 0.2709\n",
            "Epoch 74/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2008 - val_loss: 0.2710\n",
            "Epoch 75/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2001 - val_loss: 0.2708\n",
            "Epoch 76/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1996 - val_loss: 0.2709\n",
            "Epoch 77/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1994 - val_loss: 0.2709\n",
            "Epoch 78/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1988 - val_loss: 0.2708\n",
            "Epoch 79/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1982 - val_loss: 0.2708\n",
            "Epoch 80/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1976 - val_loss: 0.2709\n",
            "Epoch 81/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1973 - val_loss: 0.2709\n",
            "Epoch 82/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1964 - val_loss: 0.2708\n",
            "Epoch 83/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1961 - val_loss: 0.2709\n",
            "Epoch 84/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1956 - val_loss: 0.2709\n",
            "Epoch 85/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1951 - val_loss: 0.2709\n",
            "Epoch 86/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1948 - val_loss: 0.2709\n",
            "Epoch 87/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1945 - val_loss: 0.2709\n",
            "Epoch 88/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1937 - val_loss: 0.2710\n",
            "Epoch 89/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1937 - val_loss: 0.2710\n",
            "Epoch 90/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1932 - val_loss: 0.2710\n",
            "Epoch 91/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1925 - val_loss: 0.2710\n",
            "Epoch 92/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1922 - val_loss: 0.2710\n",
            "Epoch 93/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1919 - val_loss: 0.2711\n",
            "Epoch 94/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1913 - val_loss: 0.2710\n",
            "Epoch 95/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1910 - val_loss: 0.2712\n",
            "Epoch 96/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1906 - val_loss: 0.2711\n",
            "Epoch 97/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1903 - val_loss: 0.2712\n",
            "Epoch 98/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1901 - val_loss: 0.2711\n",
            "Epoch 99/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1897 - val_loss: 0.2712\n",
            "Epoch 100/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1890 - val_loss: 0.2713\n",
            "Epoch 101/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1887 - val_loss: 0.2712\n",
            "Epoch 102/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1883 - val_loss: 0.2713\n",
            "Epoch 103/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1881 - val_loss: 0.2715\n",
            "Epoch 104/200\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.1877 - val_loss: 0.2715\n",
            "Epoch 105/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1874 - val_loss: 0.2715\n",
            "Epoch 106/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1869 - val_loss: 0.2715\n",
            "Epoch 107/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1865 - val_loss: 0.2715\n",
            "Epoch 108/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1863 - val_loss: 0.2717\n",
            "Epoch 109/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1861 - val_loss: 0.2716\n",
            "Epoch 110/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1855 - val_loss: 0.2716\n",
            "Epoch 111/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1856 - val_loss: 0.2716\n",
            "Epoch 112/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1849 - val_loss: 0.2718\n",
            "Epoch 113/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1848 - val_loss: 0.2718\n",
            "Epoch 114/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1845 - val_loss: 0.2718\n",
            "Epoch 115/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1841 - val_loss: 0.2719\n",
            "Epoch 116/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1841 - val_loss: 0.2720\n",
            "Epoch 117/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1834 - val_loss: 0.2719\n",
            "Epoch 118/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1834 - val_loss: 0.2722\n",
            "Epoch 119/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1831 - val_loss: 0.2721\n",
            "Epoch 120/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1825 - val_loss: 0.2721\n",
            "Epoch 121/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1826 - val_loss: 0.2722\n",
            "Epoch 122/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1823 - val_loss: 0.2722\n",
            "Epoch 123/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1823 - val_loss: 0.2723\n",
            "Epoch 124/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1818 - val_loss: 0.2723\n",
            "Epoch 125/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1815 - val_loss: 0.2723\n",
            "Epoch 126/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1814 - val_loss: 0.2723\n",
            "Epoch 127/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1811 - val_loss: 0.2724\n",
            "Epoch 128/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1809 - val_loss: 0.2725\n",
            "Epoch 129/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1808 - val_loss: 0.2725\n",
            "Epoch 130/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1803 - val_loss: 0.2726\n",
            "Epoch 131/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1799 - val_loss: 0.2727\n",
            "Epoch 132/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2726\n",
            "Epoch 133/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1798 - val_loss: 0.2726\n",
            "Epoch 134/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1797 - val_loss: 0.2727\n",
            "Epoch 135/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1792 - val_loss: 0.2729\n",
            "Epoch 136/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1790 - val_loss: 0.2729\n",
            "Epoch 137/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1790 - val_loss: 0.2729\n",
            "Epoch 138/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1786 - val_loss: 0.2729\n",
            "Epoch 139/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1785 - val_loss: 0.2731\n",
            "Epoch 140/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1781 - val_loss: 0.2730\n",
            "Epoch 141/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1782 - val_loss: 0.2732\n",
            "Epoch 142/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1779 - val_loss: 0.2731\n",
            "Epoch 143/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1776 - val_loss: 0.2732\n",
            "Epoch 144/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1775 - val_loss: 0.2733\n",
            "Epoch 145/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1771 - val_loss: 0.2733\n",
            "Epoch 146/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1767 - val_loss: 0.2734\n",
            "Epoch 147/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1770 - val_loss: 0.2734\n",
            "Epoch 148/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1768 - val_loss: 0.2734\n",
            "Epoch 149/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1766 - val_loss: 0.2735\n",
            "Epoch 150/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1762 - val_loss: 0.2735\n",
            "Epoch 151/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1760 - val_loss: 0.2736\n",
            "Epoch 152/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1759 - val_loss: 0.2736\n",
            "Epoch 153/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1758 - val_loss: 0.2737\n",
            "Epoch 154/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1755 - val_loss: 0.2737\n",
            "Epoch 155/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1753 - val_loss: 0.2738\n",
            "Epoch 156/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1751 - val_loss: 0.2739\n",
            "Epoch 157/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1751 - val_loss: 0.2739\n",
            "Epoch 158/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1749 - val_loss: 0.2740\n",
            "Epoch 159/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1747 - val_loss: 0.2740\n",
            "Epoch 160/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1747 - val_loss: 0.2740\n",
            "Epoch 161/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1744 - val_loss: 0.2741\n",
            "Epoch 162/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1742 - val_loss: 0.2741\n",
            "Epoch 163/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1742 - val_loss: 0.2742\n",
            "Epoch 164/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1739 - val_loss: 0.2741\n",
            "Epoch 165/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1736 - val_loss: 0.2743\n",
            "Epoch 166/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1735 - val_loss: 0.2743\n",
            "Epoch 167/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1732 - val_loss: 0.2744\n",
            "Epoch 168/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1734 - val_loss: 0.2744\n",
            "Epoch 169/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1730 - val_loss: 0.2745\n",
            "Epoch 170/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1730 - val_loss: 0.2745\n",
            "Epoch 171/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1730 - val_loss: 0.2747\n",
            "Epoch 172/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1731 - val_loss: 0.2746\n",
            "Epoch 173/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2747\n",
            "Epoch 174/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2748\n",
            "Epoch 175/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1725 - val_loss: 0.2748\n",
            "Epoch 176/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2748\n",
            "Epoch 177/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1719 - val_loss: 0.2749\n",
            "Epoch 178/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1723 - val_loss: 0.2749\n",
            "Epoch 179/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1721 - val_loss: 0.2750\n",
            "Epoch 180/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1718 - val_loss: 0.2751\n",
            "Epoch 181/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1719 - val_loss: 0.2751\n",
            "Epoch 182/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1715 - val_loss: 0.2752\n",
            "Epoch 183/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1715 - val_loss: 0.2752\n",
            "Epoch 184/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1714 - val_loss: 0.2752\n",
            "Epoch 185/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1712 - val_loss: 0.2753\n",
            "Epoch 186/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2753\n",
            "Epoch 187/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2755\n",
            "Epoch 188/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1706 - val_loss: 0.2755\n",
            "Epoch 189/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2754\n",
            "Epoch 190/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1704 - val_loss: 0.2756\n",
            "Epoch 191/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1703 - val_loss: 0.2756\n",
            "Epoch 192/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1704 - val_loss: 0.2757\n",
            "Epoch 193/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1702 - val_loss: 0.2757\n",
            "Epoch 194/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1703 - val_loss: 0.2758\n",
            "Epoch 195/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1699 - val_loss: 0.2757\n",
            "Epoch 196/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1698 - val_loss: 0.2758\n",
            "Epoch 197/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1697 - val_loss: 0.2759\n",
            "Epoch 198/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1698 - val_loss: 0.2759\n",
            "Epoch 199/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1697 - val_loss: 0.2759\n",
            "Epoch 200/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1696 - val_loss: 0.2760\n",
            "19/19 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 128, 0.16958701610565186, 0.2759590446949005, 0.03347822, 3.8103528901047086, 0.23678921816434767)\n",
            "Epoch 1/200\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.4170 - val_loss: 0.3961\n",
            "Epoch 2/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3243 - val_loss: 0.3422\n",
            "Epoch 3/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2962 - val_loss: 0.3200\n",
            "Epoch 4/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2811 - val_loss: 0.3063\n",
            "Epoch 5/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2717 - val_loss: 0.2979\n",
            "Epoch 6/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2660 - val_loss: 0.2927\n",
            "Epoch 7/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2624 - val_loss: 0.2899\n",
            "Epoch 8/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2606 - val_loss: 0.2879\n",
            "Epoch 9/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2594 - val_loss: 0.2865\n",
            "Epoch 10/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2581 - val_loss: 0.2859\n",
            "Epoch 11/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2574 - val_loss: 0.2852\n",
            "Epoch 12/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2567 - val_loss: 0.2846\n",
            "Epoch 13/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2560 - val_loss: 0.2842\n",
            "Epoch 14/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2553 - val_loss: 0.2837\n",
            "Epoch 15/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2547 - val_loss: 0.2832\n",
            "Epoch 16/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2538 - val_loss: 0.2829\n",
            "Epoch 17/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2529 - val_loss: 0.2824\n",
            "Epoch 18/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2523 - val_loss: 0.2821\n",
            "Epoch 19/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2513 - val_loss: 0.2819\n",
            "Epoch 20/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2504 - val_loss: 0.2814\n",
            "Epoch 21/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2495 - val_loss: 0.2810\n",
            "Epoch 22/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2484 - val_loss: 0.2806\n",
            "Epoch 23/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2473 - val_loss: 0.2802\n",
            "Epoch 24/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2460 - val_loss: 0.2798\n",
            "Epoch 25/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2452 - val_loss: 0.2794\n",
            "Epoch 26/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2440 - val_loss: 0.2791\n",
            "Epoch 27/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2425 - val_loss: 0.2787\n",
            "Epoch 28/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2412 - val_loss: 0.2784\n",
            "Epoch 29/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2401 - val_loss: 0.2780\n",
            "Epoch 30/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2390 - val_loss: 0.2775\n",
            "Epoch 31/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2380 - val_loss: 0.2772\n",
            "Epoch 32/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2367 - val_loss: 0.2768\n",
            "Epoch 33/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2354 - val_loss: 0.2766\n",
            "Epoch 34/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2343 - val_loss: 0.2763\n",
            "Epoch 35/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2330 - val_loss: 0.2757\n",
            "Epoch 36/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2321 - val_loss: 0.2755\n",
            "Epoch 37/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2308 - val_loss: 0.2752\n",
            "Epoch 38/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2297 - val_loss: 0.2749\n",
            "Epoch 39/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2287 - val_loss: 0.2747\n",
            "Epoch 40/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2275 - val_loss: 0.2742\n",
            "Epoch 41/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2266 - val_loss: 0.2740\n",
            "Epoch 42/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2254 - val_loss: 0.2737\n",
            "Epoch 43/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2243 - val_loss: 0.2736\n",
            "Epoch 44/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2234 - val_loss: 0.2734\n",
            "Epoch 45/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2225 - val_loss: 0.2731\n",
            "Epoch 46/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2215 - val_loss: 0.2728\n",
            "Epoch 47/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2205 - val_loss: 0.2727\n",
            "Epoch 48/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2198 - val_loss: 0.2727\n",
            "Epoch 49/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2188 - val_loss: 0.2726\n",
            "Epoch 50/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2178 - val_loss: 0.2722\n",
            "Epoch 51/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2168 - val_loss: 0.2719\n",
            "Epoch 52/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2162 - val_loss: 0.2718\n",
            "Epoch 53/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2152 - val_loss: 0.2719\n",
            "Epoch 54/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2145 - val_loss: 0.2716\n",
            "Epoch 55/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2139 - val_loss: 0.2714\n",
            "Epoch 56/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2128 - val_loss: 0.2713\n",
            "Epoch 57/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2123 - val_loss: 0.2713\n",
            "Epoch 58/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2112 - val_loss: 0.2712\n",
            "Epoch 59/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2106 - val_loss: 0.2711\n",
            "Epoch 60/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2100 - val_loss: 0.2713\n",
            "Epoch 61/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2092 - val_loss: 0.2710\n",
            "Epoch 62/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2086 - val_loss: 0.2707\n",
            "Epoch 63/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2077 - val_loss: 0.2707\n",
            "Epoch 64/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2073 - val_loss: 0.2707\n",
            "Epoch 65/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2065 - val_loss: 0.2707\n",
            "Epoch 66/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2059 - val_loss: 0.2708\n",
            "Epoch 67/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2051 - val_loss: 0.2705\n",
            "Epoch 68/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2048 - val_loss: 0.2715\n",
            "Epoch 69/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2039 - val_loss: 0.2709\n",
            "Epoch 70/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2032 - val_loss: 0.2704\n",
            "Epoch 71/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2027 - val_loss: 0.2702\n",
            "Epoch 72/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2022 - val_loss: 0.2702\n",
            "Epoch 73/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2014 - val_loss: 0.2705\n",
            "Epoch 74/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2013 - val_loss: 0.2704\n",
            "Epoch 75/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2004 - val_loss: 0.2703\n",
            "Epoch 76/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1998 - val_loss: 0.2701\n",
            "Epoch 77/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1995 - val_loss: 0.2702\n",
            "Epoch 78/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1989 - val_loss: 0.2701\n",
            "Epoch 79/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1985 - val_loss: 0.2701\n",
            "Epoch 80/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1977 - val_loss: 0.2705\n",
            "Epoch 81/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1972 - val_loss: 0.2703\n",
            "Epoch 82/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1967 - val_loss: 0.2703\n",
            "Epoch 83/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1966 - val_loss: 0.2703\n",
            "Epoch 84/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1960 - val_loss: 0.2703\n",
            "Epoch 85/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1954 - val_loss: 0.2702\n",
            "Epoch 86/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1949 - val_loss: 0.2701\n",
            "Epoch 87/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1943 - val_loss: 0.2701\n",
            "Epoch 88/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1940 - val_loss: 0.2703\n",
            "Epoch 89/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1936 - val_loss: 0.2705\n",
            "Epoch 90/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1932 - val_loss: 0.2703\n",
            "Epoch 91/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1929 - val_loss: 0.2705\n",
            "Epoch 92/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1924 - val_loss: 0.2704\n",
            "Epoch 93/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1922 - val_loss: 0.2705\n",
            "Epoch 94/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1915 - val_loss: 0.2704\n",
            "Epoch 95/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1912 - val_loss: 0.2707\n",
            "Epoch 96/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1906 - val_loss: 0.2702\n",
            "Epoch 97/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1905 - val_loss: 0.2705\n",
            "Epoch 98/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1899 - val_loss: 0.2702\n",
            "Epoch 99/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1894 - val_loss: 0.2704\n",
            "Epoch 100/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1892 - val_loss: 0.2703\n",
            "Epoch 101/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1888 - val_loss: 0.2710\n",
            "Epoch 102/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1884 - val_loss: 0.2704\n",
            "Epoch 103/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1880 - val_loss: 0.2712\n",
            "Epoch 104/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1879 - val_loss: 0.2705\n",
            "Epoch 105/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1877 - val_loss: 0.2711\n",
            "Epoch 106/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1872 - val_loss: 0.2707\n",
            "Epoch 107/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1870 - val_loss: 0.2710\n",
            "Epoch 108/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1862 - val_loss: 0.2709\n",
            "Epoch 109/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1861 - val_loss: 0.2708\n",
            "Epoch 110/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1859 - val_loss: 0.2708\n",
            "Epoch 111/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1857 - val_loss: 0.2707\n",
            "Epoch 112/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1853 - val_loss: 0.2710\n",
            "Epoch 113/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1850 - val_loss: 0.2709\n",
            "Epoch 114/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1847 - val_loss: 0.2710\n",
            "Epoch 115/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1844 - val_loss: 0.2713\n",
            "Epoch 116/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1840 - val_loss: 0.2709\n",
            "Epoch 117/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1839 - val_loss: 0.2710\n",
            "Epoch 118/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1834 - val_loss: 0.2710\n",
            "Epoch 119/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1833 - val_loss: 0.2711\n",
            "Epoch 120/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1829 - val_loss: 0.2712\n",
            "Epoch 121/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1827 - val_loss: 0.2713\n",
            "Epoch 122/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1825 - val_loss: 0.2712\n",
            "Epoch 123/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1821 - val_loss: 0.2713\n",
            "Epoch 124/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1820 - val_loss: 0.2713\n",
            "Epoch 125/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1817 - val_loss: 0.2715\n",
            "Epoch 126/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1814 - val_loss: 0.2714\n",
            "Epoch 127/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1813 - val_loss: 0.2716\n",
            "Epoch 128/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1807 - val_loss: 0.2713\n",
            "Epoch 129/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1809 - val_loss: 0.2719\n",
            "Epoch 130/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1803 - val_loss: 0.2715\n",
            "Epoch 131/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2718\n",
            "Epoch 132/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1800 - val_loss: 0.2717\n",
            "Epoch 133/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1801 - val_loss: 0.2716\n",
            "Epoch 134/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1796 - val_loss: 0.2718\n",
            "Epoch 135/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1793 - val_loss: 0.2716\n",
            "Epoch 136/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1794 - val_loss: 0.2718\n",
            "Epoch 137/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1791 - val_loss: 0.2722\n",
            "Epoch 138/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1787 - val_loss: 0.2722\n",
            "Epoch 139/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1783 - val_loss: 0.2721\n",
            "Epoch 140/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1782 - val_loss: 0.2720\n",
            "Epoch 141/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1779 - val_loss: 0.2720\n",
            "Epoch 142/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1777 - val_loss: 0.2720\n",
            "Epoch 143/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1779 - val_loss: 0.2725\n",
            "Epoch 144/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1774 - val_loss: 0.2723\n",
            "Epoch 145/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1774 - val_loss: 0.2722\n",
            "Epoch 146/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1769 - val_loss: 0.2726\n",
            "Epoch 147/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1769 - val_loss: 0.2725\n",
            "Epoch 148/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1768 - val_loss: 0.2723\n",
            "Epoch 149/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1766 - val_loss: 0.2732\n",
            "Epoch 150/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1764 - val_loss: 0.2728\n",
            "Epoch 151/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1764 - val_loss: 0.2724\n",
            "Epoch 152/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1762 - val_loss: 0.2725\n",
            "Epoch 153/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1759 - val_loss: 0.2730\n",
            "Epoch 154/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1754 - val_loss: 0.2725\n",
            "Epoch 155/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1754 - val_loss: 0.2726\n",
            "Epoch 156/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1752 - val_loss: 0.2727\n",
            "Epoch 157/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1751 - val_loss: 0.2727\n",
            "Epoch 158/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1749 - val_loss: 0.2730\n",
            "Epoch 159/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1750 - val_loss: 0.2729\n",
            "Epoch 160/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1744 - val_loss: 0.2730\n",
            "Epoch 161/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1748 - val_loss: 0.2730\n",
            "Epoch 162/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1742 - val_loss: 0.2732\n",
            "Epoch 163/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1742 - val_loss: 0.2731\n",
            "Epoch 164/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1743 - val_loss: 0.2730\n",
            "Epoch 165/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1739 - val_loss: 0.2732\n",
            "Epoch 166/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1739 - val_loss: 0.2733\n",
            "Epoch 167/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1736 - val_loss: 0.2732\n",
            "Epoch 168/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1734 - val_loss: 0.2734\n",
            "Epoch 169/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1732 - val_loss: 0.2736\n",
            "Epoch 170/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1729 - val_loss: 0.2737\n",
            "Epoch 171/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1731 - val_loss: 0.2738\n",
            "Epoch 172/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1727 - val_loss: 0.2734\n",
            "Epoch 173/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1728 - val_loss: 0.2735\n",
            "Epoch 174/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1727 - val_loss: 0.2741\n",
            "Epoch 175/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2735\n",
            "Epoch 176/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1724 - val_loss: 0.2740\n",
            "Epoch 177/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1725 - val_loss: 0.2738\n",
            "Epoch 178/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1723 - val_loss: 0.2738\n",
            "Epoch 179/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1718 - val_loss: 0.2737\n",
            "Epoch 180/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1718 - val_loss: 0.2738\n",
            "Epoch 181/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1715 - val_loss: 0.2740\n",
            "Epoch 182/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1717 - val_loss: 0.2739\n",
            "Epoch 183/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1715 - val_loss: 0.2738\n",
            "Epoch 184/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1714 - val_loss: 0.2741\n",
            "Epoch 185/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1714 - val_loss: 0.2740\n",
            "Epoch 186/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1711 - val_loss: 0.2741\n",
            "Epoch 187/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2742\n",
            "Epoch 188/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1708 - val_loss: 0.2744\n",
            "Epoch 189/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2744\n",
            "Epoch 190/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2744\n",
            "Epoch 191/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1707 - val_loss: 0.2745\n",
            "Epoch 192/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1705 - val_loss: 0.2743\n",
            "Epoch 193/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1702 - val_loss: 0.2748\n",
            "Epoch 194/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1702 - val_loss: 0.2746\n",
            "Epoch 195/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1701 - val_loss: 0.2745\n",
            "Epoch 196/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1699 - val_loss: 0.2745\n",
            "Epoch 197/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1699 - val_loss: 0.2748\n",
            "Epoch 198/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1695 - val_loss: 0.2754\n",
            "Epoch 199/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1698 - val_loss: 0.2752\n",
            "Epoch 200/200\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.1694 - val_loss: 0.2747\n",
            "19/19 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 128, 0.16942523419857025, 0.27472010254859924, 0.033681028, 3.7343160600186325, 0.2153359372546561)\n",
            "Epoch 1/200\n",
            "131/131 [==============================] - 2s 10ms/step - loss: 0.4724 - val_loss: 0.4729\n",
            "Epoch 2/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.3631 - val_loss: 0.3980\n",
            "Epoch 3/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.3330 - val_loss: 0.3629\n",
            "Epoch 4/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.3132 - val_loss: 0.3436\n",
            "Epoch 5/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2997 - val_loss: 0.3303\n",
            "Epoch 6/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2901 - val_loss: 0.3207\n",
            "Epoch 7/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2828 - val_loss: 0.3131\n",
            "Epoch 8/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2769 - val_loss: 0.3072\n",
            "Epoch 9/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2724 - val_loss: 0.3027\n",
            "Epoch 10/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2688 - val_loss: 0.2989\n",
            "Epoch 11/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2657 - val_loss: 0.2960\n",
            "Epoch 12/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2636 - val_loss: 0.2938\n",
            "Epoch 13/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2619 - val_loss: 0.2921\n",
            "Epoch 14/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2606 - val_loss: 0.2908\n",
            "Epoch 15/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2595 - val_loss: 0.2898\n",
            "Epoch 16/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2586 - val_loss: 0.2891\n",
            "Epoch 17/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2579 - val_loss: 0.2884\n",
            "Epoch 18/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2575 - val_loss: 0.2879\n",
            "Epoch 19/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2571 - val_loss: 0.2875\n",
            "Epoch 20/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2566 - val_loss: 0.2871\n",
            "Epoch 21/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2561 - val_loss: 0.2868\n",
            "Epoch 22/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2560 - val_loss: 0.2865\n",
            "Epoch 23/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2556 - val_loss: 0.2862\n",
            "Epoch 24/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2551 - val_loss: 0.2860\n",
            "Epoch 25/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2550 - val_loss: 0.2857\n",
            "Epoch 26/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2546 - val_loss: 0.2855\n",
            "Epoch 27/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2543 - val_loss: 0.2853\n",
            "Epoch 28/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2538 - val_loss: 0.2851\n",
            "Epoch 29/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2536 - val_loss: 0.2848\n",
            "Epoch 30/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2534 - val_loss: 0.2848\n",
            "Epoch 31/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2530 - val_loss: 0.2845\n",
            "Epoch 32/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2527 - val_loss: 0.2843\n",
            "Epoch 33/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2524 - val_loss: 0.2841\n",
            "Epoch 34/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2518 - val_loss: 0.2840\n",
            "Epoch 35/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2515 - val_loss: 0.2838\n",
            "Epoch 36/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2511 - val_loss: 0.2836\n",
            "Epoch 37/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2507 - val_loss: 0.2834\n",
            "Epoch 38/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2502 - val_loss: 0.2833\n",
            "Epoch 39/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2498 - val_loss: 0.2831\n",
            "Epoch 40/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2494 - val_loss: 0.2829\n",
            "Epoch 41/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2488 - val_loss: 0.2827\n",
            "Epoch 42/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2484 - val_loss: 0.2825\n",
            "Epoch 43/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2478 - val_loss: 0.2823\n",
            "Epoch 44/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2471 - val_loss: 0.2821\n",
            "Epoch 45/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2469 - val_loss: 0.2820\n",
            "Epoch 46/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2461 - val_loss: 0.2818\n",
            "Epoch 47/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2457 - val_loss: 0.2816\n",
            "Epoch 48/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2451 - val_loss: 0.2813\n",
            "Epoch 49/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2445 - val_loss: 0.2811\n",
            "Epoch 50/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2439 - val_loss: 0.2810\n",
            "Epoch 51/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2432 - val_loss: 0.2808\n",
            "Epoch 52/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2428 - val_loss: 0.2806\n",
            "Epoch 53/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2421 - val_loss: 0.2804\n",
            "Epoch 54/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2416 - val_loss: 0.2802\n",
            "Epoch 55/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2409 - val_loss: 0.2800\n",
            "Epoch 56/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2403 - val_loss: 0.2798\n",
            "Epoch 57/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2399 - val_loss: 0.2796\n",
            "Epoch 58/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2392 - val_loss: 0.2794\n",
            "Epoch 59/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2386 - val_loss: 0.2792\n",
            "Epoch 60/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2379 - val_loss: 0.2790\n",
            "Epoch 61/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2374 - val_loss: 0.2789\n",
            "Epoch 62/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2369 - val_loss: 0.2787\n",
            "Epoch 63/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2362 - val_loss: 0.2785\n",
            "Epoch 64/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2357 - val_loss: 0.2784\n",
            "Epoch 65/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2351 - val_loss: 0.2781\n",
            "Epoch 66/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2343 - val_loss: 0.2780\n",
            "Epoch 67/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2339 - val_loss: 0.2778\n",
            "Epoch 68/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2333 - val_loss: 0.2777\n",
            "Epoch 69/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2327 - val_loss: 0.2775\n",
            "Epoch 70/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2321 - val_loss: 0.2773\n",
            "Epoch 71/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2317 - val_loss: 0.2772\n",
            "Epoch 72/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2310 - val_loss: 0.2771\n",
            "Epoch 73/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2306 - val_loss: 0.2769\n",
            "Epoch 74/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2298 - val_loss: 0.2767\n",
            "Epoch 75/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2292 - val_loss: 0.2765\n",
            "Epoch 76/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2288 - val_loss: 0.2765\n",
            "Epoch 77/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2282 - val_loss: 0.2763\n",
            "Epoch 78/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2277 - val_loss: 0.2762\n",
            "Epoch 79/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2271 - val_loss: 0.2760\n",
            "Epoch 80/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2265 - val_loss: 0.2759\n",
            "Epoch 81/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2260 - val_loss: 0.2758\n",
            "Epoch 82/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2256 - val_loss: 0.2757\n",
            "Epoch 83/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2250 - val_loss: 0.2755\n",
            "Epoch 84/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2245 - val_loss: 0.2754\n",
            "Epoch 85/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2241 - val_loss: 0.2753\n",
            "Epoch 86/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2234 - val_loss: 0.2751\n",
            "Epoch 87/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2229 - val_loss: 0.2751\n",
            "Epoch 88/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2226 - val_loss: 0.2749\n",
            "Epoch 89/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2220 - val_loss: 0.2748\n",
            "Epoch 90/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2216 - val_loss: 0.2747\n",
            "Epoch 91/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2209 - val_loss: 0.2746\n",
            "Epoch 92/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2206 - val_loss: 0.2745\n",
            "Epoch 93/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2199 - val_loss: 0.2745\n",
            "Epoch 94/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2194 - val_loss: 0.2744\n",
            "Epoch 95/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.2743\n",
            "Epoch 96/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2185 - val_loss: 0.2742\n",
            "Epoch 97/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2182 - val_loss: 0.2741\n",
            "Epoch 98/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2176 - val_loss: 0.2740\n",
            "Epoch 99/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2171 - val_loss: 0.2739\n",
            "Epoch 100/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2167 - val_loss: 0.2739\n",
            "Epoch 101/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2161 - val_loss: 0.2738\n",
            "Epoch 102/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2159 - val_loss: 0.2737\n",
            "Epoch 103/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2152 - val_loss: 0.2736\n",
            "Epoch 104/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2148 - val_loss: 0.2736\n",
            "Epoch 105/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2144 - val_loss: 0.2735\n",
            "Epoch 106/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2141 - val_loss: 0.2734\n",
            "Epoch 107/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2133 - val_loss: 0.2733\n",
            "Epoch 108/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2133 - val_loss: 0.2733\n",
            "Epoch 109/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2126 - val_loss: 0.2733\n",
            "Epoch 110/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2122 - val_loss: 0.2732\n",
            "Epoch 111/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2118 - val_loss: 0.2732\n",
            "Epoch 112/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2114 - val_loss: 0.2731\n",
            "Epoch 113/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2109 - val_loss: 0.2731\n",
            "Epoch 114/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2109 - val_loss: 0.2730\n",
            "Epoch 115/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2102 - val_loss: 0.2730\n",
            "Epoch 116/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2098 - val_loss: 0.2729\n",
            "Epoch 117/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2093 - val_loss: 0.2728\n",
            "Epoch 118/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2090 - val_loss: 0.2728\n",
            "Epoch 119/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2086 - val_loss: 0.2728\n",
            "Epoch 120/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2083 - val_loss: 0.2728\n",
            "Epoch 121/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2080 - val_loss: 0.2727\n",
            "Epoch 122/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2076 - val_loss: 0.2727\n",
            "Epoch 123/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2072 - val_loss: 0.2726\n",
            "Epoch 124/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2068 - val_loss: 0.2725\n",
            "Epoch 125/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2067 - val_loss: 0.2726\n",
            "Epoch 126/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2061 - val_loss: 0.2725\n",
            "Epoch 127/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2057 - val_loss: 0.2725\n",
            "Epoch 128/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2052 - val_loss: 0.2726\n",
            "Epoch 129/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.2724\n",
            "Epoch 130/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2045 - val_loss: 0.2724\n",
            "Epoch 131/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2041 - val_loss: 0.2724\n",
            "Epoch 132/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2040 - val_loss: 0.2723\n",
            "Epoch 133/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2036 - val_loss: 0.2723\n",
            "Epoch 134/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2032 - val_loss: 0.2723\n",
            "Epoch 135/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2029 - val_loss: 0.2723\n",
            "Epoch 136/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.2026 - val_loss: 0.2724\n",
            "Epoch 137/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2023 - val_loss: 0.2723\n",
            "Epoch 138/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2020 - val_loss: 0.2724\n",
            "Epoch 139/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2017 - val_loss: 0.2722\n",
            "Epoch 140/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.2723\n",
            "Epoch 141/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2010 - val_loss: 0.2722\n",
            "Epoch 142/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2005 - val_loss: 0.2722\n",
            "Epoch 143/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2005 - val_loss: 0.2722\n",
            "Epoch 144/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.2001 - val_loss: 0.2722\n",
            "Epoch 145/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1996 - val_loss: 0.2721\n",
            "Epoch 146/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1994 - val_loss: 0.2722\n",
            "Epoch 147/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1992 - val_loss: 0.2721\n",
            "Epoch 148/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1988 - val_loss: 0.2721\n",
            "Epoch 149/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1984 - val_loss: 0.2721\n",
            "Epoch 150/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1983 - val_loss: 0.2721\n",
            "Epoch 151/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1980 - val_loss: 0.2722\n",
            "Epoch 152/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1976 - val_loss: 0.2721\n",
            "Epoch 153/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1976 - val_loss: 0.2721\n",
            "Epoch 154/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1973 - val_loss: 0.2721\n",
            "Epoch 155/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1968 - val_loss: 0.2721\n",
            "Epoch 156/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1967 - val_loss: 0.2722\n",
            "Epoch 157/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1961 - val_loss: 0.2723\n",
            "Epoch 158/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1961 - val_loss: 0.2721\n",
            "Epoch 159/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1957 - val_loss: 0.2721\n",
            "Epoch 160/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1954 - val_loss: 0.2721\n",
            "Epoch 161/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1951 - val_loss: 0.2722\n",
            "Epoch 162/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1947 - val_loss: 0.2722\n",
            "Epoch 163/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1946 - val_loss: 0.2722\n",
            "Epoch 164/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1943 - val_loss: 0.2722\n",
            "Epoch 165/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1941 - val_loss: 0.2722\n",
            "Epoch 166/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1939 - val_loss: 0.2721\n",
            "Epoch 167/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1935 - val_loss: 0.2722\n",
            "Epoch 168/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1934 - val_loss: 0.2722\n",
            "Epoch 169/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1930 - val_loss: 0.2722\n",
            "Epoch 170/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1928 - val_loss: 0.2722\n",
            "Epoch 171/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1927 - val_loss: 0.2723\n",
            "Epoch 172/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1924 - val_loss: 0.2722\n",
            "Epoch 173/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1919 - val_loss: 0.2722\n",
            "Epoch 174/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1918 - val_loss: 0.2723\n",
            "Epoch 175/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1917 - val_loss: 0.2723\n",
            "Epoch 176/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1916 - val_loss: 0.2723\n",
            "Epoch 177/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1914 - val_loss: 0.2722\n",
            "Epoch 178/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1910 - val_loss: 0.2724\n",
            "Epoch 179/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1906 - val_loss: 0.2724\n",
            "Epoch 180/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1904 - val_loss: 0.2723\n",
            "Epoch 181/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1903 - val_loss: 0.2724\n",
            "Epoch 182/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1901 - val_loss: 0.2725\n",
            "Epoch 183/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1899 - val_loss: 0.2724\n",
            "Epoch 184/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1896 - val_loss: 0.2723\n",
            "Epoch 185/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1893 - val_loss: 0.2724\n",
            "Epoch 186/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1892 - val_loss: 0.2724\n",
            "Epoch 187/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1890 - val_loss: 0.2724\n",
            "Epoch 188/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1887 - val_loss: 0.2724\n",
            "Epoch 189/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1885 - val_loss: 0.2726\n",
            "Epoch 190/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1885 - val_loss: 0.2725\n",
            "Epoch 191/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1880 - val_loss: 0.2725\n",
            "Epoch 192/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1878 - val_loss: 0.2726\n",
            "Epoch 193/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1877 - val_loss: 0.2725\n",
            "Epoch 194/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1875 - val_loss: 0.2726\n",
            "Epoch 195/200\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 0.1873 - val_loss: 0.2727\n",
            "Epoch 196/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1870 - val_loss: 0.2726\n",
            "Epoch 197/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1870 - val_loss: 0.2726\n",
            "Epoch 198/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1867 - val_loss: 0.2728\n",
            "Epoch 199/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1867 - val_loss: 0.2727\n",
            "Epoch 200/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.1863 - val_loss: 0.2727\n",
            "10/10 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 256, 0.18629270792007446, 0.2726600468158722, 0.03195554, 3.781221428906992, 0.21245301505216996)\n",
            "Epoch 1/200\n",
            "133/133 [==============================] - 2s 10ms/step - loss: 0.4692 - val_loss: 0.4662\n",
            "Epoch 2/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3621 - val_loss: 0.3922\n",
            "Epoch 3/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3310 - val_loss: 0.3570\n",
            "Epoch 4/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3112 - val_loss: 0.3385\n",
            "Epoch 5/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2984 - val_loss: 0.3265\n",
            "Epoch 6/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2890 - val_loss: 0.3169\n",
            "Epoch 7/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2815 - val_loss: 0.3096\n",
            "Epoch 8/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2757 - val_loss: 0.3036\n",
            "Epoch 9/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2709 - val_loss: 0.2988\n",
            "Epoch 10/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2671 - val_loss: 0.2951\n",
            "Epoch 11/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2644 - val_loss: 0.2924\n",
            "Epoch 12/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2623 - val_loss: 0.2902\n",
            "Epoch 13/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2607 - val_loss: 0.2887\n",
            "Epoch 14/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2594 - val_loss: 0.2874\n",
            "Epoch 15/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2584 - val_loss: 0.2864\n",
            "Epoch 16/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2577 - val_loss: 0.2857\n",
            "Epoch 17/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2569 - val_loss: 0.2851\n",
            "Epoch 18/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2562 - val_loss: 0.2845\n",
            "Epoch 19/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2560 - val_loss: 0.2842\n",
            "Epoch 20/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2554 - val_loss: 0.2837\n",
            "Epoch 21/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2552 - val_loss: 0.2835\n",
            "Epoch 22/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2548 - val_loss: 0.2832\n",
            "Epoch 23/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2543 - val_loss: 0.2829\n",
            "Epoch 24/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2540 - val_loss: 0.2827\n",
            "Epoch 25/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2536 - val_loss: 0.2825\n",
            "Epoch 26/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2534 - val_loss: 0.2823\n",
            "Epoch 27/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2530 - val_loss: 0.2820\n",
            "Epoch 28/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2528 - val_loss: 0.2819\n",
            "Epoch 29/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2524 - val_loss: 0.2817\n",
            "Epoch 30/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2520 - val_loss: 0.2815\n",
            "Epoch 31/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2516 - val_loss: 0.2813\n",
            "Epoch 32/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2514 - val_loss: 0.2811\n",
            "Epoch 33/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2508 - val_loss: 0.2810\n",
            "Epoch 34/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2506 - val_loss: 0.2807\n",
            "Epoch 35/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2499 - val_loss: 0.2806\n",
            "Epoch 36/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2497 - val_loss: 0.2805\n",
            "Epoch 37/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2491 - val_loss: 0.2802\n",
            "Epoch 38/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2487 - val_loss: 0.2801\n",
            "Epoch 39/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2482 - val_loss: 0.2799\n",
            "Epoch 40/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2477 - val_loss: 0.2797\n",
            "Epoch 41/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2472 - val_loss: 0.2795\n",
            "Epoch 42/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2466 - val_loss: 0.2794\n",
            "Epoch 43/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2463 - val_loss: 0.2792\n",
            "Epoch 44/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2456 - val_loss: 0.2790\n",
            "Epoch 45/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2450 - val_loss: 0.2788\n",
            "Epoch 46/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2446 - val_loss: 0.2786\n",
            "Epoch 47/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2439 - val_loss: 0.2784\n",
            "Epoch 48/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2434 - val_loss: 0.2782\n",
            "Epoch 49/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2428 - val_loss: 0.2780\n",
            "Epoch 50/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2421 - val_loss: 0.2778\n",
            "Epoch 51/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2416 - val_loss: 0.2776\n",
            "Epoch 52/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2409 - val_loss: 0.2774\n",
            "Epoch 53/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2403 - val_loss: 0.2772\n",
            "Epoch 54/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2397 - val_loss: 0.2770\n",
            "Epoch 55/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2395 - val_loss: 0.2769\n",
            "Epoch 56/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2386 - val_loss: 0.2766\n",
            "Epoch 57/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2379 - val_loss: 0.2765\n",
            "Epoch 58/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2371 - val_loss: 0.2763\n",
            "Epoch 59/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2368 - val_loss: 0.2761\n",
            "Epoch 60/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2361 - val_loss: 0.2759\n",
            "Epoch 61/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2355 - val_loss: 0.2758\n",
            "Epoch 62/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2350 - val_loss: 0.2756\n",
            "Epoch 63/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2342 - val_loss: 0.2754\n",
            "Epoch 64/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2337 - val_loss: 0.2752\n",
            "Epoch 65/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2331 - val_loss: 0.2750\n",
            "Epoch 66/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2325 - val_loss: 0.2749\n",
            "Epoch 67/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2318 - val_loss: 0.2747\n",
            "Epoch 68/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2313 - val_loss: 0.2746\n",
            "Epoch 69/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2307 - val_loss: 0.2744\n",
            "Epoch 70/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2301 - val_loss: 0.2743\n",
            "Epoch 71/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2294 - val_loss: 0.2741\n",
            "Epoch 72/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2288 - val_loss: 0.2739\n",
            "Epoch 73/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2284 - val_loss: 0.2738\n",
            "Epoch 74/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2277 - val_loss: 0.2736\n",
            "Epoch 75/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2272 - val_loss: 0.2735\n",
            "Epoch 76/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2266 - val_loss: 0.2734\n",
            "Epoch 77/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2261 - val_loss: 0.2732\n",
            "Epoch 78/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2254 - val_loss: 0.2732\n",
            "Epoch 79/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2249 - val_loss: 0.2730\n",
            "Epoch 80/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2244 - val_loss: 0.2728\n",
            "Epoch 81/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2238 - val_loss: 0.2728\n",
            "Epoch 82/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2231 - val_loss: 0.2726\n",
            "Epoch 83/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2226 - val_loss: 0.2725\n",
            "Epoch 84/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2223 - val_loss: 0.2724\n",
            "Epoch 85/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2215 - val_loss: 0.2722\n",
            "Epoch 86/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2212 - val_loss: 0.2722\n",
            "Epoch 87/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2206 - val_loss: 0.2721\n",
            "Epoch 88/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2202 - val_loss: 0.2719\n",
            "Epoch 89/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2195 - val_loss: 0.2720\n",
            "Epoch 90/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2190 - val_loss: 0.2718\n",
            "Epoch 91/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2185 - val_loss: 0.2716\n",
            "Epoch 92/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2182 - val_loss: 0.2716\n",
            "Epoch 93/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2175 - val_loss: 0.2715\n",
            "Epoch 94/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2172 - val_loss: 0.2714\n",
            "Epoch 95/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2166 - val_loss: 0.2713\n",
            "Epoch 96/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2162 - val_loss: 0.2712\n",
            "Epoch 97/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2157 - val_loss: 0.2711\n",
            "Epoch 98/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2152 - val_loss: 0.2711\n",
            "Epoch 99/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2151 - val_loss: 0.2710\n",
            "Epoch 100/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2146 - val_loss: 0.2709\n",
            "Epoch 101/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2140 - val_loss: 0.2708\n",
            "Epoch 102/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2134 - val_loss: 0.2708\n",
            "Epoch 103/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2130 - val_loss: 0.2707\n",
            "Epoch 104/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2125 - val_loss: 0.2707\n",
            "Epoch 105/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2121 - val_loss: 0.2706\n",
            "Epoch 106/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2117 - val_loss: 0.2706\n",
            "Epoch 107/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2113 - val_loss: 0.2705\n",
            "Epoch 108/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2108 - val_loss: 0.2704\n",
            "Epoch 109/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2104 - val_loss: 0.2704\n",
            "Epoch 110/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2100 - val_loss: 0.2703\n",
            "Epoch 111/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2095 - val_loss: 0.2703\n",
            "Epoch 112/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2092 - val_loss: 0.2703\n",
            "Epoch 113/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2087 - val_loss: 0.2702\n",
            "Epoch 114/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2083 - val_loss: 0.2701\n",
            "Epoch 115/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2078 - val_loss: 0.2701\n",
            "Epoch 116/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2075 - val_loss: 0.2701\n",
            "Epoch 117/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2070 - val_loss: 0.2700\n",
            "Epoch 118/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2067 - val_loss: 0.2700\n",
            "Epoch 119/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2063 - val_loss: 0.2699\n",
            "Epoch 120/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2059 - val_loss: 0.2700\n",
            "Epoch 121/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2056 - val_loss: 0.2699\n",
            "Epoch 122/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2054 - val_loss: 0.2698\n",
            "Epoch 123/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2049 - val_loss: 0.2698\n",
            "Epoch 124/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2044 - val_loss: 0.2698\n",
            "Epoch 125/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2042 - val_loss: 0.2698\n",
            "Epoch 126/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2037 - val_loss: 0.2697\n",
            "Epoch 127/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2035 - val_loss: 0.2697\n",
            "Epoch 128/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2030 - val_loss: 0.2697\n",
            "Epoch 129/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2027 - val_loss: 0.2696\n",
            "Epoch 130/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2024 - val_loss: 0.2697\n",
            "Epoch 131/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2020 - val_loss: 0.2696\n",
            "Epoch 132/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2018 - val_loss: 0.2697\n",
            "Epoch 133/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2013 - val_loss: 0.2696\n",
            "Epoch 134/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2009 - val_loss: 0.2696\n",
            "Epoch 135/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2007 - val_loss: 0.2696\n",
            "Epoch 136/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2003 - val_loss: 0.2696\n",
            "Epoch 137/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1999 - val_loss: 0.2696\n",
            "Epoch 138/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1998 - val_loss: 0.2695\n",
            "Epoch 139/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1993 - val_loss: 0.2695\n",
            "Epoch 140/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1989 - val_loss: 0.2695\n",
            "Epoch 141/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1986 - val_loss: 0.2696\n",
            "Epoch 142/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1984 - val_loss: 0.2696\n",
            "Epoch 143/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1980 - val_loss: 0.2695\n",
            "Epoch 144/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1978 - val_loss: 0.2695\n",
            "Epoch 145/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1973 - val_loss: 0.2695\n",
            "Epoch 146/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1973 - val_loss: 0.2696\n",
            "Epoch 147/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1967 - val_loss: 0.2695\n",
            "Epoch 148/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1965 - val_loss: 0.2695\n",
            "Epoch 149/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1964 - val_loss: 0.2695\n",
            "Epoch 150/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1960 - val_loss: 0.2695\n",
            "Epoch 151/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1956 - val_loss: 0.2695\n",
            "Epoch 152/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1956 - val_loss: 0.2695\n",
            "Epoch 153/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1953 - val_loss: 0.2695\n",
            "Epoch 154/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1948 - val_loss: 0.2695\n",
            "Epoch 155/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1945 - val_loss: 0.2696\n",
            "Epoch 156/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1945 - val_loss: 0.2695\n",
            "Epoch 157/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1939 - val_loss: 0.2696\n",
            "Epoch 158/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1938 - val_loss: 0.2696\n",
            "Epoch 159/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1933 - val_loss: 0.2696\n",
            "Epoch 160/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1933 - val_loss: 0.2696\n",
            "Epoch 161/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1929 - val_loss: 0.2696\n",
            "Epoch 162/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1928 - val_loss: 0.2696\n",
            "Epoch 163/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1923 - val_loss: 0.2696\n",
            "Epoch 164/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1921 - val_loss: 0.2696\n",
            "Epoch 165/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1920 - val_loss: 0.2696\n",
            "Epoch 166/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1917 - val_loss: 0.2699\n",
            "Epoch 167/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1914 - val_loss: 0.2697\n",
            "Epoch 168/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1912 - val_loss: 0.2696\n",
            "Epoch 169/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1910 - val_loss: 0.2697\n",
            "Epoch 170/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1908 - val_loss: 0.2696\n",
            "Epoch 171/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1903 - val_loss: 0.2697\n",
            "Epoch 172/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1900 - val_loss: 0.2697\n",
            "Epoch 173/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1900 - val_loss: 0.2697\n",
            "Epoch 174/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1896 - val_loss: 0.2697\n",
            "Epoch 175/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1894 - val_loss: 0.2698\n",
            "Epoch 176/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1892 - val_loss: 0.2698\n",
            "Epoch 177/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1890 - val_loss: 0.2698\n",
            "Epoch 178/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1888 - val_loss: 0.2698\n",
            "Epoch 179/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1887 - val_loss: 0.2698\n",
            "Epoch 180/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1884 - val_loss: 0.2699\n",
            "Epoch 181/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1880 - val_loss: 0.2698\n",
            "Epoch 182/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1881 - val_loss: 0.2699\n",
            "Epoch 183/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1877 - val_loss: 0.2699\n",
            "Epoch 184/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1874 - val_loss: 0.2701\n",
            "Epoch 185/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1872 - val_loss: 0.2701\n",
            "Epoch 186/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1870 - val_loss: 0.2700\n",
            "Epoch 187/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1869 - val_loss: 0.2700\n",
            "Epoch 188/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1867 - val_loss: 0.2700\n",
            "Epoch 189/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1864 - val_loss: 0.2701\n",
            "Epoch 190/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1863 - val_loss: 0.2701\n",
            "Epoch 191/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1862 - val_loss: 0.2700\n",
            "Epoch 192/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1861 - val_loss: 0.2701\n",
            "Epoch 193/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1857 - val_loss: 0.2701\n",
            "Epoch 194/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1855 - val_loss: 0.2701\n",
            "Epoch 195/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1852 - val_loss: 0.2701\n",
            "Epoch 196/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1849 - val_loss: 0.2703\n",
            "Epoch 197/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1848 - val_loss: 0.2702\n",
            "Epoch 198/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1847 - val_loss: 0.2703\n",
            "Epoch 199/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1844 - val_loss: 0.2703\n",
            "Epoch 200/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1842 - val_loss: 0.2703\n",
            "10/10 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 256, 0.18418660759925842, 0.2702886760234833, 0.030675597, 3.8388033495892726, 0.234592111060444)\n",
            "Epoch 1/200\n",
            "133/133 [==============================] - 2s 10ms/step - loss: 0.4728 - val_loss: 0.4640\n",
            "Epoch 2/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3633 - val_loss: 0.3899\n",
            "Epoch 3/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3331 - val_loss: 0.3567\n",
            "Epoch 4/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3133 - val_loss: 0.3377\n",
            "Epoch 5/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2996 - val_loss: 0.3248\n",
            "Epoch 6/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2898 - val_loss: 0.3149\n",
            "Epoch 7/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2823 - val_loss: 0.3076\n",
            "Epoch 8/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2766 - val_loss: 0.3020\n",
            "Epoch 9/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2719 - val_loss: 0.2976\n",
            "Epoch 10/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2682 - val_loss: 0.2940\n",
            "Epoch 11/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2655 - val_loss: 0.2914\n",
            "Epoch 12/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2633 - val_loss: 0.2893\n",
            "Epoch 13/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2618 - val_loss: 0.2878\n",
            "Epoch 14/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2604 - val_loss: 0.2865\n",
            "Epoch 15/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2594 - val_loss: 0.2855\n",
            "Epoch 16/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2588 - val_loss: 0.2848\n",
            "Epoch 17/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2582 - val_loss: 0.2842\n",
            "Epoch 18/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2578 - val_loss: 0.2837\n",
            "Epoch 19/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2572 - val_loss: 0.2834\n",
            "Epoch 20/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2569 - val_loss: 0.2829\n",
            "Epoch 21/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2562 - val_loss: 0.2827\n",
            "Epoch 22/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2560 - val_loss: 0.2823\n",
            "Epoch 23/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2556 - val_loss: 0.2821\n",
            "Epoch 24/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2554 - val_loss: 0.2819\n",
            "Epoch 25/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2551 - val_loss: 0.2817\n",
            "Epoch 26/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2547 - val_loss: 0.2814\n",
            "Epoch 27/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2543 - val_loss: 0.2813\n",
            "Epoch 28/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2539 - val_loss: 0.2811\n",
            "Epoch 29/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2537 - val_loss: 0.2809\n",
            "Epoch 30/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2532 - val_loss: 0.2807\n",
            "Epoch 31/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2529 - val_loss: 0.2805\n",
            "Epoch 32/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2524 - val_loss: 0.2804\n",
            "Epoch 33/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2522 - val_loss: 0.2802\n",
            "Epoch 34/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2515 - val_loss: 0.2800\n",
            "Epoch 35/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2513 - val_loss: 0.2798\n",
            "Epoch 36/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2508 - val_loss: 0.2797\n",
            "Epoch 37/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2504 - val_loss: 0.2795\n",
            "Epoch 38/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2497 - val_loss: 0.2793\n",
            "Epoch 39/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2495 - val_loss: 0.2791\n",
            "Epoch 40/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2487 - val_loss: 0.2789\n",
            "Epoch 41/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2484 - val_loss: 0.2788\n",
            "Epoch 42/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2478 - val_loss: 0.2786\n",
            "Epoch 43/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2471 - val_loss: 0.2784\n",
            "Epoch 44/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2468 - val_loss: 0.2782\n",
            "Epoch 45/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2460 - val_loss: 0.2780\n",
            "Epoch 46/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2456 - val_loss: 0.2778\n",
            "Epoch 47/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2450 - val_loss: 0.2776\n",
            "Epoch 48/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.2774\n",
            "Epoch 49/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2437 - val_loss: 0.2773\n",
            "Epoch 50/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2433 - val_loss: 0.2771\n",
            "Epoch 51/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2424 - val_loss: 0.2769\n",
            "Epoch 52/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2418 - val_loss: 0.2767\n",
            "Epoch 53/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2413 - val_loss: 0.2765\n",
            "Epoch 54/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2406 - val_loss: 0.2763\n",
            "Epoch 55/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2400 - val_loss: 0.2762\n",
            "Epoch 56/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2394 - val_loss: 0.2760\n",
            "Epoch 57/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2388 - val_loss: 0.2758\n",
            "Epoch 58/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2380 - val_loss: 0.2756\n",
            "Epoch 59/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2376 - val_loss: 0.2754\n",
            "Epoch 60/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2369 - val_loss: 0.2753\n",
            "Epoch 61/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2363 - val_loss: 0.2751\n",
            "Epoch 62/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2356 - val_loss: 0.2750\n",
            "Epoch 63/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2349 - val_loss: 0.2748\n",
            "Epoch 64/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2343 - val_loss: 0.2746\n",
            "Epoch 65/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2338 - val_loss: 0.2744\n",
            "Epoch 66/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2330 - val_loss: 0.2743\n",
            "Epoch 67/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2327 - val_loss: 0.2742\n",
            "Epoch 68/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2318 - val_loss: 0.2740\n",
            "Epoch 69/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2313 - val_loss: 0.2738\n",
            "Epoch 70/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2307 - val_loss: 0.2737\n",
            "Epoch 71/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2301 - val_loss: 0.2736\n",
            "Epoch 72/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2295 - val_loss: 0.2735\n",
            "Epoch 73/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2289 - val_loss: 0.2734\n",
            "Epoch 74/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2282 - val_loss: 0.2732\n",
            "Epoch 75/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2276 - val_loss: 0.2730\n",
            "Epoch 76/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2271 - val_loss: 0.2729\n",
            "Epoch 77/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2266 - val_loss: 0.2727\n",
            "Epoch 78/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2262 - val_loss: 0.2727\n",
            "Epoch 79/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2254 - val_loss: 0.2725\n",
            "Epoch 80/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2249 - val_loss: 0.2724\n",
            "Epoch 81/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2243 - val_loss: 0.2723\n",
            "Epoch 82/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2239 - val_loss: 0.2722\n",
            "Epoch 83/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2232 - val_loss: 0.2721\n",
            "Epoch 84/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2226 - val_loss: 0.2720\n",
            "Epoch 85/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2221 - val_loss: 0.2718\n",
            "Epoch 86/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2216 - val_loss: 0.2717\n",
            "Epoch 87/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2212 - val_loss: 0.2717\n",
            "Epoch 88/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2205 - val_loss: 0.2715\n",
            "Epoch 89/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2200 - val_loss: 0.2715\n",
            "Epoch 90/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2197 - val_loss: 0.2714\n",
            "Epoch 91/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.2712\n",
            "Epoch 92/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2185 - val_loss: 0.2712\n",
            "Epoch 93/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2179 - val_loss: 0.2710\n",
            "Epoch 94/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2175 - val_loss: 0.2710\n",
            "Epoch 95/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2171 - val_loss: 0.2709\n",
            "Epoch 96/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2167 - val_loss: 0.2709\n",
            "Epoch 97/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2161 - val_loss: 0.2708\n",
            "Epoch 98/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2156 - val_loss: 0.2707\n",
            "Epoch 99/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2152 - val_loss: 0.2706\n",
            "Epoch 100/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2146 - val_loss: 0.2706\n",
            "Epoch 101/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2141 - val_loss: 0.2705\n",
            "Epoch 102/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2137 - val_loss: 0.2705\n",
            "Epoch 103/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2133 - val_loss: 0.2704\n",
            "Epoch 104/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2129 - val_loss: 0.2705\n",
            "Epoch 105/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2126 - val_loss: 0.2703\n",
            "Epoch 106/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2118 - val_loss: 0.2702\n",
            "Epoch 107/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2116 - val_loss: 0.2702\n",
            "Epoch 108/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2111 - val_loss: 0.2701\n",
            "Epoch 109/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2106 - val_loss: 0.2701\n",
            "Epoch 110/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2103 - val_loss: 0.2700\n",
            "Epoch 111/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2099 - val_loss: 0.2700\n",
            "Epoch 112/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2094 - val_loss: 0.2699\n",
            "Epoch 113/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2089 - val_loss: 0.2699\n",
            "Epoch 114/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2086 - val_loss: 0.2698\n",
            "Epoch 115/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2081 - val_loss: 0.2698\n",
            "Epoch 116/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2077 - val_loss: 0.2697\n",
            "Epoch 117/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2073 - val_loss: 0.2697\n",
            "Epoch 118/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2068 - val_loss: 0.2697\n",
            "Epoch 119/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2066 - val_loss: 0.2696\n",
            "Epoch 120/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2062 - val_loss: 0.2697\n",
            "Epoch 121/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2059 - val_loss: 0.2695\n",
            "Epoch 122/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2055 - val_loss: 0.2695\n",
            "Epoch 123/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.2696\n",
            "Epoch 124/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2047 - val_loss: 0.2694\n",
            "Epoch 125/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2042 - val_loss: 0.2694\n",
            "Epoch 126/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2041 - val_loss: 0.2695\n",
            "Epoch 127/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2036 - val_loss: 0.2694\n",
            "Epoch 128/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2033 - val_loss: 0.2693\n",
            "Epoch 129/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2028 - val_loss: 0.2693\n",
            "Epoch 130/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2026 - val_loss: 0.2698\n",
            "Epoch 131/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2021 - val_loss: 0.2696\n",
            "Epoch 132/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2018 - val_loss: 0.2693\n",
            "Epoch 133/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2015 - val_loss: 0.2693\n",
            "Epoch 134/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.2693\n",
            "Epoch 135/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2009 - val_loss: 0.2693\n",
            "Epoch 136/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2005 - val_loss: 0.2693\n",
            "Epoch 137/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2001 - val_loss: 0.2693\n",
            "Epoch 138/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1998 - val_loss: 0.2693\n",
            "Epoch 139/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1997 - val_loss: 0.2692\n",
            "Epoch 140/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1992 - val_loss: 0.2692\n",
            "Epoch 141/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1990 - val_loss: 0.2693\n",
            "Epoch 142/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1986 - val_loss: 0.2693\n",
            "Epoch 143/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1982 - val_loss: 0.2692\n",
            "Epoch 144/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1980 - val_loss: 0.2692\n",
            "Epoch 145/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1975 - val_loss: 0.2698\n",
            "Epoch 146/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1975 - val_loss: 0.2692\n",
            "Epoch 147/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1970 - val_loss: 0.2691\n",
            "Epoch 148/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1968 - val_loss: 0.2692\n",
            "Epoch 149/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1963 - val_loss: 0.2692\n",
            "Epoch 150/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1962 - val_loss: 0.2692\n",
            "Epoch 151/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1959 - val_loss: 0.2692\n",
            "Epoch 152/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1958 - val_loss: 0.2692\n",
            "Epoch 153/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1952 - val_loss: 0.2693\n",
            "Epoch 154/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1949 - val_loss: 0.2693\n",
            "Epoch 155/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1947 - val_loss: 0.2692\n",
            "Epoch 156/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1943 - val_loss: 0.2692\n",
            "Epoch 157/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1940 - val_loss: 0.2695\n",
            "Epoch 158/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1940 - val_loss: 0.2692\n",
            "Epoch 159/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1938 - val_loss: 0.2692\n",
            "Epoch 160/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1936 - val_loss: 0.2694\n",
            "Epoch 161/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1930 - val_loss: 0.2694\n",
            "Epoch 162/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1928 - val_loss: 0.2692\n",
            "Epoch 163/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1926 - val_loss: 0.2694\n",
            "Epoch 164/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1921 - val_loss: 0.2692\n",
            "Epoch 165/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1922 - val_loss: 0.2692\n",
            "Epoch 166/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1918 - val_loss: 0.2693\n",
            "Epoch 167/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1915 - val_loss: 0.2693\n",
            "Epoch 168/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1912 - val_loss: 0.2694\n",
            "Epoch 169/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1911 - val_loss: 0.2694\n",
            "Epoch 170/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1908 - val_loss: 0.2693\n",
            "Epoch 171/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1906 - val_loss: 0.2698\n",
            "Epoch 172/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1904 - val_loss: 0.2694\n",
            "Epoch 173/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1902 - val_loss: 0.2696\n",
            "Epoch 174/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1900 - val_loss: 0.2693\n",
            "Epoch 175/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1895 - val_loss: 0.2694\n",
            "Epoch 176/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1895 - val_loss: 0.2694\n",
            "Epoch 177/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1891 - val_loss: 0.2694\n",
            "Epoch 178/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1888 - val_loss: 0.2695\n",
            "Epoch 179/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1889 - val_loss: 0.2696\n",
            "Epoch 180/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1886 - val_loss: 0.2695\n",
            "Epoch 181/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1884 - val_loss: 0.2695\n",
            "Epoch 182/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1880 - val_loss: 0.2695\n",
            "Epoch 183/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1878 - val_loss: 0.2695\n",
            "Epoch 184/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1876 - val_loss: 0.2695\n",
            "Epoch 185/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1877 - val_loss: 0.2697\n",
            "Epoch 186/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1872 - val_loss: 0.2695\n",
            "Epoch 187/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1870 - val_loss: 0.2696\n",
            "Epoch 188/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1867 - val_loss: 0.2697\n",
            "Epoch 189/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1866 - val_loss: 0.2697\n",
            "Epoch 190/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1862 - val_loss: 0.2697\n",
            "Epoch 191/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1862 - val_loss: 0.2697\n",
            "Epoch 192/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1860 - val_loss: 0.2698\n",
            "Epoch 193/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1858 - val_loss: 0.2697\n",
            "Epoch 194/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1857 - val_loss: 0.2697\n",
            "Epoch 195/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1853 - val_loss: 0.2699\n",
            "Epoch 196/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1851 - val_loss: 0.2699\n",
            "Epoch 197/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1849 - val_loss: 0.2698\n",
            "Epoch 198/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1848 - val_loss: 0.2698\n",
            "Epoch 199/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1845 - val_loss: 0.2699\n",
            "Epoch 200/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1844 - val_loss: 0.2700\n",
            "10/10 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 256, 0.18436254560947418, 0.2699841558933258, 0.028844468, 3.7646288684275424, 0.2225747285502202)\n",
            "Epoch 1/200\n",
            "133/133 [==============================] - 2s 10ms/step - loss: 0.4699 - val_loss: 0.4664\n",
            "Epoch 2/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3612 - val_loss: 0.3939\n",
            "Epoch 3/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3306 - val_loss: 0.3594\n",
            "Epoch 4/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3106 - val_loss: 0.3404\n",
            "Epoch 5/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2975 - val_loss: 0.3276\n",
            "Epoch 6/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2878 - val_loss: 0.3179\n",
            "Epoch 7/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2805 - val_loss: 0.3106\n",
            "Epoch 8/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2747 - val_loss: 0.3049\n",
            "Epoch 9/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2702 - val_loss: 0.3004\n",
            "Epoch 10/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2666 - val_loss: 0.2968\n",
            "Epoch 11/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2641 - val_loss: 0.2941\n",
            "Epoch 12/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2619 - val_loss: 0.2918\n",
            "Epoch 13/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2602 - val_loss: 0.2903\n",
            "Epoch 14/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2590 - val_loss: 0.2890\n",
            "Epoch 15/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2578 - val_loss: 0.2881\n",
            "Epoch 16/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2572 - val_loss: 0.2872\n",
            "Epoch 17/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2566 - val_loss: 0.2866\n",
            "Epoch 18/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2559 - val_loss: 0.2861\n",
            "Epoch 19/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2556 - val_loss: 0.2857\n",
            "Epoch 20/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2553 - val_loss: 0.2854\n",
            "Epoch 21/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2548 - val_loss: 0.2850\n",
            "Epoch 22/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2544 - val_loss: 0.2847\n",
            "Epoch 23/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2541 - val_loss: 0.2845\n",
            "Epoch 24/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2538 - val_loss: 0.2843\n",
            "Epoch 25/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2534 - val_loss: 0.2840\n",
            "Epoch 26/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2531 - val_loss: 0.2838\n",
            "Epoch 27/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2528 - val_loss: 0.2835\n",
            "Epoch 28/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2525 - val_loss: 0.2834\n",
            "Epoch 29/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2522 - val_loss: 0.2831\n",
            "Epoch 30/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2517 - val_loss: 0.2829\n",
            "Epoch 31/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2514 - val_loss: 0.2828\n",
            "Epoch 32/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2512 - val_loss: 0.2827\n",
            "Epoch 33/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2507 - val_loss: 0.2824\n",
            "Epoch 34/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2503 - val_loss: 0.2822\n",
            "Epoch 35/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2498 - val_loss: 0.2820\n",
            "Epoch 36/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2496 - val_loss: 0.2819\n",
            "Epoch 37/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2490 - val_loss: 0.2817\n",
            "Epoch 38/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2485 - val_loss: 0.2815\n",
            "Epoch 39/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2482 - val_loss: 0.2813\n",
            "Epoch 40/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2474 - val_loss: 0.2811\n",
            "Epoch 41/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2471 - val_loss: 0.2809\n",
            "Epoch 42/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2465 - val_loss: 0.2807\n",
            "Epoch 43/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2460 - val_loss: 0.2806\n",
            "Epoch 44/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2454 - val_loss: 0.2804\n",
            "Epoch 45/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2450 - val_loss: 0.2802\n",
            "Epoch 46/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2444 - val_loss: 0.2799\n",
            "Epoch 47/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2439 - val_loss: 0.2798\n",
            "Epoch 48/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2434 - val_loss: 0.2796\n",
            "Epoch 49/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2426 - val_loss: 0.2793\n",
            "Epoch 50/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2420 - val_loss: 0.2792\n",
            "Epoch 51/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2415 - val_loss: 0.2790\n",
            "Epoch 52/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2408 - val_loss: 0.2788\n",
            "Epoch 53/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2402 - val_loss: 0.2786\n",
            "Epoch 54/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2396 - val_loss: 0.2784\n",
            "Epoch 55/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2391 - val_loss: 0.2781\n",
            "Epoch 56/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2384 - val_loss: 0.2780\n",
            "Epoch 57/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2378 - val_loss: 0.2778\n",
            "Epoch 58/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2373 - val_loss: 0.2776\n",
            "Epoch 59/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2367 - val_loss: 0.2775\n",
            "Epoch 60/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2360 - val_loss: 0.2773\n",
            "Epoch 61/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2355 - val_loss: 0.2771\n",
            "Epoch 62/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2350 - val_loss: 0.2769\n",
            "Epoch 63/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2343 - val_loss: 0.2768\n",
            "Epoch 64/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2337 - val_loss: 0.2766\n",
            "Epoch 65/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2330 - val_loss: 0.2764\n",
            "Epoch 66/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2325 - val_loss: 0.2762\n",
            "Epoch 67/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2318 - val_loss: 0.2761\n",
            "Epoch 68/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2312 - val_loss: 0.2759\n",
            "Epoch 69/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2308 - val_loss: 0.2758\n",
            "Epoch 70/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2301 - val_loss: 0.2756\n",
            "Epoch 71/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2296 - val_loss: 0.2755\n",
            "Epoch 72/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2290 - val_loss: 0.2753\n",
            "Epoch 73/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2283 - val_loss: 0.2752\n",
            "Epoch 74/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2276 - val_loss: 0.2750\n",
            "Epoch 75/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2273 - val_loss: 0.2749\n",
            "Epoch 76/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2268 - val_loss: 0.2747\n",
            "Epoch 77/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2262 - val_loss: 0.2746\n",
            "Epoch 78/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2256 - val_loss: 0.2745\n",
            "Epoch 79/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2249 - val_loss: 0.2743\n",
            "Epoch 80/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2244 - val_loss: 0.2742\n",
            "Epoch 81/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2240 - val_loss: 0.2741\n",
            "Epoch 82/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2234 - val_loss: 0.2740\n",
            "Epoch 83/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2228 - val_loss: 0.2738\n",
            "Epoch 84/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2223 - val_loss: 0.2737\n",
            "Epoch 85/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2219 - val_loss: 0.2736\n",
            "Epoch 86/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2214 - val_loss: 0.2736\n",
            "Epoch 87/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2209 - val_loss: 0.2735\n",
            "Epoch 88/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2204 - val_loss: 0.2734\n",
            "Epoch 89/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2198 - val_loss: 0.2732\n",
            "Epoch 90/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2193 - val_loss: 0.2731\n",
            "Epoch 91/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2189 - val_loss: 0.2730\n",
            "Epoch 92/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2184 - val_loss: 0.2730\n",
            "Epoch 93/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2180 - val_loss: 0.2729\n",
            "Epoch 94/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2174 - val_loss: 0.2728\n",
            "Epoch 95/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2170 - val_loss: 0.2727\n",
            "Epoch 96/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2163 - val_loss: 0.2726\n",
            "Epoch 97/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2159 - val_loss: 0.2725\n",
            "Epoch 98/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2155 - val_loss: 0.2725\n",
            "Epoch 99/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2150 - val_loss: 0.2724\n",
            "Epoch 100/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2146 - val_loss: 0.2723\n",
            "Epoch 101/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2141 - val_loss: 0.2722\n",
            "Epoch 102/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2136 - val_loss: 0.2722\n",
            "Epoch 103/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2130 - val_loss: 0.2721\n",
            "Epoch 104/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2128 - val_loss: 0.2720\n",
            "Epoch 105/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2123 - val_loss: 0.2720\n",
            "Epoch 106/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2119 - val_loss: 0.2719\n",
            "Epoch 107/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2115 - val_loss: 0.2719\n",
            "Epoch 108/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2113 - val_loss: 0.2718\n",
            "Epoch 109/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2107 - val_loss: 0.2718\n",
            "Epoch 110/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2103 - val_loss: 0.2717\n",
            "Epoch 111/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2099 - val_loss: 0.2716\n",
            "Epoch 112/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2093 - val_loss: 0.2716\n",
            "Epoch 113/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2089 - val_loss: 0.2716\n",
            "Epoch 114/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2088 - val_loss: 0.2715\n",
            "Epoch 115/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2083 - val_loss: 0.2715\n",
            "Epoch 116/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2078 - val_loss: 0.2714\n",
            "Epoch 117/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2076 - val_loss: 0.2714\n",
            "Epoch 118/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2072 - val_loss: 0.2714\n",
            "Epoch 119/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2065 - val_loss: 0.2713\n",
            "Epoch 120/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2064 - val_loss: 0.2713\n",
            "Epoch 121/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2058 - val_loss: 0.2713\n",
            "Epoch 122/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2056 - val_loss: 0.2713\n",
            "Epoch 123/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2054 - val_loss: 0.2712\n",
            "Epoch 124/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2050 - val_loss: 0.2712\n",
            "Epoch 125/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2048 - val_loss: 0.2712\n",
            "Epoch 126/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2041 - val_loss: 0.2712\n",
            "Epoch 127/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2037 - val_loss: 0.2712\n",
            "Epoch 128/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2034 - val_loss: 0.2711\n",
            "Epoch 129/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2030 - val_loss: 0.2711\n",
            "Epoch 130/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2029 - val_loss: 0.2711\n",
            "Epoch 131/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2025 - val_loss: 0.2710\n",
            "Epoch 132/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2021 - val_loss: 0.2710\n",
            "Epoch 133/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2018 - val_loss: 0.2711\n",
            "Epoch 134/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2015 - val_loss: 0.2710\n",
            "Epoch 135/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2010 - val_loss: 0.2710\n",
            "Epoch 136/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2007 - val_loss: 0.2710\n",
            "Epoch 137/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2004 - val_loss: 0.2709\n",
            "Epoch 138/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2002 - val_loss: 0.2710\n",
            "Epoch 139/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1999 - val_loss: 0.2710\n",
            "Epoch 140/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1996 - val_loss: 0.2709\n",
            "Epoch 141/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1990 - val_loss: 0.2709\n",
            "Epoch 142/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1989 - val_loss: 0.2710\n",
            "Epoch 143/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1985 - val_loss: 0.2709\n",
            "Epoch 144/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1985 - val_loss: 0.2709\n",
            "Epoch 145/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1981 - val_loss: 0.2708\n",
            "Epoch 146/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1975 - val_loss: 0.2709\n",
            "Epoch 147/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1974 - val_loss: 0.2709\n",
            "Epoch 148/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1972 - val_loss: 0.2711\n",
            "Epoch 149/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1969 - val_loss: 0.2709\n",
            "Epoch 150/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1966 - val_loss: 0.2709\n",
            "Epoch 151/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1960 - val_loss: 0.2709\n",
            "Epoch 152/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1959 - val_loss: 0.2709\n",
            "Epoch 153/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1957 - val_loss: 0.2709\n",
            "Epoch 154/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1952 - val_loss: 0.2709\n",
            "Epoch 155/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1950 - val_loss: 0.2709\n",
            "Epoch 156/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1949 - val_loss: 0.2709\n",
            "Epoch 157/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1946 - val_loss: 0.2709\n",
            "Epoch 158/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1944 - val_loss: 0.2709\n",
            "Epoch 159/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1939 - val_loss: 0.2710\n",
            "Epoch 160/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1937 - val_loss: 0.2709\n",
            "Epoch 161/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1936 - val_loss: 0.2710\n",
            "Epoch 162/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1931 - val_loss: 0.2710\n",
            "Epoch 163/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1928 - val_loss: 0.2711\n",
            "Epoch 164/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1928 - val_loss: 0.2709\n",
            "Epoch 165/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1925 - val_loss: 0.2710\n",
            "Epoch 166/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1924 - val_loss: 0.2711\n",
            "Epoch 167/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1923 - val_loss: 0.2709\n",
            "Epoch 168/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1920 - val_loss: 0.2711\n",
            "Epoch 169/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1914 - val_loss: 0.2710\n",
            "Epoch 170/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1911 - val_loss: 0.2710\n",
            "Epoch 171/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1911 - val_loss: 0.2710\n",
            "Epoch 172/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1908 - val_loss: 0.2710\n",
            "Epoch 173/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1905 - val_loss: 0.2710\n",
            "Epoch 174/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1902 - val_loss: 0.2712\n",
            "Epoch 175/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1901 - val_loss: 0.2711\n",
            "Epoch 176/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1897 - val_loss: 0.2711\n",
            "Epoch 177/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1895 - val_loss: 0.2712\n",
            "Epoch 178/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1895 - val_loss: 0.2711\n",
            "Epoch 179/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1892 - val_loss: 0.2712\n",
            "Epoch 180/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1891 - val_loss: 0.2711\n",
            "Epoch 181/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1885 - val_loss: 0.2712\n",
            "Epoch 182/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1886 - val_loss: 0.2712\n",
            "Epoch 183/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1884 - val_loss: 0.2712\n",
            "Epoch 184/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1881 - val_loss: 0.2712\n",
            "Epoch 185/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1879 - val_loss: 0.2713\n",
            "Epoch 186/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1877 - val_loss: 0.2713\n",
            "Epoch 187/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1874 - val_loss: 0.2714\n",
            "Epoch 188/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1874 - val_loss: 0.2713\n",
            "Epoch 189/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1870 - val_loss: 0.2714\n",
            "Epoch 190/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1868 - val_loss: 0.2714\n",
            "Epoch 191/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1865 - val_loss: 0.2715\n",
            "Epoch 192/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1863 - val_loss: 0.2714\n",
            "Epoch 193/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1861 - val_loss: 0.2714\n",
            "Epoch 194/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1861 - val_loss: 0.2714\n",
            "Epoch 195/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1860 - val_loss: 0.2715\n",
            "Epoch 196/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1857 - val_loss: 0.2715\n",
            "Epoch 197/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1855 - val_loss: 0.2716\n",
            "Epoch 198/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1854 - val_loss: 0.2716\n",
            "Epoch 199/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1851 - val_loss: 0.2716\n",
            "Epoch 200/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1847 - val_loss: 0.2716\n",
            "10/10 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 256, 0.18473665416240692, 0.27160802483558655, 0.031788837, 3.80253383559324, 0.23405047470260942)\n",
            "Epoch 1/200\n",
            "133/133 [==============================] - 2s 11ms/step - loss: 0.4663 - val_loss: 0.4714\n",
            "Epoch 2/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.3620 - val_loss: 0.3960\n",
            "Epoch 3/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.3310 - val_loss: 0.3612\n",
            "Epoch 4/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.3114 - val_loss: 0.3414\n",
            "Epoch 5/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2978 - val_loss: 0.3287\n",
            "Epoch 6/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2881 - val_loss: 0.3190\n",
            "Epoch 7/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2809 - val_loss: 0.3116\n",
            "Epoch 8/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2750 - val_loss: 0.3057\n",
            "Epoch 9/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2703 - val_loss: 0.3009\n",
            "Epoch 10/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2669 - val_loss: 0.2973\n",
            "Epoch 11/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2639 - val_loss: 0.2945\n",
            "Epoch 12/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2618 - val_loss: 0.2923\n",
            "Epoch 13/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2602 - val_loss: 0.2905\n",
            "Epoch 14/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2589 - val_loss: 0.2893\n",
            "Epoch 15/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2580 - val_loss: 0.2882\n",
            "Epoch 16/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2572 - val_loss: 0.2875\n",
            "Epoch 17/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2567 - val_loss: 0.2868\n",
            "Epoch 18/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2561 - val_loss: 0.2862\n",
            "Epoch 19/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2555 - val_loss: 0.2859\n",
            "Epoch 20/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2550 - val_loss: 0.2854\n",
            "Epoch 21/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2546 - val_loss: 0.2851\n",
            "Epoch 22/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2543 - val_loss: 0.2847\n",
            "Epoch 23/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2540 - val_loss: 0.2844\n",
            "Epoch 24/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2535 - val_loss: 0.2842\n",
            "Epoch 25/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2533 - val_loss: 0.2839\n",
            "Epoch 26/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2531 - val_loss: 0.2838\n",
            "Epoch 27/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2527 - val_loss: 0.2835\n",
            "Epoch 28/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2523 - val_loss: 0.2833\n",
            "Epoch 29/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2521 - val_loss: 0.2831\n",
            "Epoch 30/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2517 - val_loss: 0.2829\n",
            "Epoch 31/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2512 - val_loss: 0.2827\n",
            "Epoch 32/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2508 - val_loss: 0.2825\n",
            "Epoch 33/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2505 - val_loss: 0.2823\n",
            "Epoch 34/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2503 - val_loss: 0.2821\n",
            "Epoch 35/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2497 - val_loss: 0.2819\n",
            "Epoch 36/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2490 - val_loss: 0.2817\n",
            "Epoch 37/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2488 - val_loss: 0.2816\n",
            "Epoch 38/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2483 - val_loss: 0.2814\n",
            "Epoch 39/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2478 - val_loss: 0.2811\n",
            "Epoch 40/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2474 - val_loss: 0.2810\n",
            "Epoch 41/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2467 - val_loss: 0.2808\n",
            "Epoch 42/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2462 - val_loss: 0.2806\n",
            "Epoch 43/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2460 - val_loss: 0.2804\n",
            "Epoch 44/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2454 - val_loss: 0.2802\n",
            "Epoch 45/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2448 - val_loss: 0.2800\n",
            "Epoch 46/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.2798\n",
            "Epoch 47/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2437 - val_loss: 0.2796\n",
            "Epoch 48/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2431 - val_loss: 0.2795\n",
            "Epoch 49/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2425 - val_loss: 0.2792\n",
            "Epoch 50/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2420 - val_loss: 0.2791\n",
            "Epoch 51/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2413 - val_loss: 0.2789\n",
            "Epoch 52/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2407 - val_loss: 0.2787\n",
            "Epoch 53/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2402 - val_loss: 0.2784\n",
            "Epoch 54/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2394 - val_loss: 0.2782\n",
            "Epoch 55/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2389 - val_loss: 0.2780\n",
            "Epoch 56/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.2778\n",
            "Epoch 57/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2376 - val_loss: 0.2777\n",
            "Epoch 58/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2372 - val_loss: 0.2775\n",
            "Epoch 59/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2365 - val_loss: 0.2772\n",
            "Epoch 60/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2358 - val_loss: 0.2771\n",
            "Epoch 61/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2354 - val_loss: 0.2769\n",
            "Epoch 62/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2347 - val_loss: 0.2767\n",
            "Epoch 63/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2340 - val_loss: 0.2765\n",
            "Epoch 64/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2333 - val_loss: 0.2764\n",
            "Epoch 65/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2330 - val_loss: 0.2762\n",
            "Epoch 66/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2323 - val_loss: 0.2760\n",
            "Epoch 67/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2316 - val_loss: 0.2758\n",
            "Epoch 68/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2309 - val_loss: 0.2757\n",
            "Epoch 69/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2305 - val_loss: 0.2755\n",
            "Epoch 70/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2301 - val_loss: 0.2753\n",
            "Epoch 71/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2293 - val_loss: 0.2752\n",
            "Epoch 72/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2287 - val_loss: 0.2750\n",
            "Epoch 73/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2282 - val_loss: 0.2748\n",
            "Epoch 74/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2274 - val_loss: 0.2747\n",
            "Epoch 75/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2271 - val_loss: 0.2745\n",
            "Epoch 76/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2264 - val_loss: 0.2744\n",
            "Epoch 77/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2258 - val_loss: 0.2742\n",
            "Epoch 78/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2255 - val_loss: 0.2742\n",
            "Epoch 79/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2249 - val_loss: 0.2740\n",
            "Epoch 80/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2244 - val_loss: 0.2739\n",
            "Epoch 81/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2237 - val_loss: 0.2737\n",
            "Epoch 82/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2233 - val_loss: 0.2736\n",
            "Epoch 83/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2227 - val_loss: 0.2735\n",
            "Epoch 84/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2220 - val_loss: 0.2733\n",
            "Epoch 85/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2217 - val_loss: 0.2732\n",
            "Epoch 86/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2211 - val_loss: 0.2731\n",
            "Epoch 87/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2206 - val_loss: 0.2731\n",
            "Epoch 88/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2201 - val_loss: 0.2729\n",
            "Epoch 89/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2197 - val_loss: 0.2728\n",
            "Epoch 90/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2191 - val_loss: 0.2727\n",
            "Epoch 91/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2187 - val_loss: 0.2725\n",
            "Epoch 92/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2183 - val_loss: 0.2725\n",
            "Epoch 93/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2176 - val_loss: 0.2724\n",
            "Epoch 94/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2172 - val_loss: 0.2723\n",
            "Epoch 95/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2168 - val_loss: 0.2722\n",
            "Epoch 96/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2161 - val_loss: 0.2722\n",
            "Epoch 97/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2156 - val_loss: 0.2720\n",
            "Epoch 98/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2153 - val_loss: 0.2719\n",
            "Epoch 99/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2149 - val_loss: 0.2719\n",
            "Epoch 100/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2143 - val_loss: 0.2717\n",
            "Epoch 101/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2140 - val_loss: 0.2717\n",
            "Epoch 102/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2135 - val_loss: 0.2716\n",
            "Epoch 103/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2131 - val_loss: 0.2716\n",
            "Epoch 104/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2127 - val_loss: 0.2715\n",
            "Epoch 105/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2122 - val_loss: 0.2714\n",
            "Epoch 106/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2118 - val_loss: 0.2714\n",
            "Epoch 107/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2115 - val_loss: 0.2713\n",
            "Epoch 108/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2109 - val_loss: 0.2712\n",
            "Epoch 109/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2107 - val_loss: 0.2713\n",
            "Epoch 110/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2102 - val_loss: 0.2711\n",
            "Epoch 111/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2099 - val_loss: 0.2711\n",
            "Epoch 112/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2094 - val_loss: 0.2710\n",
            "Epoch 113/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2090 - val_loss: 0.2710\n",
            "Epoch 114/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2085 - val_loss: 0.2709\n",
            "Epoch 115/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2082 - val_loss: 0.2709\n",
            "Epoch 116/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2079 - val_loss: 0.2708\n",
            "Epoch 117/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2074 - val_loss: 0.2710\n",
            "Epoch 118/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2070 - val_loss: 0.2708\n",
            "Epoch 119/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2067 - val_loss: 0.2707\n",
            "Epoch 120/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2065 - val_loss: 0.2706\n",
            "Epoch 121/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2059 - val_loss: 0.2706\n",
            "Epoch 122/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2057 - val_loss: 0.2706\n",
            "Epoch 123/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.2706\n",
            "Epoch 124/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2048 - val_loss: 0.2705\n",
            "Epoch 125/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2045 - val_loss: 0.2705\n",
            "Epoch 126/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2042 - val_loss: 0.2704\n",
            "Epoch 127/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2039 - val_loss: 0.2704\n",
            "Epoch 128/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2036 - val_loss: 0.2704\n",
            "Epoch 129/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2032 - val_loss: 0.2704\n",
            "Epoch 130/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2027 - val_loss: 0.2704\n",
            "Epoch 131/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.2026 - val_loss: 0.2703\n",
            "Epoch 132/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2022 - val_loss: 0.2703\n",
            "Epoch 133/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2018 - val_loss: 0.2703\n",
            "Epoch 134/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.2702\n",
            "Epoch 135/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2011 - val_loss: 0.2702\n",
            "Epoch 136/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2010 - val_loss: 0.2703\n",
            "Epoch 137/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2006 - val_loss: 0.2701\n",
            "Epoch 138/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.2004 - val_loss: 0.2702\n",
            "Epoch 139/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1998 - val_loss: 0.2702\n",
            "Epoch 140/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1995 - val_loss: 0.2702\n",
            "Epoch 141/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1993 - val_loss: 0.2701\n",
            "Epoch 142/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1990 - val_loss: 0.2701\n",
            "Epoch 143/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1987 - val_loss: 0.2702\n",
            "Epoch 144/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1983 - val_loss: 0.2701\n",
            "Epoch 145/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1980 - val_loss: 0.2701\n",
            "Epoch 146/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1976 - val_loss: 0.2701\n",
            "Epoch 147/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1974 - val_loss: 0.2701\n",
            "Epoch 148/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1971 - val_loss: 0.2701\n",
            "Epoch 149/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1968 - val_loss: 0.2700\n",
            "Epoch 150/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1965 - val_loss: 0.2700\n",
            "Epoch 151/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1964 - val_loss: 0.2700\n",
            "Epoch 152/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1959 - val_loss: 0.2701\n",
            "Epoch 153/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1958 - val_loss: 0.2700\n",
            "Epoch 154/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1954 - val_loss: 0.2701\n",
            "Epoch 155/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1952 - val_loss: 0.2701\n",
            "Epoch 156/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1950 - val_loss: 0.2700\n",
            "Epoch 157/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1946 - val_loss: 0.2702\n",
            "Epoch 158/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1944 - val_loss: 0.2700\n",
            "Epoch 159/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1942 - val_loss: 0.2701\n",
            "Epoch 160/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1939 - val_loss: 0.2700\n",
            "Epoch 161/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1936 - val_loss: 0.2701\n",
            "Epoch 162/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1934 - val_loss: 0.2701\n",
            "Epoch 163/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1930 - val_loss: 0.2701\n",
            "Epoch 164/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1930 - val_loss: 0.2701\n",
            "Epoch 165/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1926 - val_loss: 0.2702\n",
            "Epoch 166/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1923 - val_loss: 0.2701\n",
            "Epoch 167/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1922 - val_loss: 0.2701\n",
            "Epoch 168/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1918 - val_loss: 0.2701\n",
            "Epoch 169/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1918 - val_loss: 0.2702\n",
            "Epoch 170/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1914 - val_loss: 0.2701\n",
            "Epoch 171/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1912 - val_loss: 0.2701\n",
            "Epoch 172/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1911 - val_loss: 0.2702\n",
            "Epoch 173/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1905 - val_loss: 0.2701\n",
            "Epoch 174/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1905 - val_loss: 0.2702\n",
            "Epoch 175/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1902 - val_loss: 0.2701\n",
            "Epoch 176/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1898 - val_loss: 0.2701\n",
            "Epoch 177/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1899 - val_loss: 0.2702\n",
            "Epoch 178/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1894 - val_loss: 0.2703\n",
            "Epoch 179/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1892 - val_loss: 0.2702\n",
            "Epoch 180/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1893 - val_loss: 0.2702\n",
            "Epoch 181/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1890 - val_loss: 0.2701\n",
            "Epoch 182/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1885 - val_loss: 0.2703\n",
            "Epoch 183/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1884 - val_loss: 0.2702\n",
            "Epoch 184/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1882 - val_loss: 0.2703\n",
            "Epoch 185/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1879 - val_loss: 0.2703\n",
            "Epoch 186/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1877 - val_loss: 0.2704\n",
            "Epoch 187/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1876 - val_loss: 0.2703\n",
            "Epoch 188/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1874 - val_loss: 0.2703\n",
            "Epoch 189/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1872 - val_loss: 0.2703\n",
            "Epoch 190/200\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 0.1871 - val_loss: 0.2704\n",
            "Epoch 191/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1867 - val_loss: 0.2704\n",
            "Epoch 192/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1865 - val_loss: 0.2706\n",
            "Epoch 193/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1863 - val_loss: 0.2703\n",
            "Epoch 194/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1863 - val_loss: 0.2704\n",
            "Epoch 195/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1860 - val_loss: 0.2704\n",
            "Epoch 196/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1859 - val_loss: 0.2705\n",
            "Epoch 197/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1856 - val_loss: 0.2704\n",
            "Epoch 198/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1852 - val_loss: 0.2705\n",
            "Epoch 199/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1853 - val_loss: 0.2705\n",
            "Epoch 200/200\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 0.1852 - val_loss: 0.2705\n",
            "10/10 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 256, 0.18517929315567017, 0.2704865634441376, 0.032206766, 3.735661328173447, 0.21626717652441943)\n",
            "Epoch 1/200\n",
            "66/66 [==============================] - 2s 17ms/step - loss: 0.5391 - val_loss: 0.5357\n",
            "Epoch 2/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.4027 - val_loss: 0.4720\n",
            "Epoch 3/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3712 - val_loss: 0.4277\n",
            "Epoch 4/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3517 - val_loss: 0.3974\n",
            "Epoch 5/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3370 - val_loss: 0.3773\n",
            "Epoch 6/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3249 - val_loss: 0.3622\n",
            "Epoch 7/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3151 - val_loss: 0.3511\n",
            "Epoch 8/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3071 - val_loss: 0.3424\n",
            "Epoch 9/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3005 - val_loss: 0.3355\n",
            "Epoch 10/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2950 - val_loss: 0.3295\n",
            "Epoch 11/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2903 - val_loss: 0.3244\n",
            "Epoch 12/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2860 - val_loss: 0.3200\n",
            "Epoch 13/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2823 - val_loss: 0.3160\n",
            "Epoch 14/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2789 - val_loss: 0.3125\n",
            "Epoch 15/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2761 - val_loss: 0.3094\n",
            "Epoch 16/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2736 - val_loss: 0.3066\n",
            "Epoch 17/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2714 - val_loss: 0.3042\n",
            "Epoch 18/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2694 - val_loss: 0.3020\n",
            "Epoch 19/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2676 - val_loss: 0.3001\n",
            "Epoch 20/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2660 - val_loss: 0.2983\n",
            "Epoch 21/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2646 - val_loss: 0.2969\n",
            "Epoch 22/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2633 - val_loss: 0.2955\n",
            "Epoch 23/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2623 - val_loss: 0.2944\n",
            "Epoch 24/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2614 - val_loss: 0.2934\n",
            "Epoch 25/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2605 - val_loss: 0.2925\n",
            "Epoch 26/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2597 - val_loss: 0.2917\n",
            "Epoch 27/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2591 - val_loss: 0.2911\n",
            "Epoch 28/200\n",
            "66/66 [==============================] - 1s 12ms/step - loss: 0.2585 - val_loss: 0.2905\n",
            "Epoch 29/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2581 - val_loss: 0.2900\n",
            "Epoch 30/200\n",
            "66/66 [==============================] - 1s 12ms/step - loss: 0.2577 - val_loss: 0.2895\n",
            "Epoch 31/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2573 - val_loss: 0.2892\n",
            "Epoch 32/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2568 - val_loss: 0.2888\n",
            "Epoch 33/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2566 - val_loss: 0.2885\n",
            "Epoch 34/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2564 - val_loss: 0.2882\n",
            "Epoch 35/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2561 - val_loss: 0.2879\n",
            "Epoch 36/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2557 - val_loss: 0.2877\n",
            "Epoch 37/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2556 - val_loss: 0.2875\n",
            "Epoch 38/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2555 - val_loss: 0.2873\n",
            "Epoch 39/200\n",
            "66/66 [==============================] - 1s 12ms/step - loss: 0.2552 - val_loss: 0.2871\n",
            "Epoch 40/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2550 - val_loss: 0.2869\n",
            "Epoch 41/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2550 - val_loss: 0.2867\n",
            "Epoch 42/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2546 - val_loss: 0.2866\n",
            "Epoch 43/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2544 - val_loss: 0.2864\n",
            "Epoch 44/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2543 - val_loss: 0.2863\n",
            "Epoch 45/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2541 - val_loss: 0.2861\n",
            "Epoch 46/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2540 - val_loss: 0.2860\n",
            "Epoch 47/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2538 - val_loss: 0.2859\n",
            "Epoch 48/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2536 - val_loss: 0.2857\n",
            "Epoch 49/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2534 - val_loss: 0.2856\n",
            "Epoch 50/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2533 - val_loss: 0.2855\n",
            "Epoch 51/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2531 - val_loss: 0.2854\n",
            "Epoch 52/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2530 - val_loss: 0.2853\n",
            "Epoch 53/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2530 - val_loss: 0.2852\n",
            "Epoch 54/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2527 - val_loss: 0.2851\n",
            "Epoch 55/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2526 - val_loss: 0.2850\n",
            "Epoch 56/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2525 - val_loss: 0.2849\n",
            "Epoch 57/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2522 - val_loss: 0.2848\n",
            "Epoch 58/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2521 - val_loss: 0.2847\n",
            "Epoch 59/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2519 - val_loss: 0.2846\n",
            "Epoch 60/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2517 - val_loss: 0.2845\n",
            "Epoch 61/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2516 - val_loss: 0.2844\n",
            "Epoch 62/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2513 - val_loss: 0.2843\n",
            "Epoch 63/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2514 - val_loss: 0.2843\n",
            "Epoch 64/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2511 - val_loss: 0.2842\n",
            "Epoch 65/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2508 - val_loss: 0.2840\n",
            "Epoch 66/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2507 - val_loss: 0.2840\n",
            "Epoch 67/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2504 - val_loss: 0.2839\n",
            "Epoch 68/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2503 - val_loss: 0.2838\n",
            "Epoch 69/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2501 - val_loss: 0.2837\n",
            "Epoch 70/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2499 - val_loss: 0.2836\n",
            "Epoch 71/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2496 - val_loss: 0.2835\n",
            "Epoch 72/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2495 - val_loss: 0.2834\n",
            "Epoch 73/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2493 - val_loss: 0.2833\n",
            "Epoch 74/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2491 - val_loss: 0.2833\n",
            "Epoch 75/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2487 - val_loss: 0.2831\n",
            "Epoch 76/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2485 - val_loss: 0.2831\n",
            "Epoch 77/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2483 - val_loss: 0.2830\n",
            "Epoch 78/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2480 - val_loss: 0.2828\n",
            "Epoch 79/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2478 - val_loss: 0.2827\n",
            "Epoch 80/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2476 - val_loss: 0.2827\n",
            "Epoch 81/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2474 - val_loss: 0.2826\n",
            "Epoch 82/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2471 - val_loss: 0.2825\n",
            "Epoch 83/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2468 - val_loss: 0.2824\n",
            "Epoch 84/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2468 - val_loss: 0.2823\n",
            "Epoch 85/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2464 - val_loss: 0.2822\n",
            "Epoch 86/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2461 - val_loss: 0.2821\n",
            "Epoch 87/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2458 - val_loss: 0.2820\n",
            "Epoch 88/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2455 - val_loss: 0.2819\n",
            "Epoch 89/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2454 - val_loss: 0.2818\n",
            "Epoch 90/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2450 - val_loss: 0.2817\n",
            "Epoch 91/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2448 - val_loss: 0.2816\n",
            "Epoch 92/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2444 - val_loss: 0.2815\n",
            "Epoch 93/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2442 - val_loss: 0.2814\n",
            "Epoch 94/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2439 - val_loss: 0.2813\n",
            "Epoch 95/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2436 - val_loss: 0.2812\n",
            "Epoch 96/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2432 - val_loss: 0.2811\n",
            "Epoch 97/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2430 - val_loss: 0.2810\n",
            "Epoch 98/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2428 - val_loss: 0.2809\n",
            "Epoch 99/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2425 - val_loss: 0.2808\n",
            "Epoch 100/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2424 - val_loss: 0.2807\n",
            "Epoch 101/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2421 - val_loss: 0.2806\n",
            "Epoch 102/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2415 - val_loss: 0.2805\n",
            "Epoch 103/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2412 - val_loss: 0.2804\n",
            "Epoch 104/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2410 - val_loss: 0.2803\n",
            "Epoch 105/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2408 - val_loss: 0.2802\n",
            "Epoch 106/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2404 - val_loss: 0.2801\n",
            "Epoch 107/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2403 - val_loss: 0.2800\n",
            "Epoch 108/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2400 - val_loss: 0.2799\n",
            "Epoch 109/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2395 - val_loss: 0.2798\n",
            "Epoch 110/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2393 - val_loss: 0.2797\n",
            "Epoch 111/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2390 - val_loss: 0.2796\n",
            "Epoch 112/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2387 - val_loss: 0.2795\n",
            "Epoch 113/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2383 - val_loss: 0.2794\n",
            "Epoch 114/200\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2381 - val_loss: 0.2793\n",
            "Epoch 115/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2376 - val_loss: 0.2792\n",
            "Epoch 116/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2374 - val_loss: 0.2791\n",
            "Epoch 117/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2371 - val_loss: 0.2790\n",
            "Epoch 118/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2369 - val_loss: 0.2789\n",
            "Epoch 119/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2365 - val_loss: 0.2788\n",
            "Epoch 120/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2362 - val_loss: 0.2787\n",
            "Epoch 121/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2358 - val_loss: 0.2787\n",
            "Epoch 122/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2356 - val_loss: 0.2786\n",
            "Epoch 123/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2353 - val_loss: 0.2785\n",
            "Epoch 124/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2352 - val_loss: 0.2784\n",
            "Epoch 125/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2348 - val_loss: 0.2783\n",
            "Epoch 126/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2344 - val_loss: 0.2782\n",
            "Epoch 127/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2342 - val_loss: 0.2781\n",
            "Epoch 128/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2339 - val_loss: 0.2780\n",
            "Epoch 129/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2336 - val_loss: 0.2780\n",
            "Epoch 130/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2332 - val_loss: 0.2779\n",
            "Epoch 131/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2329 - val_loss: 0.2778\n",
            "Epoch 132/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2326 - val_loss: 0.2777\n",
            "Epoch 133/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2323 - val_loss: 0.2776\n",
            "Epoch 134/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2321 - val_loss: 0.2776\n",
            "Epoch 135/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2318 - val_loss: 0.2775\n",
            "Epoch 136/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2315 - val_loss: 0.2774\n",
            "Epoch 137/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2312 - val_loss: 0.2773\n",
            "Epoch 138/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2309 - val_loss: 0.2772\n",
            "Epoch 139/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2306 - val_loss: 0.2771\n",
            "Epoch 140/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2304 - val_loss: 0.2771\n",
            "Epoch 141/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2301 - val_loss: 0.2770\n",
            "Epoch 142/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2297 - val_loss: 0.2769\n",
            "Epoch 143/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2295 - val_loss: 0.2768\n",
            "Epoch 144/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2291 - val_loss: 0.2768\n",
            "Epoch 145/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2288 - val_loss: 0.2767\n",
            "Epoch 146/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2285 - val_loss: 0.2766\n",
            "Epoch 147/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2283 - val_loss: 0.2766\n",
            "Epoch 148/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2280 - val_loss: 0.2765\n",
            "Epoch 149/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2277 - val_loss: 0.2764\n",
            "Epoch 150/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2274 - val_loss: 0.2763\n",
            "Epoch 151/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2272 - val_loss: 0.2762\n",
            "Epoch 152/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2269 - val_loss: 0.2762\n",
            "Epoch 153/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2265 - val_loss: 0.2761\n",
            "Epoch 154/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2262 - val_loss: 0.2760\n",
            "Epoch 155/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2261 - val_loss: 0.2759\n",
            "Epoch 156/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2258 - val_loss: 0.2759\n",
            "Epoch 157/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2254 - val_loss: 0.2759\n",
            "Epoch 158/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2253 - val_loss: 0.2758\n",
            "Epoch 159/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2249 - val_loss: 0.2757\n",
            "Epoch 160/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2247 - val_loss: 0.2757\n",
            "Epoch 161/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2245 - val_loss: 0.2756\n",
            "Epoch 162/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2242 - val_loss: 0.2755\n",
            "Epoch 163/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2240 - val_loss: 0.2755\n",
            "Epoch 164/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2235 - val_loss: 0.2755\n",
            "Epoch 165/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2232 - val_loss: 0.2754\n",
            "Epoch 166/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2231 - val_loss: 0.2753\n",
            "Epoch 167/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2228 - val_loss: 0.2753\n",
            "Epoch 168/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2224 - val_loss: 0.2751\n",
            "Epoch 169/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2222 - val_loss: 0.2751\n",
            "Epoch 170/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2220 - val_loss: 0.2750\n",
            "Epoch 171/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2217 - val_loss: 0.2750\n",
            "Epoch 172/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2214 - val_loss: 0.2749\n",
            "Epoch 173/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2213 - val_loss: 0.2749\n",
            "Epoch 174/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2210 - val_loss: 0.2748\n",
            "Epoch 175/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2208 - val_loss: 0.2748\n",
            "Epoch 176/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2204 - val_loss: 0.2748\n",
            "Epoch 177/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2202 - val_loss: 0.2747\n",
            "Epoch 178/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2200 - val_loss: 0.2746\n",
            "Epoch 179/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2198 - val_loss: 0.2746\n",
            "Epoch 180/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2194 - val_loss: 0.2746\n",
            "Epoch 181/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2191 - val_loss: 0.2744\n",
            "Epoch 182/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2189 - val_loss: 0.2744\n",
            "Epoch 183/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2185 - val_loss: 0.2744\n",
            "Epoch 184/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2184 - val_loss: 0.2743\n",
            "Epoch 185/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2182 - val_loss: 0.2744\n",
            "Epoch 186/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2179 - val_loss: 0.2742\n",
            "Epoch 187/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2178 - val_loss: 0.2742\n",
            "Epoch 188/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2175 - val_loss: 0.2742\n",
            "Epoch 189/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2173 - val_loss: 0.2741\n",
            "Epoch 190/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2169 - val_loss: 0.2741\n",
            "Epoch 191/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2168 - val_loss: 0.2740\n",
            "Epoch 192/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2164 - val_loss: 0.2740\n",
            "Epoch 193/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2163 - val_loss: 0.2739\n",
            "Epoch 194/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2161 - val_loss: 0.2739\n",
            "Epoch 195/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2158 - val_loss: 0.2739\n",
            "Epoch 196/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2156 - val_loss: 0.2738\n",
            "Epoch 197/200\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2154 - val_loss: 0.2738\n",
            "Epoch 198/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2151 - val_loss: 0.2737\n",
            "Epoch 199/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2148 - val_loss: 0.2737\n",
            "Epoch 200/200\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.2147 - val_loss: 0.2737\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 512, 0.21466189622879028, 0.2736956477165222, 0.029838169, 3.838621240070956, 0.20219654885068924)\n",
            "Epoch 1/200\n",
            "67/67 [==============================] - 2s 18ms/step - loss: 0.5339 - val_loss: 0.5264\n",
            "Epoch 2/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.4012 - val_loss: 0.4657\n",
            "Epoch 3/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3707 - val_loss: 0.4220\n",
            "Epoch 4/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3505 - val_loss: 0.3914\n",
            "Epoch 5/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3353 - val_loss: 0.3710\n",
            "Epoch 6/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3229 - val_loss: 0.3564\n",
            "Epoch 7/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3134 - val_loss: 0.3458\n",
            "Epoch 8/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3055 - val_loss: 0.3379\n",
            "Epoch 9/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2991 - val_loss: 0.3313\n",
            "Epoch 10/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2939 - val_loss: 0.3257\n",
            "Epoch 11/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2891 - val_loss: 0.3208\n",
            "Epoch 12/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2851 - val_loss: 0.3164\n",
            "Epoch 13/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2814 - val_loss: 0.3125\n",
            "Epoch 14/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2781 - val_loss: 0.3089\n",
            "Epoch 15/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2751 - val_loss: 0.3057\n",
            "Epoch 16/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2724 - val_loss: 0.3029\n",
            "Epoch 17/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2701 - val_loss: 0.3004\n",
            "Epoch 18/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2681 - val_loss: 0.2983\n",
            "Epoch 19/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2663 - val_loss: 0.2963\n",
            "Epoch 20/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2646 - val_loss: 0.2946\n",
            "Epoch 21/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2633 - val_loss: 0.2932\n",
            "Epoch 22/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2622 - val_loss: 0.2920\n",
            "Epoch 23/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2611 - val_loss: 0.2909\n",
            "Epoch 24/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2602 - val_loss: 0.2899\n",
            "Epoch 25/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2593 - val_loss: 0.2891\n",
            "Epoch 26/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2587 - val_loss: 0.2884\n",
            "Epoch 27/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2581 - val_loss: 0.2877\n",
            "Epoch 28/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2575 - val_loss: 0.2872\n",
            "Epoch 29/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2571 - val_loss: 0.2867\n",
            "Epoch 30/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2567 - val_loss: 0.2863\n",
            "Epoch 31/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2562 - val_loss: 0.2858\n",
            "Epoch 32/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2559 - val_loss: 0.2855\n",
            "Epoch 33/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2555 - val_loss: 0.2852\n",
            "Epoch 34/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2553 - val_loss: 0.2849\n",
            "Epoch 35/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2549 - val_loss: 0.2846\n",
            "Epoch 36/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2547 - val_loss: 0.2844\n",
            "Epoch 37/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2545 - val_loss: 0.2842\n",
            "Epoch 38/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2544 - val_loss: 0.2840\n",
            "Epoch 39/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2541 - val_loss: 0.2838\n",
            "Epoch 40/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2539 - val_loss: 0.2836\n",
            "Epoch 41/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2537 - val_loss: 0.2835\n",
            "Epoch 42/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2535 - val_loss: 0.2833\n",
            "Epoch 43/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2534 - val_loss: 0.2832\n",
            "Epoch 44/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2530 - val_loss: 0.2830\n",
            "Epoch 45/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2530 - val_loss: 0.2829\n",
            "Epoch 46/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2529 - val_loss: 0.2828\n",
            "Epoch 47/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2526 - val_loss: 0.2826\n",
            "Epoch 48/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2525 - val_loss: 0.2826\n",
            "Epoch 49/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2524 - val_loss: 0.2824\n",
            "Epoch 50/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2523 - val_loss: 0.2823\n",
            "Epoch 51/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2520 - val_loss: 0.2822\n",
            "Epoch 52/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2518 - val_loss: 0.2821\n",
            "Epoch 53/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2516 - val_loss: 0.2820\n",
            "Epoch 54/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2515 - val_loss: 0.2819\n",
            "Epoch 55/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2514 - val_loss: 0.2818\n",
            "Epoch 56/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2512 - val_loss: 0.2817\n",
            "Epoch 57/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2511 - val_loss: 0.2816\n",
            "Epoch 58/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2509 - val_loss: 0.2815\n",
            "Epoch 59/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2506 - val_loss: 0.2814\n",
            "Epoch 60/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2505 - val_loss: 0.2813\n",
            "Epoch 61/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2502 - val_loss: 0.2812\n",
            "Epoch 62/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2500 - val_loss: 0.2811\n",
            "Epoch 63/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2498 - val_loss: 0.2810\n",
            "Epoch 64/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2498 - val_loss: 0.2809\n",
            "Epoch 65/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2495 - val_loss: 0.2808\n",
            "Epoch 66/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2492 - val_loss: 0.2808\n",
            "Epoch 67/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2490 - val_loss: 0.2807\n",
            "Epoch 68/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2490 - val_loss: 0.2806\n",
            "Epoch 69/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2486 - val_loss: 0.2805\n",
            "Epoch 70/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2484 - val_loss: 0.2804\n",
            "Epoch 71/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2483 - val_loss: 0.2803\n",
            "Epoch 72/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2479 - val_loss: 0.2802\n",
            "Epoch 73/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2477 - val_loss: 0.2801\n",
            "Epoch 74/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2475 - val_loss: 0.2800\n",
            "Epoch 75/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2474 - val_loss: 0.2799\n",
            "Epoch 76/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2471 - val_loss: 0.2798\n",
            "Epoch 77/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2468 - val_loss: 0.2797\n",
            "Epoch 78/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2466 - val_loss: 0.2796\n",
            "Epoch 79/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2463 - val_loss: 0.2795\n",
            "Epoch 80/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2460 - val_loss: 0.2795\n",
            "Epoch 81/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2458 - val_loss: 0.2793\n",
            "Epoch 82/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2455 - val_loss: 0.2792\n",
            "Epoch 83/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2453 - val_loss: 0.2792\n",
            "Epoch 84/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2450 - val_loss: 0.2791\n",
            "Epoch 85/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2447 - val_loss: 0.2790\n",
            "Epoch 86/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2444 - val_loss: 0.2788\n",
            "Epoch 87/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2442 - val_loss: 0.2787\n",
            "Epoch 88/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2438 - val_loss: 0.2787\n",
            "Epoch 89/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2437 - val_loss: 0.2786\n",
            "Epoch 90/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2434 - val_loss: 0.2785\n",
            "Epoch 91/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2431 - val_loss: 0.2784\n",
            "Epoch 92/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2428 - val_loss: 0.2783\n",
            "Epoch 93/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2426 - val_loss: 0.2782\n",
            "Epoch 94/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2422 - val_loss: 0.2781\n",
            "Epoch 95/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2419 - val_loss: 0.2780\n",
            "Epoch 96/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2416 - val_loss: 0.2779\n",
            "Epoch 97/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2414 - val_loss: 0.2778\n",
            "Epoch 98/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2410 - val_loss: 0.2777\n",
            "Epoch 99/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2407 - val_loss: 0.2776\n",
            "Epoch 100/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2404 - val_loss: 0.2775\n",
            "Epoch 101/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2402 - val_loss: 0.2774\n",
            "Epoch 102/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2398 - val_loss: 0.2773\n",
            "Epoch 103/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2395 - val_loss: 0.2772\n",
            "Epoch 104/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2392 - val_loss: 0.2771\n",
            "Epoch 105/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2388 - val_loss: 0.2770\n",
            "Epoch 106/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2386 - val_loss: 0.2769\n",
            "Epoch 107/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2382 - val_loss: 0.2768\n",
            "Epoch 108/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2379 - val_loss: 0.2767\n",
            "Epoch 109/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2378 - val_loss: 0.2766\n",
            "Epoch 110/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2374 - val_loss: 0.2765\n",
            "Epoch 111/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2371 - val_loss: 0.2764\n",
            "Epoch 112/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2367 - val_loss: 0.2763\n",
            "Epoch 113/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2363 - val_loss: 0.2763\n",
            "Epoch 114/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2362 - val_loss: 0.2762\n",
            "Epoch 115/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2358 - val_loss: 0.2761\n",
            "Epoch 116/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2356 - val_loss: 0.2760\n",
            "Epoch 117/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2351 - val_loss: 0.2759\n",
            "Epoch 118/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2350 - val_loss: 0.2758\n",
            "Epoch 119/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2346 - val_loss: 0.2757\n",
            "Epoch 120/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2342 - val_loss: 0.2756\n",
            "Epoch 121/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2340 - val_loss: 0.2756\n",
            "Epoch 122/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2336 - val_loss: 0.2754\n",
            "Epoch 123/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2333 - val_loss: 0.2754\n",
            "Epoch 124/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2330 - val_loss: 0.2752\n",
            "Epoch 125/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2328 - val_loss: 0.2752\n",
            "Epoch 126/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2324 - val_loss: 0.2751\n",
            "Epoch 127/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2320 - val_loss: 0.2750\n",
            "Epoch 128/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2319 - val_loss: 0.2749\n",
            "Epoch 129/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2314 - val_loss: 0.2748\n",
            "Epoch 130/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2312 - val_loss: 0.2747\n",
            "Epoch 131/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2309 - val_loss: 0.2747\n",
            "Epoch 132/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2306 - val_loss: 0.2746\n",
            "Epoch 133/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2304 - val_loss: 0.2746\n",
            "Epoch 134/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2300 - val_loss: 0.2745\n",
            "Epoch 135/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2298 - val_loss: 0.2744\n",
            "Epoch 136/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2294 - val_loss: 0.2743\n",
            "Epoch 137/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2291 - val_loss: 0.2742\n",
            "Epoch 138/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2286 - val_loss: 0.2741\n",
            "Epoch 139/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2285 - val_loss: 0.2740\n",
            "Epoch 140/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2282 - val_loss: 0.2740\n",
            "Epoch 141/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2278 - val_loss: 0.2739\n",
            "Epoch 142/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2276 - val_loss: 0.2738\n",
            "Epoch 143/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2273 - val_loss: 0.2737\n",
            "Epoch 144/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2270 - val_loss: 0.2736\n",
            "Epoch 145/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2268 - val_loss: 0.2736\n",
            "Epoch 146/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2264 - val_loss: 0.2735\n",
            "Epoch 147/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2262 - val_loss: 0.2734\n",
            "Epoch 148/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2259 - val_loss: 0.2734\n",
            "Epoch 149/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2256 - val_loss: 0.2733\n",
            "Epoch 150/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2253 - val_loss: 0.2732\n",
            "Epoch 151/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2250 - val_loss: 0.2731\n",
            "Epoch 152/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2247 - val_loss: 0.2731\n",
            "Epoch 153/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2244 - val_loss: 0.2730\n",
            "Epoch 154/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2242 - val_loss: 0.2729\n",
            "Epoch 155/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2239 - val_loss: 0.2729\n",
            "Epoch 156/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2236 - val_loss: 0.2728\n",
            "Epoch 157/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2234 - val_loss: 0.2728\n",
            "Epoch 158/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2231 - val_loss: 0.2727\n",
            "Epoch 159/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2228 - val_loss: 0.2726\n",
            "Epoch 160/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2225 - val_loss: 0.2726\n",
            "Epoch 161/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2221 - val_loss: 0.2725\n",
            "Epoch 162/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2219 - val_loss: 0.2725\n",
            "Epoch 163/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2216 - val_loss: 0.2724\n",
            "Epoch 164/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2214 - val_loss: 0.2723\n",
            "Epoch 165/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2212 - val_loss: 0.2725\n",
            "Epoch 166/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2208 - val_loss: 0.2722\n",
            "Epoch 167/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2207 - val_loss: 0.2722\n",
            "Epoch 168/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2203 - val_loss: 0.2721\n",
            "Epoch 169/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2200 - val_loss: 0.2721\n",
            "Epoch 170/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2197 - val_loss: 0.2720\n",
            "Epoch 171/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2195 - val_loss: 0.2720\n",
            "Epoch 172/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2193 - val_loss: 0.2719\n",
            "Epoch 173/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2189 - val_loss: 0.2718\n",
            "Epoch 174/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2187 - val_loss: 0.2719\n",
            "Epoch 175/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2185 - val_loss: 0.2717\n",
            "Epoch 176/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2183 - val_loss: 0.2717\n",
            "Epoch 177/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2180 - val_loss: 0.2717\n",
            "Epoch 178/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2176 - val_loss: 0.2716\n",
            "Epoch 179/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2174 - val_loss: 0.2715\n",
            "Epoch 180/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2172 - val_loss: 0.2715\n",
            "Epoch 181/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2169 - val_loss: 0.2715\n",
            "Epoch 182/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2166 - val_loss: 0.2714\n",
            "Epoch 183/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2164 - val_loss: 0.2714\n",
            "Epoch 184/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2162 - val_loss: 0.2713\n",
            "Epoch 185/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2159 - val_loss: 0.2713\n",
            "Epoch 186/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2157 - val_loss: 0.2712\n",
            "Epoch 187/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2155 - val_loss: 0.2712\n",
            "Epoch 188/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2152 - val_loss: 0.2711\n",
            "Epoch 189/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2150 - val_loss: 0.2712\n",
            "Epoch 190/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2147 - val_loss: 0.2711\n",
            "Epoch 191/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2145 - val_loss: 0.2711\n",
            "Epoch 192/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2142 - val_loss: 0.2710\n",
            "Epoch 193/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2140 - val_loss: 0.2709\n",
            "Epoch 194/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2138 - val_loss: 0.2709\n",
            "Epoch 195/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2135 - val_loss: 0.2709\n",
            "Epoch 196/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2134 - val_loss: 0.2708\n",
            "Epoch 197/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2129 - val_loss: 0.2708\n",
            "Epoch 198/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2127 - val_loss: 0.2708\n",
            "Epoch 199/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2125 - val_loss: 0.2707\n",
            "Epoch 200/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2123 - val_loss: 0.2708\n",
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 512, 0.21230511367321014, 0.27075275778770447, 0.028877195, 3.8827544130057476, 0.23164165634347045)\n",
            "Epoch 1/200\n",
            "67/67 [==============================] - 2s 17ms/step - loss: 0.5388 - val_loss: 0.5248\n",
            "Epoch 2/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.4023 - val_loss: 0.4632\n",
            "Epoch 3/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3714 - val_loss: 0.4196\n",
            "Epoch 4/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3516 - val_loss: 0.3894\n",
            "Epoch 5/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3369 - val_loss: 0.3697\n",
            "Epoch 6/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3252 - val_loss: 0.3562\n",
            "Epoch 7/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.3154 - val_loss: 0.3455\n",
            "Epoch 8/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.3072 - val_loss: 0.3369\n",
            "Epoch 9/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.3005 - val_loss: 0.3297\n",
            "Epoch 10/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2946 - val_loss: 0.3236\n",
            "Epoch 11/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2898 - val_loss: 0.3186\n",
            "Epoch 12/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2855 - val_loss: 0.3142\n",
            "Epoch 13/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2820 - val_loss: 0.3105\n",
            "Epoch 14/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2787 - val_loss: 0.3071\n",
            "Epoch 15/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2759 - val_loss: 0.3041\n",
            "Epoch 16/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2734 - val_loss: 0.3015\n",
            "Epoch 17/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2710 - val_loss: 0.2992\n",
            "Epoch 18/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2690 - val_loss: 0.2971\n",
            "Epoch 19/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2673 - val_loss: 0.2953\n",
            "Epoch 20/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2658 - val_loss: 0.2937\n",
            "Epoch 21/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2643 - val_loss: 0.2923\n",
            "Epoch 22/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2632 - val_loss: 0.2911\n",
            "Epoch 23/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2622 - val_loss: 0.2900\n",
            "Epoch 24/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2614 - val_loss: 0.2890\n",
            "Epoch 25/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2605 - val_loss: 0.2882\n",
            "Epoch 26/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2598 - val_loss: 0.2876\n",
            "Epoch 27/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2592 - val_loss: 0.2869\n",
            "Epoch 28/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2587 - val_loss: 0.2863\n",
            "Epoch 29/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2582 - val_loss: 0.2859\n",
            "Epoch 30/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2578 - val_loss: 0.2854\n",
            "Epoch 31/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2574 - val_loss: 0.2850\n",
            "Epoch 32/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2570 - val_loss: 0.2846\n",
            "Epoch 33/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2567 - val_loss: 0.2843\n",
            "Epoch 34/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2564 - val_loss: 0.2840\n",
            "Epoch 35/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2562 - val_loss: 0.2838\n",
            "Epoch 36/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2560 - val_loss: 0.2836\n",
            "Epoch 37/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2557 - val_loss: 0.2834\n",
            "Epoch 38/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2555 - val_loss: 0.2832\n",
            "Epoch 39/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2553 - val_loss: 0.2830\n",
            "Epoch 40/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2550 - val_loss: 0.2828\n",
            "Epoch 41/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2549 - val_loss: 0.2826\n",
            "Epoch 42/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2547 - val_loss: 0.2825\n",
            "Epoch 43/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2546 - val_loss: 0.2824\n",
            "Epoch 44/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2543 - val_loss: 0.2822\n",
            "Epoch 45/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2542 - val_loss: 0.2821\n",
            "Epoch 46/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2539 - val_loss: 0.2820\n",
            "Epoch 47/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2538 - val_loss: 0.2819\n",
            "Epoch 48/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2537 - val_loss: 0.2818\n",
            "Epoch 49/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2535 - val_loss: 0.2817\n",
            "Epoch 50/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2534 - val_loss: 0.2815\n",
            "Epoch 51/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2532 - val_loss: 0.2815\n",
            "Epoch 52/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2529 - val_loss: 0.2813\n",
            "Epoch 53/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2529 - val_loss: 0.2812\n",
            "Epoch 54/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2527 - val_loss: 0.2812\n",
            "Epoch 55/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2525 - val_loss: 0.2811\n",
            "Epoch 56/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2524 - val_loss: 0.2810\n",
            "Epoch 57/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2522 - val_loss: 0.2809\n",
            "Epoch 58/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2520 - val_loss: 0.2808\n",
            "Epoch 59/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2518 - val_loss: 0.2807\n",
            "Epoch 60/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2517 - val_loss: 0.2806\n",
            "Epoch 61/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2514 - val_loss: 0.2805\n",
            "Epoch 62/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2512 - val_loss: 0.2804\n",
            "Epoch 63/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2510 - val_loss: 0.2803\n",
            "Epoch 64/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2508 - val_loss: 0.2802\n",
            "Epoch 65/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2506 - val_loss: 0.2802\n",
            "Epoch 66/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2505 - val_loss: 0.2801\n",
            "Epoch 67/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2503 - val_loss: 0.2800\n",
            "Epoch 68/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2501 - val_loss: 0.2799\n",
            "Epoch 69/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2499 - val_loss: 0.2798\n",
            "Epoch 70/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2496 - val_loss: 0.2798\n",
            "Epoch 71/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2494 - val_loss: 0.2797\n",
            "Epoch 72/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2491 - val_loss: 0.2795\n",
            "Epoch 73/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2488 - val_loss: 0.2794\n",
            "Epoch 74/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2487 - val_loss: 0.2794\n",
            "Epoch 75/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2484 - val_loss: 0.2793\n",
            "Epoch 76/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2483 - val_loss: 0.2791\n",
            "Epoch 77/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2478 - val_loss: 0.2791\n",
            "Epoch 78/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2476 - val_loss: 0.2789\n",
            "Epoch 79/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2474 - val_loss: 0.2789\n",
            "Epoch 80/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2472 - val_loss: 0.2788\n",
            "Epoch 81/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2469 - val_loss: 0.2787\n",
            "Epoch 82/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2467 - val_loss: 0.2786\n",
            "Epoch 83/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2462 - val_loss: 0.2785\n",
            "Epoch 84/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2461 - val_loss: 0.2784\n",
            "Epoch 85/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2458 - val_loss: 0.2783\n",
            "Epoch 86/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2455 - val_loss: 0.2783\n",
            "Epoch 87/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2452 - val_loss: 0.2782\n",
            "Epoch 88/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2450 - val_loss: 0.2780\n",
            "Epoch 89/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2446 - val_loss: 0.2780\n",
            "Epoch 90/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2443 - val_loss: 0.2779\n",
            "Epoch 91/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2440 - val_loss: 0.2778\n",
            "Epoch 92/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2437 - val_loss: 0.2777\n",
            "Epoch 93/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2436 - val_loss: 0.2776\n",
            "Epoch 94/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2433 - val_loss: 0.2775\n",
            "Epoch 95/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2429 - val_loss: 0.2774\n",
            "Epoch 96/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2426 - val_loss: 0.2773\n",
            "Epoch 97/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2422 - val_loss: 0.2772\n",
            "Epoch 98/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2419 - val_loss: 0.2771\n",
            "Epoch 99/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2416 - val_loss: 0.2770\n",
            "Epoch 100/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2413 - val_loss: 0.2769\n",
            "Epoch 101/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2410 - val_loss: 0.2768\n",
            "Epoch 102/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2407 - val_loss: 0.2767\n",
            "Epoch 103/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2405 - val_loss: 0.2766\n",
            "Epoch 104/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2401 - val_loss: 0.2766\n",
            "Epoch 105/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2398 - val_loss: 0.2765\n",
            "Epoch 106/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2395 - val_loss: 0.2764\n",
            "Epoch 107/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2391 - val_loss: 0.2762\n",
            "Epoch 108/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2389 - val_loss: 0.2762\n",
            "Epoch 109/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2385 - val_loss: 0.2761\n",
            "Epoch 110/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2381 - val_loss: 0.2760\n",
            "Epoch 111/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2379 - val_loss: 0.2759\n",
            "Epoch 112/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2376 - val_loss: 0.2758\n",
            "Epoch 113/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2373 - val_loss: 0.2758\n",
            "Epoch 114/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2370 - val_loss: 0.2756\n",
            "Epoch 115/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2366 - val_loss: 0.2755\n",
            "Epoch 116/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2362 - val_loss: 0.2755\n",
            "Epoch 117/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2360 - val_loss: 0.2754\n",
            "Epoch 118/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2356 - val_loss: 0.2753\n",
            "Epoch 119/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2352 - val_loss: 0.2752\n",
            "Epoch 120/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2350 - val_loss: 0.2751\n",
            "Epoch 121/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2347 - val_loss: 0.2751\n",
            "Epoch 122/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2343 - val_loss: 0.2750\n",
            "Epoch 123/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2340 - val_loss: 0.2749\n",
            "Epoch 124/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2337 - val_loss: 0.2748\n",
            "Epoch 125/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2333 - val_loss: 0.2747\n",
            "Epoch 126/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2330 - val_loss: 0.2746\n",
            "Epoch 127/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2328 - val_loss: 0.2745\n",
            "Epoch 128/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2325 - val_loss: 0.2745\n",
            "Epoch 129/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2321 - val_loss: 0.2744\n",
            "Epoch 130/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2319 - val_loss: 0.2743\n",
            "Epoch 131/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2315 - val_loss: 0.2742\n",
            "Epoch 132/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2312 - val_loss: 0.2742\n",
            "Epoch 133/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2310 - val_loss: 0.2741\n",
            "Epoch 134/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2306 - val_loss: 0.2740\n",
            "Epoch 135/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2303 - val_loss: 0.2739\n",
            "Epoch 136/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2300 - val_loss: 0.2739\n",
            "Epoch 137/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2297 - val_loss: 0.2737\n",
            "Epoch 138/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2294 - val_loss: 0.2737\n",
            "Epoch 139/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2290 - val_loss: 0.2736\n",
            "Epoch 140/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2288 - val_loss: 0.2735\n",
            "Epoch 141/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2285 - val_loss: 0.2736\n",
            "Epoch 142/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2281 - val_loss: 0.2734\n",
            "Epoch 143/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2278 - val_loss: 0.2733\n",
            "Epoch 144/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2275 - val_loss: 0.2733\n",
            "Epoch 145/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2273 - val_loss: 0.2732\n",
            "Epoch 146/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2269 - val_loss: 0.2731\n",
            "Epoch 147/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2265 - val_loss: 0.2732\n",
            "Epoch 148/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2263 - val_loss: 0.2731\n",
            "Epoch 149/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2260 - val_loss: 0.2729\n",
            "Epoch 150/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2257 - val_loss: 0.2728\n",
            "Epoch 151/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2256 - val_loss: 0.2729\n",
            "Epoch 152/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2252 - val_loss: 0.2728\n",
            "Epoch 153/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2248 - val_loss: 0.2727\n",
            "Epoch 154/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2245 - val_loss: 0.2726\n",
            "Epoch 155/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2242 - val_loss: 0.2726\n",
            "Epoch 156/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2240 - val_loss: 0.2725\n",
            "Epoch 157/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2238 - val_loss: 0.2725\n",
            "Epoch 158/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2233 - val_loss: 0.2724\n",
            "Epoch 159/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2232 - val_loss: 0.2723\n",
            "Epoch 160/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2230 - val_loss: 0.2722\n",
            "Epoch 161/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2226 - val_loss: 0.2722\n",
            "Epoch 162/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2223 - val_loss: 0.2722\n",
            "Epoch 163/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2219 - val_loss: 0.2721\n",
            "Epoch 164/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2217 - val_loss: 0.2721\n",
            "Epoch 165/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2215 - val_loss: 0.2719\n",
            "Epoch 166/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2211 - val_loss: 0.2719\n",
            "Epoch 167/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2208 - val_loss: 0.2719\n",
            "Epoch 168/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2206 - val_loss: 0.2718\n",
            "Epoch 169/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2204 - val_loss: 0.2718\n",
            "Epoch 170/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2201 - val_loss: 0.2717\n",
            "Epoch 171/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2199 - val_loss: 0.2717\n",
            "Epoch 172/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2196 - val_loss: 0.2716\n",
            "Epoch 173/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2192 - val_loss: 0.2715\n",
            "Epoch 174/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2190 - val_loss: 0.2718\n",
            "Epoch 175/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2188 - val_loss: 0.2714\n",
            "Epoch 176/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2185 - val_loss: 0.2716\n",
            "Epoch 177/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2183 - val_loss: 0.2714\n",
            "Epoch 178/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2181 - val_loss: 0.2713\n",
            "Epoch 179/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2177 - val_loss: 0.2713\n",
            "Epoch 180/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2175 - val_loss: 0.2712\n",
            "Epoch 181/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2172 - val_loss: 0.2712\n",
            "Epoch 182/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2170 - val_loss: 0.2711\n",
            "Epoch 183/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2167 - val_loss: 0.2711\n",
            "Epoch 184/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2165 - val_loss: 0.2711\n",
            "Epoch 185/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2160 - val_loss: 0.2710\n",
            "Epoch 186/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2159 - val_loss: 0.2710\n",
            "Epoch 187/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2157 - val_loss: 0.2709\n",
            "Epoch 188/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2155 - val_loss: 0.2709\n",
            "Epoch 189/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2152 - val_loss: 0.2710\n",
            "Epoch 190/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2149 - val_loss: 0.2709\n",
            "Epoch 191/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2146 - val_loss: 0.2709\n",
            "Epoch 192/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2144 - val_loss: 0.2707\n",
            "Epoch 193/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2143 - val_loss: 0.2708\n",
            "Epoch 194/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2139 - val_loss: 0.2707\n",
            "Epoch 195/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2137 - val_loss: 0.2707\n",
            "Epoch 196/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2135 - val_loss: 0.2706\n",
            "Epoch 197/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2132 - val_loss: 0.2705\n",
            "Epoch 198/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2130 - val_loss: 0.2706\n",
            "Epoch 199/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2127 - val_loss: 0.2705\n",
            "Epoch 200/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2125 - val_loss: 0.2705\n",
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 512, 0.2125099152326584, 0.2704858183860779, 0.026916504, 3.817905524150574, 0.201529226262117)\n",
            "Epoch 1/200\n",
            "67/67 [==============================] - 2s 18ms/step - loss: 0.5373 - val_loss: 0.5282\n",
            "Epoch 2/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3999 - val_loss: 0.4656\n",
            "Epoch 3/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3687 - val_loss: 0.4229\n",
            "Epoch 4/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3495 - val_loss: 0.3931\n",
            "Epoch 5/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3346 - val_loss: 0.3727\n",
            "Epoch 6/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3224 - val_loss: 0.3585\n",
            "Epoch 7/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3127 - val_loss: 0.3479\n",
            "Epoch 8/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3049 - val_loss: 0.3396\n",
            "Epoch 9/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2984 - val_loss: 0.3326\n",
            "Epoch 10/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2927 - val_loss: 0.3268\n",
            "Epoch 11/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2879 - val_loss: 0.3217\n",
            "Epoch 12/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2838 - val_loss: 0.3172\n",
            "Epoch 13/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2801 - val_loss: 0.3133\n",
            "Epoch 14/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2769 - val_loss: 0.3098\n",
            "Epoch 15/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2740 - val_loss: 0.3068\n",
            "Epoch 16/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2716 - val_loss: 0.3042\n",
            "Epoch 17/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2693 - val_loss: 0.3018\n",
            "Epoch 18/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2673 - val_loss: 0.2997\n",
            "Epoch 19/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2656 - val_loss: 0.2978\n",
            "Epoch 20/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2642 - val_loss: 0.2962\n",
            "Epoch 21/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2627 - val_loss: 0.2947\n",
            "Epoch 22/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2615 - val_loss: 0.2934\n",
            "Epoch 23/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2605 - val_loss: 0.2924\n",
            "Epoch 24/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2595 - val_loss: 0.2914\n",
            "Epoch 25/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2588 - val_loss: 0.2905\n",
            "Epoch 26/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2580 - val_loss: 0.2898\n",
            "Epoch 27/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2574 - val_loss: 0.2891\n",
            "Epoch 28/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2569 - val_loss: 0.2886\n",
            "Epoch 29/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2566 - val_loss: 0.2881\n",
            "Epoch 30/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2561 - val_loss: 0.2877\n",
            "Epoch 31/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2557 - val_loss: 0.2873\n",
            "Epoch 32/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2554 - val_loss: 0.2869\n",
            "Epoch 33/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2551 - val_loss: 0.2866\n",
            "Epoch 34/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2549 - val_loss: 0.2864\n",
            "Epoch 35/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2545 - val_loss: 0.2861\n",
            "Epoch 36/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2544 - val_loss: 0.2859\n",
            "Epoch 37/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2541 - val_loss: 0.2856\n",
            "Epoch 38/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2539 - val_loss: 0.2854\n",
            "Epoch 39/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2536 - val_loss: 0.2853\n",
            "Epoch 40/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2535 - val_loss: 0.2851\n",
            "Epoch 41/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2533 - val_loss: 0.2849\n",
            "Epoch 42/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2532 - val_loss: 0.2848\n",
            "Epoch 43/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2531 - val_loss: 0.2846\n",
            "Epoch 44/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2529 - val_loss: 0.2845\n",
            "Epoch 45/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2527 - val_loss: 0.2844\n",
            "Epoch 46/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2525 - val_loss: 0.2842\n",
            "Epoch 47/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2524 - val_loss: 0.2841\n",
            "Epoch 48/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2522 - val_loss: 0.2840\n",
            "Epoch 49/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2521 - val_loss: 0.2839\n",
            "Epoch 50/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2519 - val_loss: 0.2838\n",
            "Epoch 51/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2516 - val_loss: 0.2837\n",
            "Epoch 52/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2516 - val_loss: 0.2836\n",
            "Epoch 53/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2514 - val_loss: 0.2834\n",
            "Epoch 54/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2512 - val_loss: 0.2833\n",
            "Epoch 55/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2511 - val_loss: 0.2832\n",
            "Epoch 56/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2508 - val_loss: 0.2832\n",
            "Epoch 57/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2507 - val_loss: 0.2831\n",
            "Epoch 58/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2506 - val_loss: 0.2830\n",
            "Epoch 59/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2504 - val_loss: 0.2829\n",
            "Epoch 60/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2503 - val_loss: 0.2828\n",
            "Epoch 61/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2501 - val_loss: 0.2827\n",
            "Epoch 62/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2498 - val_loss: 0.2826\n",
            "Epoch 63/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2497 - val_loss: 0.2825\n",
            "Epoch 64/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2495 - val_loss: 0.2825\n",
            "Epoch 65/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2493 - val_loss: 0.2823\n",
            "Epoch 66/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2490 - val_loss: 0.2822\n",
            "Epoch 67/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2488 - val_loss: 0.2821\n",
            "Epoch 68/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2486 - val_loss: 0.2820\n",
            "Epoch 69/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2484 - val_loss: 0.2819\n",
            "Epoch 70/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2483 - val_loss: 0.2819\n",
            "Epoch 71/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2480 - val_loss: 0.2818\n",
            "Epoch 72/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2477 - val_loss: 0.2817\n",
            "Epoch 73/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2475 - val_loss: 0.2816\n",
            "Epoch 74/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2473 - val_loss: 0.2815\n",
            "Epoch 75/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2471 - val_loss: 0.2814\n",
            "Epoch 76/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2468 - val_loss: 0.2813\n",
            "Epoch 77/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2466 - val_loss: 0.2812\n",
            "Epoch 78/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2464 - val_loss: 0.2811\n",
            "Epoch 79/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2461 - val_loss: 0.2810\n",
            "Epoch 80/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2459 - val_loss: 0.2809\n",
            "Epoch 81/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2458 - val_loss: 0.2808\n",
            "Epoch 82/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2455 - val_loss: 0.2807\n",
            "Epoch 83/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2451 - val_loss: 0.2806\n",
            "Epoch 84/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2449 - val_loss: 0.2805\n",
            "Epoch 85/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2446 - val_loss: 0.2804\n",
            "Epoch 86/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2444 - val_loss: 0.2804\n",
            "Epoch 87/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2440 - val_loss: 0.2802\n",
            "Epoch 88/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2437 - val_loss: 0.2801\n",
            "Epoch 89/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2434 - val_loss: 0.2800\n",
            "Epoch 90/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2433 - val_loss: 0.2799\n",
            "Epoch 91/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2430 - val_loss: 0.2799\n",
            "Epoch 92/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2426 - val_loss: 0.2797\n",
            "Epoch 93/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2423 - val_loss: 0.2796\n",
            "Epoch 94/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2421 - val_loss: 0.2796\n",
            "Epoch 95/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2419 - val_loss: 0.2795\n",
            "Epoch 96/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2415 - val_loss: 0.2793\n",
            "Epoch 97/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2412 - val_loss: 0.2793\n",
            "Epoch 98/200\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2409 - val_loss: 0.2791\n",
            "Epoch 99/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2407 - val_loss: 0.2791\n",
            "Epoch 100/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2403 - val_loss: 0.2790\n",
            "Epoch 101/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2401 - val_loss: 0.2789\n",
            "Epoch 102/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2397 - val_loss: 0.2788\n",
            "Epoch 103/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2394 - val_loss: 0.2787\n",
            "Epoch 104/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2391 - val_loss: 0.2786\n",
            "Epoch 105/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2387 - val_loss: 0.2785\n",
            "Epoch 106/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2385 - val_loss: 0.2784\n",
            "Epoch 107/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2384 - val_loss: 0.2783\n",
            "Epoch 108/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2380 - val_loss: 0.2782\n",
            "Epoch 109/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2376 - val_loss: 0.2781\n",
            "Epoch 110/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2372 - val_loss: 0.2780\n",
            "Epoch 111/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2370 - val_loss: 0.2779\n",
            "Epoch 112/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2368 - val_loss: 0.2778\n",
            "Epoch 113/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2365 - val_loss: 0.2777\n",
            "Epoch 114/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2362 - val_loss: 0.2776\n",
            "Epoch 115/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2359 - val_loss: 0.2775\n",
            "Epoch 116/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2355 - val_loss: 0.2775\n",
            "Epoch 117/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2353 - val_loss: 0.2774\n",
            "Epoch 118/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2350 - val_loss: 0.2773\n",
            "Epoch 119/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2346 - val_loss: 0.2772\n",
            "Epoch 120/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2343 - val_loss: 0.2771\n",
            "Epoch 121/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2341 - val_loss: 0.2770\n",
            "Epoch 122/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2337 - val_loss: 0.2769\n",
            "Epoch 123/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2334 - val_loss: 0.2768\n",
            "Epoch 124/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2330 - val_loss: 0.2767\n",
            "Epoch 125/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2327 - val_loss: 0.2767\n",
            "Epoch 126/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2325 - val_loss: 0.2766\n",
            "Epoch 127/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2323 - val_loss: 0.2765\n",
            "Epoch 128/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2320 - val_loss: 0.2764\n",
            "Epoch 129/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2315 - val_loss: 0.2763\n",
            "Epoch 130/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2313 - val_loss: 0.2762\n",
            "Epoch 131/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2310 - val_loss: 0.2762\n",
            "Epoch 132/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2307 - val_loss: 0.2761\n",
            "Epoch 133/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2305 - val_loss: 0.2760\n",
            "Epoch 134/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2300 - val_loss: 0.2759\n",
            "Epoch 135/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2298 - val_loss: 0.2758\n",
            "Epoch 136/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2295 - val_loss: 0.2758\n",
            "Epoch 137/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2292 - val_loss: 0.2757\n",
            "Epoch 138/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2288 - val_loss: 0.2756\n",
            "Epoch 139/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2285 - val_loss: 0.2755\n",
            "Epoch 140/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2284 - val_loss: 0.2755\n",
            "Epoch 141/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2280 - val_loss: 0.2754\n",
            "Epoch 142/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2277 - val_loss: 0.2753\n",
            "Epoch 143/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2274 - val_loss: 0.2752\n",
            "Epoch 144/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2272 - val_loss: 0.2752\n",
            "Epoch 145/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2268 - val_loss: 0.2751\n",
            "Epoch 146/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2265 - val_loss: 0.2750\n",
            "Epoch 147/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2262 - val_loss: 0.2749\n",
            "Epoch 148/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2259 - val_loss: 0.2749\n",
            "Epoch 149/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2256 - val_loss: 0.2748\n",
            "Epoch 150/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2253 - val_loss: 0.2747\n",
            "Epoch 151/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2253 - val_loss: 0.2747\n",
            "Epoch 152/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2247 - val_loss: 0.2746\n",
            "Epoch 153/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2246 - val_loss: 0.2745\n",
            "Epoch 154/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2243 - val_loss: 0.2745\n",
            "Epoch 155/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2239 - val_loss: 0.2744\n",
            "Epoch 156/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2237 - val_loss: 0.2743\n",
            "Epoch 157/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2235 - val_loss: 0.2742\n",
            "Epoch 158/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2231 - val_loss: 0.2742\n",
            "Epoch 159/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2229 - val_loss: 0.2741\n",
            "Epoch 160/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2225 - val_loss: 0.2741\n",
            "Epoch 161/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2224 - val_loss: 0.2740\n",
            "Epoch 162/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2221 - val_loss: 0.2740\n",
            "Epoch 163/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2217 - val_loss: 0.2739\n",
            "Epoch 164/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2216 - val_loss: 0.2738\n",
            "Epoch 165/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2213 - val_loss: 0.2738\n",
            "Epoch 166/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2210 - val_loss: 0.2737\n",
            "Epoch 167/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2207 - val_loss: 0.2737\n",
            "Epoch 168/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2204 - val_loss: 0.2736\n",
            "Epoch 169/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2201 - val_loss: 0.2736\n",
            "Epoch 170/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2199 - val_loss: 0.2735\n",
            "Epoch 171/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2196 - val_loss: 0.2734\n",
            "Epoch 172/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2193 - val_loss: 0.2734\n",
            "Epoch 173/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2191 - val_loss: 0.2733\n",
            "Epoch 174/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2189 - val_loss: 0.2733\n",
            "Epoch 175/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2186 - val_loss: 0.2732\n",
            "Epoch 176/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2184 - val_loss: 0.2732\n",
            "Epoch 177/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2181 - val_loss: 0.2731\n",
            "Epoch 178/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2178 - val_loss: 0.2732\n",
            "Epoch 179/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2176 - val_loss: 0.2730\n",
            "Epoch 180/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2174 - val_loss: 0.2730\n",
            "Epoch 181/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2171 - val_loss: 0.2729\n",
            "Epoch 182/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2168 - val_loss: 0.2729\n",
            "Epoch 183/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2166 - val_loss: 0.2729\n",
            "Epoch 184/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2164 - val_loss: 0.2728\n",
            "Epoch 185/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2161 - val_loss: 0.2728\n",
            "Epoch 186/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2158 - val_loss: 0.2728\n",
            "Epoch 187/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2156 - val_loss: 0.2727\n",
            "Epoch 188/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2154 - val_loss: 0.2726\n",
            "Epoch 189/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2150 - val_loss: 0.2726\n",
            "Epoch 190/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2150 - val_loss: 0.2726\n",
            "Epoch 191/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2146 - val_loss: 0.2725\n",
            "Epoch 192/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2145 - val_loss: 0.2726\n",
            "Epoch 193/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2142 - val_loss: 0.2724\n",
            "Epoch 194/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2141 - val_loss: 0.2724\n",
            "Epoch 195/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2138 - val_loss: 0.2724\n",
            "Epoch 196/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2134 - val_loss: 0.2724\n",
            "Epoch 197/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2133 - val_loss: 0.2723\n",
            "Epoch 198/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2129 - val_loss: 0.2723\n",
            "Epoch 199/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2128 - val_loss: 0.2722\n",
            "Epoch 200/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2125 - val_loss: 0.2722\n",
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 512, 0.2124718278646469, 0.2722206711769104, 0.029483832, 3.826250238636939, 0.21810360216703753)\n",
            "Epoch 1/200\n",
            "67/67 [==============================] - 2s 17ms/step - loss: 0.5310 - val_loss: 0.5293\n",
            "Epoch 2/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3993 - val_loss: 0.4704\n",
            "Epoch 3/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.4267\n",
            "Epoch 4/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3500 - val_loss: 0.3950\n",
            "Epoch 5/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.3349 - val_loss: 0.3743\n",
            "Epoch 6/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.3229 - val_loss: 0.3598\n",
            "Epoch 7/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.3132 - val_loss: 0.3492\n",
            "Epoch 8/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.3054 - val_loss: 0.3406\n",
            "Epoch 9/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2987 - val_loss: 0.3337\n",
            "Epoch 10/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2933 - val_loss: 0.3279\n",
            "Epoch 11/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2884 - val_loss: 0.3228\n",
            "Epoch 12/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2843 - val_loss: 0.3183\n",
            "Epoch 13/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2806 - val_loss: 0.3144\n",
            "Epoch 14/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2773 - val_loss: 0.3109\n",
            "Epoch 15/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2743 - val_loss: 0.3078\n",
            "Epoch 16/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2719 - val_loss: 0.3050\n",
            "Epoch 17/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2695 - val_loss: 0.3026\n",
            "Epoch 18/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2675 - val_loss: 0.3004\n",
            "Epoch 19/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2658 - val_loss: 0.2986\n",
            "Epoch 20/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2642 - val_loss: 0.2969\n",
            "Epoch 21/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2629 - val_loss: 0.2954\n",
            "Epoch 22/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2616 - val_loss: 0.2941\n",
            "Epoch 23/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2606 - val_loss: 0.2929\n",
            "Epoch 24/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2599 - val_loss: 0.2919\n",
            "Epoch 25/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2589 - val_loss: 0.2910\n",
            "Epoch 26/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2582 - val_loss: 0.2903\n",
            "Epoch 27/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2577 - val_loss: 0.2896\n",
            "Epoch 28/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2570 - val_loss: 0.2890\n",
            "Epoch 29/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2566 - val_loss: 0.2885\n",
            "Epoch 30/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2561 - val_loss: 0.2880\n",
            "Epoch 31/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2558 - val_loss: 0.2876\n",
            "Epoch 32/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2555 - val_loss: 0.2872\n",
            "Epoch 33/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2551 - val_loss: 0.2869\n",
            "Epoch 34/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2548 - val_loss: 0.2866\n",
            "Epoch 35/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2546 - val_loss: 0.2863\n",
            "Epoch 36/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2543 - val_loss: 0.2861\n",
            "Epoch 37/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2541 - val_loss: 0.2858\n",
            "Epoch 38/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2539 - val_loss: 0.2856\n",
            "Epoch 39/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2536 - val_loss: 0.2854\n",
            "Epoch 40/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2535 - val_loss: 0.2852\n",
            "Epoch 41/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2533 - val_loss: 0.2851\n",
            "Epoch 42/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2531 - val_loss: 0.2849\n",
            "Epoch 43/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2529 - val_loss: 0.2848\n",
            "Epoch 44/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2527 - val_loss: 0.2846\n",
            "Epoch 45/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2526 - val_loss: 0.2845\n",
            "Epoch 46/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2526 - val_loss: 0.2843\n",
            "Epoch 47/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2522 - val_loss: 0.2842\n",
            "Epoch 48/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2522 - val_loss: 0.2841\n",
            "Epoch 49/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2519 - val_loss: 0.2839\n",
            "Epoch 50/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2518 - val_loss: 0.2838\n",
            "Epoch 51/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2516 - val_loss: 0.2837\n",
            "Epoch 52/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2515 - val_loss: 0.2836\n",
            "Epoch 53/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2513 - val_loss: 0.2835\n",
            "Epoch 54/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2511 - val_loss: 0.2834\n",
            "Epoch 55/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2509 - val_loss: 0.2833\n",
            "Epoch 56/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2508 - val_loss: 0.2832\n",
            "Epoch 57/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2506 - val_loss: 0.2831\n",
            "Epoch 58/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2504 - val_loss: 0.2830\n",
            "Epoch 59/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2503 - val_loss: 0.2829\n",
            "Epoch 60/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2501 - val_loss: 0.2828\n",
            "Epoch 61/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2498 - val_loss: 0.2827\n",
            "Epoch 62/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2497 - val_loss: 0.2826\n",
            "Epoch 63/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2494 - val_loss: 0.2825\n",
            "Epoch 64/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2493 - val_loss: 0.2824\n",
            "Epoch 65/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2492 - val_loss: 0.2823\n",
            "Epoch 66/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2491 - val_loss: 0.2822\n",
            "Epoch 67/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2488 - val_loss: 0.2821\n",
            "Epoch 68/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2485 - val_loss: 0.2820\n",
            "Epoch 69/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2482 - val_loss: 0.2819\n",
            "Epoch 70/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2481 - val_loss: 0.2818\n",
            "Epoch 71/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2480 - val_loss: 0.2817\n",
            "Epoch 72/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2478 - val_loss: 0.2816\n",
            "Epoch 73/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2474 - val_loss: 0.2816\n",
            "Epoch 74/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2473 - val_loss: 0.2815\n",
            "Epoch 75/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2470 - val_loss: 0.2814\n",
            "Epoch 76/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2467 - val_loss: 0.2812\n",
            "Epoch 77/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2465 - val_loss: 0.2812\n",
            "Epoch 78/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2464 - val_loss: 0.2811\n",
            "Epoch 79/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2460 - val_loss: 0.2810\n",
            "Epoch 80/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2458 - val_loss: 0.2809\n",
            "Epoch 81/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2454 - val_loss: 0.2808\n",
            "Epoch 82/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2453 - val_loss: 0.2807\n",
            "Epoch 83/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2449 - val_loss: 0.2806\n",
            "Epoch 84/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2448 - val_loss: 0.2805\n",
            "Epoch 85/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2444 - val_loss: 0.2803\n",
            "Epoch 86/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2443 - val_loss: 0.2802\n",
            "Epoch 87/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2439 - val_loss: 0.2802\n",
            "Epoch 88/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2437 - val_loss: 0.2801\n",
            "Epoch 89/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2434 - val_loss: 0.2799\n",
            "Epoch 90/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2431 - val_loss: 0.2798\n",
            "Epoch 91/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2428 - val_loss: 0.2798\n",
            "Epoch 92/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2426 - val_loss: 0.2796\n",
            "Epoch 93/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2422 - val_loss: 0.2795\n",
            "Epoch 94/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2420 - val_loss: 0.2795\n",
            "Epoch 95/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2417 - val_loss: 0.2794\n",
            "Epoch 96/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2414 - val_loss: 0.2793\n",
            "Epoch 97/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2410 - val_loss: 0.2792\n",
            "Epoch 98/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2408 - val_loss: 0.2791\n",
            "Epoch 99/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2405 - val_loss: 0.2790\n",
            "Epoch 100/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2402 - val_loss: 0.2789\n",
            "Epoch 101/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2398 - val_loss: 0.2788\n",
            "Epoch 102/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2396 - val_loss: 0.2787\n",
            "Epoch 103/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2393 - val_loss: 0.2785\n",
            "Epoch 104/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2391 - val_loss: 0.2784\n",
            "Epoch 105/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2386 - val_loss: 0.2784\n",
            "Epoch 106/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2385 - val_loss: 0.2783\n",
            "Epoch 107/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2381 - val_loss: 0.2782\n",
            "Epoch 108/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2378 - val_loss: 0.2780\n",
            "Epoch 109/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2376 - val_loss: 0.2779\n",
            "Epoch 110/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2371 - val_loss: 0.2778\n",
            "Epoch 111/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2369 - val_loss: 0.2777\n",
            "Epoch 112/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2365 - val_loss: 0.2777\n",
            "Epoch 113/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2362 - val_loss: 0.2775\n",
            "Epoch 114/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2359 - val_loss: 0.2775\n",
            "Epoch 115/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2356 - val_loss: 0.2774\n",
            "Epoch 116/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2354 - val_loss: 0.2773\n",
            "Epoch 117/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2351 - val_loss: 0.2772\n",
            "Epoch 118/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2347 - val_loss: 0.2771\n",
            "Epoch 119/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2344 - val_loss: 0.2770\n",
            "Epoch 120/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2340 - val_loss: 0.2769\n",
            "Epoch 121/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2336 - val_loss: 0.2768\n",
            "Epoch 122/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2335 - val_loss: 0.2767\n",
            "Epoch 123/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2332 - val_loss: 0.2766\n",
            "Epoch 124/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2329 - val_loss: 0.2765\n",
            "Epoch 125/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2325 - val_loss: 0.2764\n",
            "Epoch 126/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2323 - val_loss: 0.2763\n",
            "Epoch 127/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2319 - val_loss: 0.2762\n",
            "Epoch 128/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2316 - val_loss: 0.2762\n",
            "Epoch 129/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2313 - val_loss: 0.2760\n",
            "Epoch 130/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2311 - val_loss: 0.2760\n",
            "Epoch 131/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2307 - val_loss: 0.2759\n",
            "Epoch 132/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2305 - val_loss: 0.2758\n",
            "Epoch 133/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2302 - val_loss: 0.2757\n",
            "Epoch 134/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2298 - val_loss: 0.2756\n",
            "Epoch 135/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2295 - val_loss: 0.2755\n",
            "Epoch 136/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2292 - val_loss: 0.2754\n",
            "Epoch 137/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2290 - val_loss: 0.2753\n",
            "Epoch 138/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2287 - val_loss: 0.2752\n",
            "Epoch 139/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2283 - val_loss: 0.2752\n",
            "Epoch 140/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2280 - val_loss: 0.2751\n",
            "Epoch 141/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2277 - val_loss: 0.2750\n",
            "Epoch 142/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2275 - val_loss: 0.2749\n",
            "Epoch 143/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2271 - val_loss: 0.2748\n",
            "Epoch 144/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2268 - val_loss: 0.2748\n",
            "Epoch 145/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2265 - val_loss: 0.2747\n",
            "Epoch 146/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2262 - val_loss: 0.2746\n",
            "Epoch 147/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2261 - val_loss: 0.2745\n",
            "Epoch 148/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2258 - val_loss: 0.2745\n",
            "Epoch 149/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2254 - val_loss: 0.2744\n",
            "Epoch 150/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2251 - val_loss: 0.2743\n",
            "Epoch 151/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2248 - val_loss: 0.2742\n",
            "Epoch 152/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2247 - val_loss: 0.2742\n",
            "Epoch 153/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2243 - val_loss: 0.2741\n",
            "Epoch 154/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2240 - val_loss: 0.2741\n",
            "Epoch 155/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2237 - val_loss: 0.2740\n",
            "Epoch 156/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2234 - val_loss: 0.2739\n",
            "Epoch 157/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2231 - val_loss: 0.2738\n",
            "Epoch 158/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2229 - val_loss: 0.2738\n",
            "Epoch 159/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2226 - val_loss: 0.2738\n",
            "Epoch 160/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2224 - val_loss: 0.2736\n",
            "Epoch 161/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2221 - val_loss: 0.2736\n",
            "Epoch 162/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2219 - val_loss: 0.2735\n",
            "Epoch 163/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2216 - val_loss: 0.2734\n",
            "Epoch 164/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2212 - val_loss: 0.2734\n",
            "Epoch 165/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2210 - val_loss: 0.2733\n",
            "Epoch 166/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2208 - val_loss: 0.2733\n",
            "Epoch 167/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2204 - val_loss: 0.2732\n",
            "Epoch 168/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2202 - val_loss: 0.2731\n",
            "Epoch 169/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2200 - val_loss: 0.2731\n",
            "Epoch 170/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2197 - val_loss: 0.2730\n",
            "Epoch 171/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2194 - val_loss: 0.2730\n",
            "Epoch 172/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2192 - val_loss: 0.2729\n",
            "Epoch 173/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2189 - val_loss: 0.2729\n",
            "Epoch 174/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2187 - val_loss: 0.2728\n",
            "Epoch 175/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2184 - val_loss: 0.2728\n",
            "Epoch 176/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2182 - val_loss: 0.2728\n",
            "Epoch 177/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2180 - val_loss: 0.2727\n",
            "Epoch 178/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2177 - val_loss: 0.2726\n",
            "Epoch 179/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2173 - val_loss: 0.2725\n",
            "Epoch 180/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2172 - val_loss: 0.2725\n",
            "Epoch 181/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2169 - val_loss: 0.2725\n",
            "Epoch 182/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2168 - val_loss: 0.2724\n",
            "Epoch 183/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2165 - val_loss: 0.2725\n",
            "Epoch 184/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2161 - val_loss: 0.2723\n",
            "Epoch 185/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2159 - val_loss: 0.2725\n",
            "Epoch 186/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2158 - val_loss: 0.2722\n",
            "Epoch 187/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2155 - val_loss: 0.2721\n",
            "Epoch 188/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2153 - val_loss: 0.2721\n",
            "Epoch 189/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2150 - val_loss: 0.2721\n",
            "Epoch 190/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2148 - val_loss: 0.2720\n",
            "Epoch 191/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2146 - val_loss: 0.2721\n",
            "Epoch 192/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2143 - val_loss: 0.2719\n",
            "Epoch 193/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2140 - val_loss: 0.2719\n",
            "Epoch 194/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2139 - val_loss: 0.2719\n",
            "Epoch 195/200\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2136 - val_loss: 0.2718\n",
            "Epoch 196/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2133 - val_loss: 0.2718\n",
            "Epoch 197/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2130 - val_loss: 0.2717\n",
            "Epoch 198/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2130 - val_loss: 0.2718\n",
            "Epoch 199/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2126 - val_loss: 0.2716\n",
            "Epoch 200/200\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.2125 - val_loss: 0.2716\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 512, 0.21247123181819916, 0.2716176211833954, 0.031578492, 3.746022847248271, 0.20727763633338614)\n",
            "Epoch 1/200\n",
            "33/33 [==============================] - 2s 34ms/step - loss: 0.6218 - val_loss: 0.5672\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.4552 - val_loss: 0.5355\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.4127 - val_loss: 0.5004\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.3905 - val_loss: 0.4719\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.3758 - val_loss: 0.4482\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 1s 26ms/step - loss: 0.3642 - val_loss: 0.4280\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.3547 - val_loss: 0.4108\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.3466 - val_loss: 0.3974\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.3394 - val_loss: 0.3861\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.3327 - val_loss: 0.3767\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.3268 - val_loss: 0.3688\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.3213 - val_loss: 0.3620\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.3164 - val_loss: 0.3560\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.3120 - val_loss: 0.3509\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 1s 27ms/step - loss: 0.3079 - val_loss: 0.3463\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 1s 27ms/step - loss: 0.3045 - val_loss: 0.3423\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 1s 27ms/step - loss: 0.3012 - val_loss: 0.3387\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 1s 26ms/step - loss: 0.2982 - val_loss: 0.3354\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 1s 26ms/step - loss: 0.2954 - val_loss: 0.3322\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 1s 26ms/step - loss: 0.2929 - val_loss: 0.3294\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 1s 26ms/step - loss: 0.2905 - val_loss: 0.3268\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2882 - val_loss: 0.3243\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2861 - val_loss: 0.3220\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2842 - val_loss: 0.3198\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2823 - val_loss: 0.3177\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2807 - val_loss: 0.3158\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2790 - val_loss: 0.3140\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2776 - val_loss: 0.3123\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2761 - val_loss: 0.3107\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2746 - val_loss: 0.3092\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2734 - val_loss: 0.3078\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2723 - val_loss: 0.3065\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2712 - val_loss: 0.3052\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2701 - val_loss: 0.3040\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2690 - val_loss: 0.3029\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2681 - val_loss: 0.3018\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2673 - val_loss: 0.3008\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2664 - val_loss: 0.2999\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2655 - val_loss: 0.2990\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2649 - val_loss: 0.2982\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2641 - val_loss: 0.2974\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2634 - val_loss: 0.2967\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2628 - val_loss: 0.2960\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2623 - val_loss: 0.2954\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2618 - val_loss: 0.2948\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2613 - val_loss: 0.2942\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2609 - val_loss: 0.2937\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2603 - val_loss: 0.2932\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2599 - val_loss: 0.2928\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2595 - val_loss: 0.2924\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2592 - val_loss: 0.2920\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2589 - val_loss: 0.2916\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2586 - val_loss: 0.2913\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2582 - val_loss: 0.2909\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2580 - val_loss: 0.2906\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2577 - val_loss: 0.2904\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2574 - val_loss: 0.2901\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2572 - val_loss: 0.2899\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2571 - val_loss: 0.2896\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2568 - val_loss: 0.2894\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2567 - val_loss: 0.2892\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2565 - val_loss: 0.2890\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2563 - val_loss: 0.2888\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2561 - val_loss: 0.2887\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2559 - val_loss: 0.2886\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2558 - val_loss: 0.2883\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2557 - val_loss: 0.2882\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2556 - val_loss: 0.2881\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2554 - val_loss: 0.2879\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2553 - val_loss: 0.2878\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 1s 26ms/step - loss: 0.2552 - val_loss: 0.2877\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2550 - val_loss: 0.2876\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2549 - val_loss: 0.2875\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2548 - val_loss: 0.2874\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2546 - val_loss: 0.2873\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2546 - val_loss: 0.2872\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2545 - val_loss: 0.2871\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2544 - val_loss: 0.2870\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2543 - val_loss: 0.2869\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2542 - val_loss: 0.2868\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2542 - val_loss: 0.2867\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2541 - val_loss: 0.2866\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2539 - val_loss: 0.2865\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2538 - val_loss: 0.2865\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2538 - val_loss: 0.2864\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2537 - val_loss: 0.2863\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2536 - val_loss: 0.2862\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2536 - val_loss: 0.2862\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2535 - val_loss: 0.2861\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2860\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2533 - val_loss: 0.2860\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2859\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2859\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2531 - val_loss: 0.2858\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2857\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2857\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2528 - val_loss: 0.2856\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2527 - val_loss: 0.2855\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2527 - val_loss: 0.2855\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2526 - val_loss: 0.2854\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2854\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2525 - val_loss: 0.2853\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2523 - val_loss: 0.2853\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2523 - val_loss: 0.2852\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2522 - val_loss: 0.2852\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2521 - val_loss: 0.2851\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2521 - val_loss: 0.2851\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2520 - val_loss: 0.2850\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2519 - val_loss: 0.2850\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2849\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2517 - val_loss: 0.2849\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2517 - val_loss: 0.2848\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2516 - val_loss: 0.2848\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2847\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2847\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2513 - val_loss: 0.2846\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2513 - val_loss: 0.2846\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2512 - val_loss: 0.2845\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2511 - val_loss: 0.2845\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2510 - val_loss: 0.2845\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2509 - val_loss: 0.2844\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2509 - val_loss: 0.2844\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2508 - val_loss: 0.2843\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2843\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2505 - val_loss: 0.2842\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2505 - val_loss: 0.2842\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2504 - val_loss: 0.2841\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2503 - val_loss: 0.2841\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2503 - val_loss: 0.2840\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2501 - val_loss: 0.2840\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2501 - val_loss: 0.2840\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2499 - val_loss: 0.2839\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2498 - val_loss: 0.2839\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2499 - val_loss: 0.2838\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2497 - val_loss: 0.2838\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2495 - val_loss: 0.2837\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2494 - val_loss: 0.2837\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2494 - val_loss: 0.2836\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2493 - val_loss: 0.2836\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2492 - val_loss: 0.2836\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2490 - val_loss: 0.2835\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2490 - val_loss: 0.2835\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2489 - val_loss: 0.2834\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2834\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2486 - val_loss: 0.2833\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2485 - val_loss: 0.2833\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2832\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2484 - val_loss: 0.2832\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2482 - val_loss: 0.2831\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2481 - val_loss: 0.2831\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2479 - val_loss: 0.2830\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2479 - val_loss: 0.2830\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2479 - val_loss: 0.2829\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2477 - val_loss: 0.2829\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2475 - val_loss: 0.2829\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2474 - val_loss: 0.2828\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2473 - val_loss: 0.2828\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2472 - val_loss: 0.2827\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2470 - val_loss: 0.2827\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2470 - val_loss: 0.2826\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2468 - val_loss: 0.2826\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2468 - val_loss: 0.2825\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2825\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2464 - val_loss: 0.2824\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2463 - val_loss: 0.2824\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2462 - val_loss: 0.2823\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2461 - val_loss: 0.2823\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2459 - val_loss: 0.2822\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2458 - val_loss: 0.2822\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2457 - val_loss: 0.2821\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2455 - val_loss: 0.2821\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2455 - val_loss: 0.2820\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2453 - val_loss: 0.2820\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2819\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2450 - val_loss: 0.2819\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2449 - val_loss: 0.2819\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2447 - val_loss: 0.2818\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2446 - val_loss: 0.2818\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2444 - val_loss: 0.2817\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2444 - val_loss: 0.2816\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2441 - val_loss: 0.2816\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2440 - val_loss: 0.2815\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2439 - val_loss: 0.2815\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2438 - val_loss: 0.2815\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2435 - val_loss: 0.2814\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2436 - val_loss: 0.2814\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2434 - val_loss: 0.2813\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2431 - val_loss: 0.2813\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2431 - val_loss: 0.2812\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2429 - val_loss: 0.2811\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2427 - val_loss: 0.2811\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2427 - val_loss: 0.2810\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2424 - val_loss: 0.2810\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2424 - val_loss: 0.2809\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2422 - val_loss: 0.2809\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2421 - val_loss: 0.2809\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2420 - val_loss: 0.2808\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2418 - val_loss: 0.2807\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2416 - val_loss: 0.2807\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2415 - val_loss: 0.2806\n",
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 1024, 0.2414788454771042, 0.28063276410102844, 0.031409822, 3.9219245780535394, 0.16068319461280062)\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 2s 31ms/step - loss: 0.6165 - val_loss: 0.5552\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4487 - val_loss: 0.5257\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4101 - val_loss: 0.4914\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3893 - val_loss: 0.4644\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3748 - val_loss: 0.4408\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3630 - val_loss: 0.4202\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3531 - val_loss: 0.4032\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3445 - val_loss: 0.3900\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3370 - val_loss: 0.3786\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3301 - val_loss: 0.3695\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3241 - val_loss: 0.3617\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3186 - val_loss: 0.3551\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3137 - val_loss: 0.3497\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3095 - val_loss: 0.3449\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3056 - val_loss: 0.3406\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3022 - val_loss: 0.3368\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2991 - val_loss: 0.3333\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2962 - val_loss: 0.3302\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2936 - val_loss: 0.3273\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2910 - val_loss: 0.3247\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2888 - val_loss: 0.3221\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2866 - val_loss: 0.3197\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2845 - val_loss: 0.3175\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2825 - val_loss: 0.3154\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2808 - val_loss: 0.3134\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2791 - val_loss: 0.3115\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2773 - val_loss: 0.3097\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2759 - val_loss: 0.3080\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2744 - val_loss: 0.3064\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2729 - val_loss: 0.3049\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2716 - val_loss: 0.3034\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2704 - val_loss: 0.3021\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2692 - val_loss: 0.3008\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2682 - val_loss: 0.2997\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2672 - val_loss: 0.2986\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2663 - val_loss: 0.2975\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2653 - val_loss: 0.2966\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2646 - val_loss: 0.2957\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2637 - val_loss: 0.2948\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2631 - val_loss: 0.2941\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2623 - val_loss: 0.2933\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2617 - val_loss: 0.2927\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2612 - val_loss: 0.2921\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2606 - val_loss: 0.2914\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2602 - val_loss: 0.2909\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2597 - val_loss: 0.2904\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2593 - val_loss: 0.2899\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2588 - val_loss: 0.2895\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2585 - val_loss: 0.2891\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2581 - val_loss: 0.2887\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2578 - val_loss: 0.2883\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2575 - val_loss: 0.2880\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2572 - val_loss: 0.2877\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2569 - val_loss: 0.2874\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2567 - val_loss: 0.2871\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2564 - val_loss: 0.2869\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2562 - val_loss: 0.2866\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2561 - val_loss: 0.2864\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2558 - val_loss: 0.2862\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2556 - val_loss: 0.2860\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2554 - val_loss: 0.2858\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2552 - val_loss: 0.2856\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2551 - val_loss: 0.2854\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2549 - val_loss: 0.2853\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2547 - val_loss: 0.2851\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2546 - val_loss: 0.2850\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2545 - val_loss: 0.2848\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2847\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2542 - val_loss: 0.2846\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2541 - val_loss: 0.2845\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2540 - val_loss: 0.2843\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2539 - val_loss: 0.2842\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2537 - val_loss: 0.2841\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2536 - val_loss: 0.2840\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2535 - val_loss: 0.2839\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2838\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2534 - val_loss: 0.2837\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2837\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2531 - val_loss: 0.2836\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2530 - val_loss: 0.2835\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2529 - val_loss: 0.2834\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2528 - val_loss: 0.2833\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2527 - val_loss: 0.2832\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2526 - val_loss: 0.2831\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2831\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2830\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2524 - val_loss: 0.2829\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2523 - val_loss: 0.2829\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2522 - val_loss: 0.2828\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2522 - val_loss: 0.2827\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2521 - val_loss: 0.2827\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2520 - val_loss: 0.2826\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2825\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2825\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2824\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2517 - val_loss: 0.2824\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2515 - val_loss: 0.2823\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2514 - val_loss: 0.2823\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2514 - val_loss: 0.2822\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2514 - val_loss: 0.2821\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2512 - val_loss: 0.2821\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2511 - val_loss: 0.2821\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2511 - val_loss: 0.2820\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2510 - val_loss: 0.2819\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2509 - val_loss: 0.2819\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2508 - val_loss: 0.2818\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2507 - val_loss: 0.2818\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2818\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2505 - val_loss: 0.2817\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2817\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2504 - val_loss: 0.2816\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2503 - val_loss: 0.2815\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2503 - val_loss: 0.2815\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2500 - val_loss: 0.2815\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2501 - val_loss: 0.2814\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2814\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2498 - val_loss: 0.2814\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2496 - val_loss: 0.2813\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2497 - val_loss: 0.2812\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2495 - val_loss: 0.2812\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2495 - val_loss: 0.2811\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2494 - val_loss: 0.2811\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2493 - val_loss: 0.2810\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2492 - val_loss: 0.2810\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2491 - val_loss: 0.2810\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2490 - val_loss: 0.2809\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2809\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2808\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2486 - val_loss: 0.2807\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2486 - val_loss: 0.2807\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2485 - val_loss: 0.2806\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2485 - val_loss: 0.2806\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2482 - val_loss: 0.2806\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2482 - val_loss: 0.2806\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2481 - val_loss: 0.2805\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2480 - val_loss: 0.2804\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2478 - val_loss: 0.2804\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2477 - val_loss: 0.2804\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2476 - val_loss: 0.2803\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2474 - val_loss: 0.2802\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2474 - val_loss: 0.2802\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2473 - val_loss: 0.2801\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2472 - val_loss: 0.2801\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2470 - val_loss: 0.2801\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2469 - val_loss: 0.2800\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2468 - val_loss: 0.2800\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2466 - val_loss: 0.2800\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2466 - val_loss: 0.2799\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2799\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2463 - val_loss: 0.2799\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2462 - val_loss: 0.2798\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2461 - val_loss: 0.2797\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2460 - val_loss: 0.2796\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2457 - val_loss: 0.2796\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2457 - val_loss: 0.2796\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2456 - val_loss: 0.2795\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2455 - val_loss: 0.2795\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2453 - val_loss: 0.2794\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2794\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2450 - val_loss: 0.2793\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2449 - val_loss: 0.2793\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2448 - val_loss: 0.2792\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2447 - val_loss: 0.2792\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2445 - val_loss: 0.2791\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2444 - val_loss: 0.2791\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2442 - val_loss: 0.2790\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2441 - val_loss: 0.2790\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2439 - val_loss: 0.2789\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2438 - val_loss: 0.2789\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2436 - val_loss: 0.2788\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2435 - val_loss: 0.2788\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2434 - val_loss: 0.2788\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2433 - val_loss: 0.2787\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2431 - val_loss: 0.2787\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2430 - val_loss: 0.2786\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2428 - val_loss: 0.2785\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2427 - val_loss: 0.2785\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2426 - val_loss: 0.2784\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2424 - val_loss: 0.2784\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2422 - val_loss: 0.2784\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2421 - val_loss: 0.2783\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2420 - val_loss: 0.2783\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2418 - val_loss: 0.2782\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2417 - val_loss: 0.2782\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2415 - val_loss: 0.2781\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2414 - val_loss: 0.2781\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2413 - val_loss: 0.2780\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2410 - val_loss: 0.2780\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2409 - val_loss: 0.2779\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2407 - val_loss: 0.2779\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2406 - val_loss: 0.2778\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2404 - val_loss: 0.2778\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2402 - val_loss: 0.2777\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2401 - val_loss: 0.2777\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2401 - val_loss: 0.2776\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2398 - val_loss: 0.2776\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2398 - val_loss: 0.2776\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2396 - val_loss: 0.2775\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2394 - val_loss: 0.2774\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2392 - val_loss: 0.2774\n",
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 1024, 0.23924843966960907, 0.27740660309791565, 0.03200194, 3.9122170473185514, 0.20169377501974708)\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 2s 32ms/step - loss: 0.6217 - val_loss: 0.5567\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4549 - val_loss: 0.5252\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4123 - val_loss: 0.4897\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3902 - val_loss: 0.4621\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3757 - val_loss: 0.4386\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3639 - val_loss: 0.4181\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3542 - val_loss: 0.4013\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3458 - val_loss: 0.3882\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3386 - val_loss: 0.3773\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3322 - val_loss: 0.3689\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3264 - val_loss: 0.3615\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3208 - val_loss: 0.3549\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3160 - val_loss: 0.3493\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3115 - val_loss: 0.3444\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3075 - val_loss: 0.3399\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3038 - val_loss: 0.3358\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3004 - val_loss: 0.3321\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2973 - val_loss: 0.3288\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2943 - val_loss: 0.3256\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2917 - val_loss: 0.3229\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2893 - val_loss: 0.3202\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2872 - val_loss: 0.3179\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2851 - val_loss: 0.3156\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2830 - val_loss: 0.3135\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2813 - val_loss: 0.3115\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2795 - val_loss: 0.3098\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2780 - val_loss: 0.3079\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2765 - val_loss: 0.3064\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2751 - val_loss: 0.3049\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2737 - val_loss: 0.3034\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2725 - val_loss: 0.3020\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2713 - val_loss: 0.3008\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2703 - val_loss: 0.2997\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2692 - val_loss: 0.2985\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2682 - val_loss: 0.2975\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2673 - val_loss: 0.2965\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2664 - val_loss: 0.2956\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2656 - val_loss: 0.2948\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2648 - val_loss: 0.2940\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2641 - val_loss: 0.2932\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2635 - val_loss: 0.2925\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2629 - val_loss: 0.2918\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2623 - val_loss: 0.2912\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2619 - val_loss: 0.2906\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2614 - val_loss: 0.2901\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2608 - val_loss: 0.2896\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2605 - val_loss: 0.2891\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2600 - val_loss: 0.2887\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2597 - val_loss: 0.2884\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2593 - val_loss: 0.2879\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2589 - val_loss: 0.2876\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2586 - val_loss: 0.2872\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2583 - val_loss: 0.2869\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2582 - val_loss: 0.2867\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2578 - val_loss: 0.2864\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2576 - val_loss: 0.2861\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2574 - val_loss: 0.2859\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2572 - val_loss: 0.2856\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2569 - val_loss: 0.2854\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2567 - val_loss: 0.2852\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2565 - val_loss: 0.2850\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2563 - val_loss: 0.2848\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2562 - val_loss: 0.2847\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2560 - val_loss: 0.2845\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2559 - val_loss: 0.2843\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2557 - val_loss: 0.2842\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2557 - val_loss: 0.2841\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2555 - val_loss: 0.2839\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2554 - val_loss: 0.2838\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2552 - val_loss: 0.2837\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2552 - val_loss: 0.2836\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2550 - val_loss: 0.2834\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2549 - val_loss: 0.2833\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2548 - val_loss: 0.2832\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2547 - val_loss: 0.2831\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2546 - val_loss: 0.2830\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2546 - val_loss: 0.2829\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2544 - val_loss: 0.2829\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2827\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2542 - val_loss: 0.2827\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2541 - val_loss: 0.2826\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2540 - val_loss: 0.2825\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2539 - val_loss: 0.2824\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2538 - val_loss: 0.2824\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2537 - val_loss: 0.2823\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2536 - val_loss: 0.2822\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2535 - val_loss: 0.2822\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2535 - val_loss: 0.2821\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2820\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2533 - val_loss: 0.2820\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2819\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2819\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2819\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2818\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2529 - val_loss: 0.2817\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2528 - val_loss: 0.2817\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2527 - val_loss: 0.2816\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2526 - val_loss: 0.2816\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2526 - val_loss: 0.2815\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2525 - val_loss: 0.2814\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2524 - val_loss: 0.2813\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2523 - val_loss: 0.2813\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2523 - val_loss: 0.2813\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2521 - val_loss: 0.2812\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2521 - val_loss: 0.2811\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2520 - val_loss: 0.2811\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2518 - val_loss: 0.2810\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2810\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2517 - val_loss: 0.2809\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2516 - val_loss: 0.2809\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2809\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2514 - val_loss: 0.2808\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2514 - val_loss: 0.2808\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2513 - val_loss: 0.2807\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2512 - val_loss: 0.2807\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2511 - val_loss: 0.2807\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2509 - val_loss: 0.2806\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2509 - val_loss: 0.2805\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2508 - val_loss: 0.2805\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2507 - val_loss: 0.2805\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2506 - val_loss: 0.2804\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2803\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2803\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2503 - val_loss: 0.2803\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2503 - val_loss: 0.2803\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2501 - val_loss: 0.2802\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2500 - val_loss: 0.2801\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2801\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2800\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2497 - val_loss: 0.2800\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2496 - val_loss: 0.2800\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2495 - val_loss: 0.2799\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2494 - val_loss: 0.2799\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2493 - val_loss: 0.2798\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2492 - val_loss: 0.2798\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2491 - val_loss: 0.2798\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2489 - val_loss: 0.2797\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2797\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2487 - val_loss: 0.2796\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2487 - val_loss: 0.2796\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2795\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2483 - val_loss: 0.2795\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2483 - val_loss: 0.2794\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2480 - val_loss: 0.2794\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2480 - val_loss: 0.2793\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2479 - val_loss: 0.2793\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2478 - val_loss: 0.2792\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2476 - val_loss: 0.2792\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2474 - val_loss: 0.2791\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2474 - val_loss: 0.2791\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2472 - val_loss: 0.2790\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2471 - val_loss: 0.2790\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2470 - val_loss: 0.2789\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2469 - val_loss: 0.2789\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2467 - val_loss: 0.2788\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2788\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2787\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2463 - val_loss: 0.2787\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2461 - val_loss: 0.2786\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2461 - val_loss: 0.2786\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2459 - val_loss: 0.2786\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2457 - val_loss: 0.2786\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2456 - val_loss: 0.2785\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2455 - val_loss: 0.2785\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2453 - val_loss: 0.2784\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2453 - val_loss: 0.2783\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2451 - val_loss: 0.2783\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2449 - val_loss: 0.2783\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2447 - val_loss: 0.2782\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2446 - val_loss: 0.2781\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2445 - val_loss: 0.2781\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2443 - val_loss: 0.2780\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2442 - val_loss: 0.2780\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2441 - val_loss: 0.2780\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2439 - val_loss: 0.2779\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2438 - val_loss: 0.2779\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2437 - val_loss: 0.2778\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2435 - val_loss: 0.2777\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2433 - val_loss: 0.2777\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2432 - val_loss: 0.2776\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2430 - val_loss: 0.2776\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2429 - val_loss: 0.2775\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2426 - val_loss: 0.2775\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2426 - val_loss: 0.2775\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2424 - val_loss: 0.2774\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2423 - val_loss: 0.2774\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2421 - val_loss: 0.2773\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2420 - val_loss: 0.2772\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2418 - val_loss: 0.2772\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2417 - val_loss: 0.2773\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2415 - val_loss: 0.2772\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2413 - val_loss: 0.2771\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2411 - val_loss: 0.2771\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2411 - val_loss: 0.2770\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2409 - val_loss: 0.2770\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2407 - val_loss: 0.2769\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2406 - val_loss: 0.2768\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2405 - val_loss: 0.2768\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2403 - val_loss: 0.2767\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2401 - val_loss: 0.2768\n",
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 1024, 0.24012017250061035, 0.276777446269989, 0.02871176, 3.8203647873650737, 0.17961899686283514)\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 2s 31ms/step - loss: 0.6207 - val_loss: 0.5603\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4520 - val_loss: 0.5279\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.4093 - val_loss: 0.4926\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3872 - val_loss: 0.4642\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3729 - val_loss: 0.4407\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3617 - val_loss: 0.4214\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3521 - val_loss: 0.4048\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3437 - val_loss: 0.3916\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3362 - val_loss: 0.3806\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3295 - val_loss: 0.3716\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.3235 - val_loss: 0.3638\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.3180 - val_loss: 0.3574\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.3133 - val_loss: 0.3517\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.3090 - val_loss: 0.3469\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3050 - val_loss: 0.3425\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3015 - val_loss: 0.3387\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2983 - val_loss: 0.3351\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2953 - val_loss: 0.3317\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2925 - val_loss: 0.3288\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2899 - val_loss: 0.3259\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2875 - val_loss: 0.3233\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2853 - val_loss: 0.3208\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2833 - val_loss: 0.3185\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2813 - val_loss: 0.3164\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2795 - val_loss: 0.3144\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2778 - val_loss: 0.3125\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2762 - val_loss: 0.3108\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2747 - val_loss: 0.3091\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2734 - val_loss: 0.3076\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2720 - val_loss: 0.3061\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2707 - val_loss: 0.3048\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2696 - val_loss: 0.3035\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2686 - val_loss: 0.3023\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2675 - val_loss: 0.3011\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2665 - val_loss: 0.3001\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2656 - val_loss: 0.2991\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2648 - val_loss: 0.2981\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2640 - val_loss: 0.2973\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2633 - val_loss: 0.2964\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2626 - val_loss: 0.2957\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2619 - val_loss: 0.2950\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2613 - val_loss: 0.2943\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2607 - val_loss: 0.2936\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2602 - val_loss: 0.2930\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2596 - val_loss: 0.2925\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2592 - val_loss: 0.2920\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2588 - val_loss: 0.2915\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2584 - val_loss: 0.2910\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2579 - val_loss: 0.2906\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2576 - val_loss: 0.2903\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2573 - val_loss: 0.2899\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2569 - val_loss: 0.2896\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2567 - val_loss: 0.2892\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2564 - val_loss: 0.2889\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2562 - val_loss: 0.2887\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2560 - val_loss: 0.2884\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2557 - val_loss: 0.2882\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2555 - val_loss: 0.2879\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2553 - val_loss: 0.2877\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2551 - val_loss: 0.2875\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2550 - val_loss: 0.2874\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2548 - val_loss: 0.2872\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2546 - val_loss: 0.2870\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2545 - val_loss: 0.2868\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2544 - val_loss: 0.2867\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2542 - val_loss: 0.2865\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2541 - val_loss: 0.2864\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2540 - val_loss: 0.2863\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2538 - val_loss: 0.2861\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2537 - val_loss: 0.2860\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2536 - val_loss: 0.2859\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2535 - val_loss: 0.2858\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2857\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2532 - val_loss: 0.2856\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2531 - val_loss: 0.2855\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2531 - val_loss: 0.2854\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2530 - val_loss: 0.2853\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2529 - val_loss: 0.2852\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2527 - val_loss: 0.2851\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2527 - val_loss: 0.2850\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2526 - val_loss: 0.2849\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2848\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2524 - val_loss: 0.2847\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2523 - val_loss: 0.2847\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2523 - val_loss: 0.2846\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2522 - val_loss: 0.2845\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2521 - val_loss: 0.2845\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2520 - val_loss: 0.2844\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2519 - val_loss: 0.2843\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2518 - val_loss: 0.2843\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2517 - val_loss: 0.2842\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2516 - val_loss: 0.2841\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2516 - val_loss: 0.2841\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2840\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2514 - val_loss: 0.2839\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2514 - val_loss: 0.2839\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2513 - val_loss: 0.2838\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2512 - val_loss: 0.2838\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2510 - val_loss: 0.2837\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2510 - val_loss: 0.2837\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2509 - val_loss: 0.2836\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2509 - val_loss: 0.2836\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2835\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2835\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2506 - val_loss: 0.2834\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2505 - val_loss: 0.2834\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2504 - val_loss: 0.2833\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2504 - val_loss: 0.2833\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2503 - val_loss: 0.2832\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2502 - val_loss: 0.2832\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2501 - val_loss: 0.2831\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2500 - val_loss: 0.2831\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2500 - val_loss: 0.2830\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2498 - val_loss: 0.2830\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2497 - val_loss: 0.2829\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2497 - val_loss: 0.2829\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2495 - val_loss: 0.2828\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2495 - val_loss: 0.2828\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2494 - val_loss: 0.2827\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2493 - val_loss: 0.2827\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2493 - val_loss: 0.2826\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2490 - val_loss: 0.2826\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2490 - val_loss: 0.2825\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2489 - val_loss: 0.2825\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2489 - val_loss: 0.2824\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2488 - val_loss: 0.2824\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2486 - val_loss: 0.2823\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2485 - val_loss: 0.2823\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2485 - val_loss: 0.2822\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2822\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2481 - val_loss: 0.2822\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2481 - val_loss: 0.2821\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2479 - val_loss: 0.2820\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2479 - val_loss: 0.2820\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2479 - val_loss: 0.2820\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2477 - val_loss: 0.2819\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2475 - val_loss: 0.2818\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2474 - val_loss: 0.2818\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2474 - val_loss: 0.2818\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2472 - val_loss: 0.2817\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2471 - val_loss: 0.2816\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2470 - val_loss: 0.2816\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2469 - val_loss: 0.2816\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2468 - val_loss: 0.2815\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2466 - val_loss: 0.2814\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2465 - val_loss: 0.2815\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2464 - val_loss: 0.2813\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2463 - val_loss: 0.2813\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2462 - val_loss: 0.2813\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2460 - val_loss: 0.2812\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2459 - val_loss: 0.2812\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2459 - val_loss: 0.2811\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2456 - val_loss: 0.2811\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2455 - val_loss: 0.2810\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2454 - val_loss: 0.2810\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2453 - val_loss: 0.2809\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2810\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2451 - val_loss: 0.2808\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2450 - val_loss: 0.2808\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2449 - val_loss: 0.2807\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2446 - val_loss: 0.2807\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2446 - val_loss: 0.2806\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2445 - val_loss: 0.2806\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2443 - val_loss: 0.2805\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2441 - val_loss: 0.2805\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2440 - val_loss: 0.2804\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2439 - val_loss: 0.2804\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2437 - val_loss: 0.2803\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2436 - val_loss: 0.2803\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2435 - val_loss: 0.2802\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2432 - val_loss: 0.2802\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2431 - val_loss: 0.2801\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2430 - val_loss: 0.2801\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2429 - val_loss: 0.2800\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2428 - val_loss: 0.2800\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2426 - val_loss: 0.2799\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2424 - val_loss: 0.2799\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2423 - val_loss: 0.2798\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2422 - val_loss: 0.2797\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2421 - val_loss: 0.2797\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2419 - val_loss: 0.2797\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2418 - val_loss: 0.2796\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2416 - val_loss: 0.2795\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2414 - val_loss: 0.2795\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2413 - val_loss: 0.2795\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2412 - val_loss: 0.2795\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2410 - val_loss: 0.2794\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2409 - val_loss: 0.2794\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2407 - val_loss: 0.2793\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2406 - val_loss: 0.2792\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2404 - val_loss: 0.2792\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2403 - val_loss: 0.2791\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2401 - val_loss: 0.2791\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2399 - val_loss: 0.2790\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2398 - val_loss: 0.2790\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2397 - val_loss: 0.2789\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2396 - val_loss: 0.2789\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2393 - val_loss: 0.2789\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2392 - val_loss: 0.2788\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2391 - val_loss: 0.2787\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdf8845a670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 1024, 0.23906196653842926, 0.2787390649318695, 0.03142514, 3.869072012640152, 0.19524111022592705)\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 2s 32ms/step - loss: 0.6133 - val_loss: 0.5609\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.4453 - val_loss: 0.5284\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.4069 - val_loss: 0.4948\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3884 - val_loss: 0.4691\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3746 - val_loss: 0.4452\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3626 - val_loss: 0.4248\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3525 - val_loss: 0.4077\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3439 - val_loss: 0.3937\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3365 - val_loss: 0.3822\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3300 - val_loss: 0.3730\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3240 - val_loss: 0.3653\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3186 - val_loss: 0.3587\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3137 - val_loss: 0.3530\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3093 - val_loss: 0.3479\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3054 - val_loss: 0.3436\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3018 - val_loss: 0.3396\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2987 - val_loss: 0.3359\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2956 - val_loss: 0.3326\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2928 - val_loss: 0.3296\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2903 - val_loss: 0.3269\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2880 - val_loss: 0.3242\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2857 - val_loss: 0.3217\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2837 - val_loss: 0.3195\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2817 - val_loss: 0.3174\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2799 - val_loss: 0.3154\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2782 - val_loss: 0.3136\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2766 - val_loss: 0.3118\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2750 - val_loss: 0.3101\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2736 - val_loss: 0.3084\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2722 - val_loss: 0.3070\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2710 - val_loss: 0.3056\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2698 - val_loss: 0.3043\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2687 - val_loss: 0.3030\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2676 - val_loss: 0.3019\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2667 - val_loss: 0.3008\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2658 - val_loss: 0.2998\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2650 - val_loss: 0.2988\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2642 - val_loss: 0.2980\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2634 - val_loss: 0.2971\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2627 - val_loss: 0.2963\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2620 - val_loss: 0.2955\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2615 - val_loss: 0.2948\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2608 - val_loss: 0.2942\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2603 - val_loss: 0.2936\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2598 - val_loss: 0.2930\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2593 - val_loss: 0.2925\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2588 - val_loss: 0.2921\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2585 - val_loss: 0.2915\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2581 - val_loss: 0.2911\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2577 - val_loss: 0.2907\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2575 - val_loss: 0.2903\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2571 - val_loss: 0.2899\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2568 - val_loss: 0.2896\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2564 - val_loss: 0.2893\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2562 - val_loss: 0.2890\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2560 - val_loss: 0.2887\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2558 - val_loss: 0.2885\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2555 - val_loss: 0.2882\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2553 - val_loss: 0.2880\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2552 - val_loss: 0.2878\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2549 - val_loss: 0.2876\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2549 - val_loss: 0.2874\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2546 - val_loss: 0.2872\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2545 - val_loss: 0.2870\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2544 - val_loss: 0.2869\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2542 - val_loss: 0.2867\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2541 - val_loss: 0.2866\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2540 - val_loss: 0.2864\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2538 - val_loss: 0.2863\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2537 - val_loss: 0.2861\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2536 - val_loss: 0.2860\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2859\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2533 - val_loss: 0.2858\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2532 - val_loss: 0.2857\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2531 - val_loss: 0.2856\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2855\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2529 - val_loss: 0.2854\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2529 - val_loss: 0.2853\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2528 - val_loss: 0.2852\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2526 - val_loss: 0.2851\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2526 - val_loss: 0.2850\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2850\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2524 - val_loss: 0.2849\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2523 - val_loss: 0.2848\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2521 - val_loss: 0.2847\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2521 - val_loss: 0.2846\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2520 - val_loss: 0.2845\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2519 - val_loss: 0.2844\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2519 - val_loss: 0.2844\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2517 - val_loss: 0.2843\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2518 - val_loss: 0.2842\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2516 - val_loss: 0.2842\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2841\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2840\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2513 - val_loss: 0.2840\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2513 - val_loss: 0.2839\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2512 - val_loss: 0.2839\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2510 - val_loss: 0.2838\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2511 - val_loss: 0.2837\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2509 - val_loss: 0.2837\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2508 - val_loss: 0.2836\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2836\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2835\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2835\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2505 - val_loss: 0.2834\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2834\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2833\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2832\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2502 - val_loss: 0.2832\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2500 - val_loss: 0.2831\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2500 - val_loss: 0.2831\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2831\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2498 - val_loss: 0.2830\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2497 - val_loss: 0.2829\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2496 - val_loss: 0.2829\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2497 - val_loss: 0.2828\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2495 - val_loss: 0.2828\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2495 - val_loss: 0.2827\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2493 - val_loss: 0.2827\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2491 - val_loss: 0.2827\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2491 - val_loss: 0.2826\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2490 - val_loss: 0.2825\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2490 - val_loss: 0.2825\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2487 - val_loss: 0.2824\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2487 - val_loss: 0.2824\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2486 - val_loss: 0.2823\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2823\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2822\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2483 - val_loss: 0.2821\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2482 - val_loss: 0.2821\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2481 - val_loss: 0.2821\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2480 - val_loss: 0.2820\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2479 - val_loss: 0.2820\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2479 - val_loss: 0.2819\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2477 - val_loss: 0.2819\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2475 - val_loss: 0.2818\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2474 - val_loss: 0.2818\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2473 - val_loss: 0.2818\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2473 - val_loss: 0.2818\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2471 - val_loss: 0.2816\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2470 - val_loss: 0.2816\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2469 - val_loss: 0.2815\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2469 - val_loss: 0.2815\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2466 - val_loss: 0.2815\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2466 - val_loss: 0.2815\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2814\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2463 - val_loss: 0.2813\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2462 - val_loss: 0.2813\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2461 - val_loss: 0.2812\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2459 - val_loss: 0.2812\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2458 - val_loss: 0.2811\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2457 - val_loss: 0.2811\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2456 - val_loss: 0.2810\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2455 - val_loss: 0.2810\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2454 - val_loss: 0.2809\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2809\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2451 - val_loss: 0.2808\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2450 - val_loss: 0.2808\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2448 - val_loss: 0.2807\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2447 - val_loss: 0.2806\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2447 - val_loss: 0.2806\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2444 - val_loss: 0.2806\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2443 - val_loss: 0.2805\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2442 - val_loss: 0.2805\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2440 - val_loss: 0.2805\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2438 - val_loss: 0.2804\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2438 - val_loss: 0.2803\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2436 - val_loss: 0.2803\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2435 - val_loss: 0.2802\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2434 - val_loss: 0.2802\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2432 - val_loss: 0.2801\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2431 - val_loss: 0.2801\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2429 - val_loss: 0.2800\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2428 - val_loss: 0.2800\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2426 - val_loss: 0.2799\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2425 - val_loss: 0.2799\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2424 - val_loss: 0.2798\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2422 - val_loss: 0.2798\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2421 - val_loss: 0.2797\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2419 - val_loss: 0.2797\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2417 - val_loss: 0.2796\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2417 - val_loss: 0.2796\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2414 - val_loss: 0.2795\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2413 - val_loss: 0.2795\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2412 - val_loss: 0.2794\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2411 - val_loss: 0.2794\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2409 - val_loss: 0.2793\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2407 - val_loss: 0.2793\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2406 - val_loss: 0.2792\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2405 - val_loss: 0.2792\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2403 - val_loss: 0.2792\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2402 - val_loss: 0.2791\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2400 - val_loss: 0.2790\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2399 - val_loss: 0.2789\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2397 - val_loss: 0.2789\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2396 - val_loss: 0.2789\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2395 - val_loss: 0.2788\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2394 - val_loss: 0.2788\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2391 - val_loss: 0.2787\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2390 - val_loss: 0.2787\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdf8845a700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 200, 1024, 0.23896922171115875, 0.27865204215049744, 0.033239804, 3.748506542218375, 0.16977392771142016)\n",
            "Epoch 1/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.3792 - val_loss: 0.3459\n",
            "Epoch 2/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2971 - val_loss: 0.3097\n",
            "Epoch 3/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2771 - val_loss: 0.2954\n",
            "Epoch 4/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2691 - val_loss: 0.2901\n",
            "Epoch 5/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2654 - val_loss: 0.2882\n",
            "Epoch 6/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2640 - val_loss: 0.2869\n",
            "Epoch 7/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2623 - val_loss: 0.2860\n",
            "Epoch 8/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2610 - val_loss: 0.2852\n",
            "Epoch 9/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2598 - val_loss: 0.2844\n",
            "Epoch 10/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2580 - val_loss: 0.2837\n",
            "Epoch 11/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2560 - val_loss: 0.2829\n",
            "Epoch 12/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2539 - val_loss: 0.2823\n",
            "Epoch 13/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2517 - val_loss: 0.2815\n",
            "Epoch 14/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2496 - val_loss: 0.2808\n",
            "Epoch 15/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2472 - val_loss: 0.2800\n",
            "Epoch 16/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2448 - val_loss: 0.2793\n",
            "Epoch 17/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2427 - val_loss: 0.2786\n",
            "Epoch 18/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2404 - val_loss: 0.2780\n",
            "Epoch 19/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2386 - val_loss: 0.2774\n",
            "Epoch 20/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2363 - val_loss: 0.2769\n",
            "Epoch 21/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2346 - val_loss: 0.2764\n",
            "Epoch 22/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2322 - val_loss: 0.2758\n",
            "Epoch 23/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2305 - val_loss: 0.2755\n",
            "Epoch 24/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2283 - val_loss: 0.2752\n",
            "Epoch 25/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2270 - val_loss: 0.2747\n",
            "Epoch 26/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2253 - val_loss: 0.2745\n",
            "Epoch 27/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2236 - val_loss: 0.2741\n",
            "Epoch 28/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2218 - val_loss: 0.2738\n",
            "Epoch 29/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2204 - val_loss: 0.2736\n",
            "Epoch 30/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2189 - val_loss: 0.2735\n",
            "Epoch 31/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2178 - val_loss: 0.2732\n",
            "Epoch 32/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2163 - val_loss: 0.2730\n",
            "Epoch 33/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2149 - val_loss: 0.2728\n",
            "Epoch 34/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2135 - val_loss: 0.2726\n",
            "Epoch 35/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2125 - val_loss: 0.2725\n",
            "Epoch 36/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2111 - val_loss: 0.2725\n",
            "Epoch 37/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2101 - val_loss: 0.2724\n",
            "Epoch 38/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2094 - val_loss: 0.2724\n",
            "Epoch 39/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2082 - val_loss: 0.2723\n",
            "Epoch 40/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2070 - val_loss: 0.2722\n",
            "Epoch 41/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2058 - val_loss: 0.2721\n",
            "Epoch 42/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2052 - val_loss: 0.2721\n",
            "Epoch 43/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2041 - val_loss: 0.2721\n",
            "Epoch 44/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2033 - val_loss: 0.2720\n",
            "Epoch 45/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2027 - val_loss: 0.2720\n",
            "Epoch 46/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2019 - val_loss: 0.2720\n",
            "Epoch 47/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2007 - val_loss: 0.2720\n",
            "Epoch 48/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.2001 - val_loss: 0.2719\n",
            "Epoch 49/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1995 - val_loss: 0.2720\n",
            "Epoch 50/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1990 - val_loss: 0.2719\n",
            "Epoch 51/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1978 - val_loss: 0.2720\n",
            "Epoch 52/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1972 - val_loss: 0.2721\n",
            "Epoch 53/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1963 - val_loss: 0.2720\n",
            "Epoch 54/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1958 - val_loss: 0.2722\n",
            "Epoch 55/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1956 - val_loss: 0.2722\n",
            "Epoch 56/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1946 - val_loss: 0.2722\n",
            "Epoch 57/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1944 - val_loss: 0.2723\n",
            "Epoch 58/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1943 - val_loss: 0.2723\n",
            "Epoch 59/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1931 - val_loss: 0.2724\n",
            "Epoch 60/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1927 - val_loss: 0.2724\n",
            "Epoch 61/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1919 - val_loss: 0.2725\n",
            "Epoch 62/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1915 - val_loss: 0.2725\n",
            "Epoch 63/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1913 - val_loss: 0.2726\n",
            "Epoch 64/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1907 - val_loss: 0.2727\n",
            "Epoch 65/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1901 - val_loss: 0.2728\n",
            "Epoch 66/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1895 - val_loss: 0.2727\n",
            "Epoch 67/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1894 - val_loss: 0.2729\n",
            "Epoch 68/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1888 - val_loss: 0.2730\n",
            "Epoch 69/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1882 - val_loss: 0.2731\n",
            "Epoch 70/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1881 - val_loss: 0.2731\n",
            "Epoch 71/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1876 - val_loss: 0.2731\n",
            "Epoch 72/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1872 - val_loss: 0.2732\n",
            "Epoch 73/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1874 - val_loss: 0.2733\n",
            "Epoch 74/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1864 - val_loss: 0.2735\n",
            "Epoch 75/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1859 - val_loss: 0.2733\n",
            "Epoch 76/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1860 - val_loss: 0.2734\n",
            "Epoch 77/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1852 - val_loss: 0.2735\n",
            "Epoch 78/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1853 - val_loss: 0.2737\n",
            "Epoch 79/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1850 - val_loss: 0.2737\n",
            "Epoch 80/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1845 - val_loss: 0.2738\n",
            "Epoch 81/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1845 - val_loss: 0.2739\n",
            "Epoch 82/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1839 - val_loss: 0.2739\n",
            "Epoch 83/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1838 - val_loss: 0.2740\n",
            "Epoch 84/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1834 - val_loss: 0.2740\n",
            "Epoch 85/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1832 - val_loss: 0.2741\n",
            "Epoch 86/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1829 - val_loss: 0.2743\n",
            "Epoch 87/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1825 - val_loss: 0.2743\n",
            "Epoch 88/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1826 - val_loss: 0.2744\n",
            "Epoch 89/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1821 - val_loss: 0.2744\n",
            "Epoch 90/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1818 - val_loss: 0.2745\n",
            "Epoch 91/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1815 - val_loss: 0.2745\n",
            "Epoch 92/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1814 - val_loss: 0.2746\n",
            "Epoch 93/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2747\n",
            "Epoch 94/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2748\n",
            "Epoch 95/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2748\n",
            "Epoch 96/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1806 - val_loss: 0.2749\n",
            "Epoch 97/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1805 - val_loss: 0.2750\n",
            "Epoch 98/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1802 - val_loss: 0.2751\n",
            "Epoch 99/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1799 - val_loss: 0.2752\n",
            "Epoch 100/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1800 - val_loss: 0.2752\n",
            "Epoch 101/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2753\n",
            "Epoch 102/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1797 - val_loss: 0.2753\n",
            "Epoch 103/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1792 - val_loss: 0.2753\n",
            "Epoch 104/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1790 - val_loss: 0.2755\n",
            "Epoch 105/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1789 - val_loss: 0.2755\n",
            "Epoch 106/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1783 - val_loss: 0.2756\n",
            "Epoch 107/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1788 - val_loss: 0.2756\n",
            "Epoch 108/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1785 - val_loss: 0.2757\n",
            "Epoch 109/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1780 - val_loss: 0.2758\n",
            "Epoch 110/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1779 - val_loss: 0.2759\n",
            "Epoch 111/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.2759\n",
            "Epoch 112/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1776 - val_loss: 0.2760\n",
            "Epoch 113/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2762\n",
            "Epoch 114/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1774 - val_loss: 0.2760\n",
            "Epoch 115/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2762\n",
            "Epoch 116/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1771 - val_loss: 0.2763\n",
            "Epoch 117/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2764\n",
            "Epoch 118/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1766 - val_loss: 0.2764\n",
            "Epoch 119/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1763 - val_loss: 0.2764\n",
            "Epoch 120/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1768 - val_loss: 0.2765\n",
            "Epoch 121/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2765\n",
            "Epoch 122/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2766\n",
            "Epoch 123/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2767\n",
            "Epoch 124/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2767\n",
            "Epoch 125/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1761 - val_loss: 0.2768\n",
            "Epoch 126/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2769\n",
            "Epoch 127/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2769\n",
            "Epoch 128/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2769\n",
            "Epoch 129/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2770\n",
            "Epoch 130/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2771\n",
            "Epoch 131/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2771\n",
            "Epoch 132/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1748 - val_loss: 0.2771\n",
            "Epoch 133/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2772\n",
            "Epoch 134/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2774\n",
            "Epoch 135/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2774\n",
            "Epoch 136/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1741 - val_loss: 0.2774\n",
            "Epoch 137/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1748 - val_loss: 0.2774\n",
            "Epoch 138/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2775\n",
            "Epoch 139/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2777\n",
            "Epoch 140/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2776\n",
            "Epoch 141/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1741 - val_loss: 0.2777\n",
            "Epoch 142/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1738 - val_loss: 0.2778\n",
            "Epoch 143/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2778\n",
            "Epoch 144/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1743 - val_loss: 0.2779\n",
            "Epoch 145/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2779\n",
            "Epoch 146/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1738 - val_loss: 0.2779\n",
            "Epoch 147/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2779\n",
            "Epoch 148/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1734 - val_loss: 0.2781\n",
            "Epoch 149/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2780\n",
            "Epoch 150/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2781\n",
            "Epoch 151/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2783\n",
            "Epoch 152/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2782\n",
            "Epoch 153/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2782\n",
            "Epoch 154/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2784\n",
            "Epoch 155/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2783\n",
            "Epoch 156/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2784\n",
            "Epoch 157/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2785\n",
            "Epoch 158/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2785\n",
            "Epoch 159/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2786\n",
            "Epoch 160/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2786\n",
            "Epoch 161/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2787\n",
            "Epoch 162/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2788\n",
            "Epoch 163/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2788\n",
            "Epoch 164/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2788\n",
            "Epoch 165/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1718 - val_loss: 0.2788\n",
            "Epoch 166/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2789\n",
            "Epoch 167/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1718 - val_loss: 0.2789\n",
            "Epoch 168/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2790\n",
            "Epoch 169/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2790\n",
            "Epoch 170/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2791\n",
            "Epoch 171/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2791\n",
            "Epoch 172/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2792\n",
            "Epoch 173/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2793\n",
            "Epoch 174/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2793\n",
            "Epoch 175/500\n",
            "523/523 [==============================] - 3s 6ms/step - loss: 0.1716 - val_loss: 0.2793\n",
            "Epoch 176/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2793\n",
            "Epoch 177/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2794\n",
            "Epoch 178/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2795\n",
            "Epoch 179/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2794\n",
            "Epoch 180/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2795\n",
            "Epoch 181/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2795\n",
            "Epoch 182/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2795\n",
            "Epoch 183/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2795\n",
            "Epoch 184/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2797\n",
            "Epoch 185/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2797\n",
            "Epoch 186/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2797\n",
            "Epoch 187/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2798\n",
            "Epoch 188/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2799\n",
            "Epoch 189/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2799\n",
            "Epoch 190/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2799\n",
            "Epoch 191/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2799\n",
            "Epoch 192/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2800\n",
            "Epoch 193/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2799\n",
            "Epoch 194/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2800\n",
            "Epoch 195/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2801\n",
            "Epoch 196/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2802\n",
            "Epoch 197/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2802\n",
            "Epoch 198/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2802\n",
            "Epoch 199/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2802\n",
            "Epoch 200/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2802\n",
            "Epoch 201/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2802\n",
            "Epoch 202/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2803\n",
            "Epoch 203/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2803\n",
            "Epoch 204/500\n",
            "523/523 [==============================] - 3s 6ms/step - loss: 0.1697 - val_loss: 0.2803\n",
            "Epoch 205/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2804\n",
            "Epoch 206/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2804\n",
            "Epoch 207/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2806\n",
            "Epoch 208/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2805\n",
            "Epoch 209/500\n",
            "523/523 [==============================] - 3s 6ms/step - loss: 0.1692 - val_loss: 0.2806\n",
            "Epoch 210/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2806\n",
            "Epoch 211/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2806\n",
            "Epoch 212/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2806\n",
            "Epoch 213/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2808\n",
            "Epoch 214/500\n",
            "523/523 [==============================] - 3s 6ms/step - loss: 0.1700 - val_loss: 0.2807\n",
            "Epoch 215/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2808\n",
            "Epoch 216/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2808\n",
            "Epoch 217/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2808\n",
            "Epoch 218/500\n",
            "523/523 [==============================] - 3s 6ms/step - loss: 0.1690 - val_loss: 0.2809\n",
            "Epoch 219/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2809\n",
            "Epoch 220/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2809\n",
            "Epoch 221/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2810\n",
            "Epoch 222/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2809\n",
            "Epoch 223/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2811\n",
            "Epoch 224/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2811\n",
            "Epoch 225/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2812\n",
            "Epoch 226/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2811\n",
            "Epoch 227/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2812\n",
            "Epoch 228/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2812\n",
            "Epoch 229/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2812\n",
            "Epoch 230/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1687 - val_loss: 0.2813\n",
            "Epoch 231/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2813\n",
            "Epoch 232/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2813\n",
            "Epoch 233/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2813\n",
            "Epoch 234/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2814\n",
            "Epoch 235/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2815\n",
            "Epoch 236/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2814\n",
            "Epoch 237/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2814\n",
            "Epoch 238/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2815\n",
            "Epoch 239/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2815\n",
            "Epoch 240/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2817\n",
            "Epoch 241/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2815\n",
            "Epoch 242/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2816\n",
            "Epoch 243/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2817\n",
            "Epoch 244/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2816\n",
            "Epoch 245/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2818\n",
            "Epoch 246/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2817\n",
            "Epoch 247/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2817\n",
            "Epoch 248/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2819\n",
            "Epoch 249/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2818\n",
            "Epoch 250/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2818\n",
            "Epoch 251/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2818\n",
            "Epoch 252/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2819\n",
            "Epoch 253/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2819\n",
            "Epoch 254/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2820\n",
            "Epoch 255/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2820\n",
            "Epoch 256/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2821\n",
            "Epoch 257/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2820\n",
            "Epoch 258/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2820\n",
            "Epoch 259/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2821\n",
            "Epoch 260/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2821\n",
            "Epoch 261/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2821\n",
            "Epoch 262/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2822\n",
            "Epoch 263/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2822\n",
            "Epoch 264/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2821\n",
            "Epoch 265/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2822\n",
            "Epoch 266/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2822\n",
            "Epoch 267/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2823\n",
            "Epoch 268/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2823\n",
            "Epoch 269/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2823\n",
            "Epoch 270/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2823\n",
            "Epoch 271/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2824\n",
            "Epoch 272/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2823\n",
            "Epoch 273/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2823\n",
            "Epoch 274/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2825\n",
            "Epoch 275/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2823\n",
            "Epoch 276/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2824\n",
            "Epoch 277/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2824\n",
            "Epoch 278/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2827\n",
            "Epoch 279/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2825\n",
            "Epoch 280/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2826\n",
            "Epoch 281/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2826\n",
            "Epoch 282/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2825\n",
            "Epoch 283/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2827\n",
            "Epoch 284/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2827\n",
            "Epoch 285/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2828\n",
            "Epoch 286/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2827\n",
            "Epoch 287/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2827\n",
            "Epoch 288/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2827\n",
            "Epoch 289/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2828\n",
            "Epoch 290/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2828\n",
            "Epoch 291/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2829\n",
            "Epoch 292/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2827\n",
            "Epoch 293/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2828\n",
            "Epoch 294/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2829\n",
            "Epoch 295/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2830\n",
            "Epoch 296/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2829\n",
            "Epoch 297/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2830\n",
            "Epoch 298/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2828\n",
            "Epoch 299/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2830\n",
            "Epoch 300/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2829\n",
            "Epoch 301/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2831\n",
            "Epoch 302/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2831\n",
            "Epoch 303/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2832\n",
            "Epoch 304/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2831\n",
            "Epoch 305/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2831\n",
            "Epoch 306/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2830\n",
            "Epoch 307/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2832\n",
            "Epoch 308/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2832\n",
            "Epoch 309/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2832\n",
            "Epoch 310/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2831\n",
            "Epoch 311/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2832\n",
            "Epoch 312/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2833\n",
            "Epoch 313/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2833\n",
            "Epoch 314/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2834\n",
            "Epoch 315/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2834\n",
            "Epoch 316/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2834\n",
            "Epoch 317/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2834\n",
            "Epoch 318/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2834\n",
            "Epoch 319/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2834\n",
            "Epoch 320/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2835\n",
            "Epoch 321/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2834\n",
            "Epoch 322/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2836\n",
            "Epoch 323/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2836\n",
            "Epoch 324/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2835\n",
            "Epoch 325/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2835\n",
            "Epoch 326/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2835\n",
            "Epoch 327/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2837\n",
            "Epoch 328/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2838\n",
            "Epoch 329/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2837\n",
            "Epoch 330/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2837\n",
            "Epoch 331/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2836\n",
            "Epoch 332/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2837\n",
            "Epoch 333/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2837\n",
            "Epoch 334/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2837\n",
            "Epoch 335/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2838\n",
            "Epoch 336/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2839\n",
            "Epoch 337/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2839\n",
            "Epoch 338/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2837\n",
            "Epoch 339/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2838\n",
            "Epoch 340/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2839\n",
            "Epoch 341/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2838\n",
            "Epoch 342/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2839\n",
            "Epoch 343/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2840\n",
            "Epoch 344/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2839\n",
            "Epoch 345/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2838\n",
            "Epoch 346/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2839\n",
            "Epoch 347/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2838\n",
            "Epoch 348/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2840\n",
            "Epoch 349/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2840\n",
            "Epoch 350/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2841\n",
            "Epoch 351/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2840\n",
            "Epoch 352/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2840\n",
            "Epoch 353/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2840\n",
            "Epoch 354/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2841\n",
            "Epoch 355/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2843\n",
            "Epoch 356/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2841\n",
            "Epoch 357/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2841\n",
            "Epoch 358/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2841\n",
            "Epoch 359/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2842\n",
            "Epoch 360/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2844\n",
            "Epoch 361/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2841\n",
            "Epoch 362/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2842\n",
            "Epoch 363/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2842\n",
            "Epoch 364/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2843\n",
            "Epoch 365/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2842\n",
            "Epoch 366/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2843\n",
            "Epoch 367/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2842\n",
            "Epoch 368/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2844\n",
            "Epoch 369/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2844\n",
            "Epoch 370/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2844\n",
            "Epoch 371/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2843\n",
            "Epoch 372/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2846\n",
            "Epoch 373/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2845\n",
            "Epoch 374/500\n",
            "523/523 [==============================] - 3s 6ms/step - loss: 0.1654 - val_loss: 0.2845\n",
            "Epoch 375/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2845\n",
            "Epoch 376/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2845\n",
            "Epoch 377/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2845\n",
            "Epoch 378/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2845\n",
            "Epoch 379/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2845\n",
            "Epoch 380/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2845\n",
            "Epoch 381/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2846\n",
            "Epoch 382/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2847\n",
            "Epoch 383/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2846\n",
            "Epoch 384/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2846\n",
            "Epoch 385/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2846\n",
            "Epoch 386/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2847\n",
            "Epoch 387/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2847\n",
            "Epoch 388/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2845\n",
            "Epoch 389/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2847\n",
            "Epoch 390/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2847\n",
            "Epoch 391/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2847\n",
            "Epoch 392/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2846\n",
            "Epoch 393/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2847\n",
            "Epoch 394/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2847\n",
            "Epoch 395/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2847\n",
            "Epoch 396/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2848\n",
            "Epoch 397/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2847\n",
            "Epoch 398/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2848\n",
            "Epoch 399/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2848\n",
            "Epoch 400/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2848\n",
            "Epoch 401/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2848\n",
            "Epoch 402/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2848\n",
            "Epoch 403/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2850\n",
            "Epoch 404/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2849\n",
            "Epoch 405/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2849\n",
            "Epoch 406/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2850\n",
            "Epoch 407/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2850\n",
            "Epoch 408/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2849\n",
            "Epoch 409/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2850\n",
            "Epoch 410/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2850\n",
            "Epoch 411/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2851\n",
            "Epoch 412/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2851\n",
            "Epoch 413/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2850\n",
            "Epoch 414/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2851\n",
            "Epoch 415/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2850\n",
            "Epoch 416/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2852\n",
            "Epoch 417/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2851\n",
            "Epoch 418/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2850\n",
            "Epoch 419/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2851\n",
            "Epoch 420/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2851\n",
            "Epoch 421/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2850\n",
            "Epoch 422/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2851\n",
            "Epoch 423/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2852\n",
            "Epoch 424/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2852\n",
            "Epoch 425/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2852\n",
            "Epoch 426/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2851\n",
            "Epoch 427/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2853\n",
            "Epoch 428/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2851\n",
            "Epoch 429/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2852\n",
            "Epoch 430/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2853\n",
            "Epoch 431/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2853\n",
            "Epoch 432/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2852\n",
            "Epoch 433/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2853\n",
            "Epoch 434/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2853\n",
            "Epoch 435/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2853\n",
            "Epoch 436/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2853\n",
            "Epoch 437/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2854\n",
            "Epoch 438/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2854\n",
            "Epoch 439/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2854\n",
            "Epoch 440/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2853\n",
            "Epoch 441/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2853\n",
            "Epoch 442/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2854\n",
            "Epoch 443/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2854\n",
            "Epoch 444/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2855\n",
            "Epoch 445/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2855\n",
            "Epoch 446/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2855\n",
            "Epoch 447/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2855\n",
            "Epoch 448/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2855\n",
            "Epoch 449/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2855\n",
            "Epoch 450/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2855\n",
            "Epoch 451/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2855\n",
            "Epoch 452/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2857\n",
            "Epoch 453/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2856\n",
            "Epoch 454/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2857\n",
            "Epoch 455/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2856\n",
            "Epoch 456/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2855\n",
            "Epoch 457/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2856\n",
            "Epoch 458/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2856\n",
            "Epoch 459/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2857\n",
            "Epoch 460/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2857\n",
            "Epoch 461/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2858\n",
            "Epoch 462/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2856\n",
            "Epoch 463/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2857\n",
            "Epoch 464/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2857\n",
            "Epoch 465/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2857\n",
            "Epoch 466/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2857\n",
            "Epoch 467/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2856\n",
            "Epoch 468/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2857\n",
            "Epoch 469/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2859\n",
            "Epoch 470/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2858\n",
            "Epoch 471/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2859\n",
            "Epoch 472/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2858\n",
            "Epoch 473/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2859\n",
            "Epoch 474/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2857\n",
            "Epoch 475/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2859\n",
            "Epoch 476/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2858\n",
            "Epoch 477/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2858\n",
            "Epoch 478/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2859\n",
            "Epoch 479/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2859\n",
            "Epoch 480/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2859\n",
            "Epoch 481/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2860\n",
            "Epoch 482/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2859\n",
            "Epoch 483/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2858\n",
            "Epoch 484/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2859\n",
            "Epoch 485/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2859\n",
            "Epoch 486/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2860\n",
            "Epoch 487/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2859\n",
            "Epoch 488/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2859\n",
            "Epoch 489/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2860\n",
            "Epoch 490/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2860\n",
            "Epoch 491/500\n",
            "523/523 [==============================] - 3s 6ms/step - loss: 0.1643 - val_loss: 0.2861\n",
            "Epoch 492/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2860\n",
            "Epoch 493/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2860\n",
            "Epoch 494/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2861\n",
            "Epoch 495/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2860\n",
            "Epoch 496/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2861\n",
            "Epoch 497/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2861\n",
            "Epoch 498/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2861\n",
            "Epoch 499/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2861\n",
            "Epoch 500/500\n",
            "523/523 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2861\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 500, 64, 0.16386516392230988, 0.28612396121025085, 0.02802916, 3.9973739817292135, 0.19710654227018398)\n",
            "Epoch 1/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.3773 - val_loss: 0.3409\n",
            "Epoch 2/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2958 - val_loss: 0.3063\n",
            "Epoch 3/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2756 - val_loss: 0.2924\n",
            "Epoch 4/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2680 - val_loss: 0.2875\n",
            "Epoch 5/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2646 - val_loss: 0.2855\n",
            "Epoch 6/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2629 - val_loss: 0.2838\n",
            "Epoch 7/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2614 - val_loss: 0.2831\n",
            "Epoch 8/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2600 - val_loss: 0.2821\n",
            "Epoch 9/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2585 - val_loss: 0.2816\n",
            "Epoch 10/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2567 - val_loss: 0.2809\n",
            "Epoch 11/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2546 - val_loss: 0.2799\n",
            "Epoch 12/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2522 - val_loss: 0.2791\n",
            "Epoch 13/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2502 - val_loss: 0.2785\n",
            "Epoch 14/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2478 - val_loss: 0.2778\n",
            "Epoch 15/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2453 - val_loss: 0.2770\n",
            "Epoch 16/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2429 - val_loss: 0.2763\n",
            "Epoch 17/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2403 - val_loss: 0.2756\n",
            "Epoch 18/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2388 - val_loss: 0.2752\n",
            "Epoch 19/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2362 - val_loss: 0.2745\n",
            "Epoch 20/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2342 - val_loss: 0.2739\n",
            "Epoch 21/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2323 - val_loss: 0.2735\n",
            "Epoch 22/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2304 - val_loss: 0.2730\n",
            "Epoch 23/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2283 - val_loss: 0.2725\n",
            "Epoch 24/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2263 - val_loss: 0.2722\n",
            "Epoch 25/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2249 - val_loss: 0.2718\n",
            "Epoch 26/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2230 - val_loss: 0.2715\n",
            "Epoch 27/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2211 - val_loss: 0.2712\n",
            "Epoch 28/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2199 - val_loss: 0.2710\n",
            "Epoch 29/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2180 - val_loss: 0.2708\n",
            "Epoch 30/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2170 - val_loss: 0.2705\n",
            "Epoch 31/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2153 - val_loss: 0.2704\n",
            "Epoch 32/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2138 - val_loss: 0.2703\n",
            "Epoch 33/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2128 - val_loss: 0.2701\n",
            "Epoch 34/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2114 - val_loss: 0.2700\n",
            "Epoch 35/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2103 - val_loss: 0.2699\n",
            "Epoch 36/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2088 - val_loss: 0.2697\n",
            "Epoch 37/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2077 - val_loss: 0.2698\n",
            "Epoch 38/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2068 - val_loss: 0.2696\n",
            "Epoch 39/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2063 - val_loss: 0.2698\n",
            "Epoch 40/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2050 - val_loss: 0.2696\n",
            "Epoch 41/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2044 - val_loss: 0.2695\n",
            "Epoch 42/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2031 - val_loss: 0.2697\n",
            "Epoch 43/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2022 - val_loss: 0.2695\n",
            "Epoch 44/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2014 - val_loss: 0.2696\n",
            "Epoch 45/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2005 - val_loss: 0.2697\n",
            "Epoch 46/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1997 - val_loss: 0.2698\n",
            "Epoch 47/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1990 - val_loss: 0.2696\n",
            "Epoch 48/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1983 - val_loss: 0.2699\n",
            "Epoch 49/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1977 - val_loss: 0.2698\n",
            "Epoch 50/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1968 - val_loss: 0.2698\n",
            "Epoch 51/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1959 - val_loss: 0.2697\n",
            "Epoch 52/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1956 - val_loss: 0.2699\n",
            "Epoch 53/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1946 - val_loss: 0.2699\n",
            "Epoch 54/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1942 - val_loss: 0.2701\n",
            "Epoch 55/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1933 - val_loss: 0.2701\n",
            "Epoch 56/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1929 - val_loss: 0.2701\n",
            "Epoch 57/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1926 - val_loss: 0.2702\n",
            "Epoch 58/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1915 - val_loss: 0.2702\n",
            "Epoch 59/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1912 - val_loss: 0.2704\n",
            "Epoch 60/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1911 - val_loss: 0.2705\n",
            "Epoch 61/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1903 - val_loss: 0.2704\n",
            "Epoch 62/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1898 - val_loss: 0.2705\n",
            "Epoch 63/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1894 - val_loss: 0.2706\n",
            "Epoch 64/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1890 - val_loss: 0.2706\n",
            "Epoch 65/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1882 - val_loss: 0.2707\n",
            "Epoch 66/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1880 - val_loss: 0.2708\n",
            "Epoch 67/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1876 - val_loss: 0.2709\n",
            "Epoch 68/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1872 - val_loss: 0.2710\n",
            "Epoch 69/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1868 - val_loss: 0.2710\n",
            "Epoch 70/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1862 - val_loss: 0.2711\n",
            "Epoch 71/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1861 - val_loss: 0.2712\n",
            "Epoch 72/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1855 - val_loss: 0.2713\n",
            "Epoch 73/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1850 - val_loss: 0.2714\n",
            "Epoch 74/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1852 - val_loss: 0.2714\n",
            "Epoch 75/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1850 - val_loss: 0.2715\n",
            "Epoch 76/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1843 - val_loss: 0.2716\n",
            "Epoch 77/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1840 - val_loss: 0.2716\n",
            "Epoch 78/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1837 - val_loss: 0.2718\n",
            "Epoch 79/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1836 - val_loss: 0.2719\n",
            "Epoch 80/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1829 - val_loss: 0.2720\n",
            "Epoch 81/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1829 - val_loss: 0.2720\n",
            "Epoch 82/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1821 - val_loss: 0.2720\n",
            "Epoch 83/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1819 - val_loss: 0.2722\n",
            "Epoch 84/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1821 - val_loss: 0.2722\n",
            "Epoch 85/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1815 - val_loss: 0.2723\n",
            "Epoch 86/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1813 - val_loss: 0.2724\n",
            "Epoch 87/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1812 - val_loss: 0.2725\n",
            "Epoch 88/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1808 - val_loss: 0.2725\n",
            "Epoch 89/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1804 - val_loss: 0.2726\n",
            "Epoch 90/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1801 - val_loss: 0.2726\n",
            "Epoch 91/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1802 - val_loss: 0.2728\n",
            "Epoch 92/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1799 - val_loss: 0.2728\n",
            "Epoch 93/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1801 - val_loss: 0.2728\n",
            "Epoch 94/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2730\n",
            "Epoch 95/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2730\n",
            "Epoch 96/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1791 - val_loss: 0.2732\n",
            "Epoch 97/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1791 - val_loss: 0.2730\n",
            "Epoch 98/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1789 - val_loss: 0.2732\n",
            "Epoch 99/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1789 - val_loss: 0.2733\n",
            "Epoch 100/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1778 - val_loss: 0.2733\n",
            "Epoch 101/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1781 - val_loss: 0.2734\n",
            "Epoch 102/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1777 - val_loss: 0.2735\n",
            "Epoch 103/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1776 - val_loss: 0.2736\n",
            "Epoch 104/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1779 - val_loss: 0.2736\n",
            "Epoch 105/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.2737\n",
            "Epoch 106/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2738\n",
            "Epoch 107/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2739\n",
            "Epoch 108/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2739\n",
            "Epoch 109/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2741\n",
            "Epoch 110/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2741\n",
            "Epoch 111/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1764 - val_loss: 0.2742\n",
            "Epoch 112/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1764 - val_loss: 0.2742\n",
            "Epoch 113/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2742\n",
            "Epoch 114/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1761 - val_loss: 0.2743\n",
            "Epoch 115/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1757 - val_loss: 0.2744\n",
            "Epoch 116/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2744\n",
            "Epoch 117/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1755 - val_loss: 0.2746\n",
            "Epoch 118/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1759 - val_loss: 0.2745\n",
            "Epoch 119/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1756 - val_loss: 0.2746\n",
            "Epoch 120/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2747\n",
            "Epoch 121/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2748\n",
            "Epoch 122/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2749\n",
            "Epoch 123/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2750\n",
            "Epoch 124/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2749\n",
            "Epoch 125/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2750\n",
            "Epoch 126/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2751\n",
            "Epoch 127/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2752\n",
            "Epoch 128/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2752\n",
            "Epoch 129/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2753\n",
            "Epoch 130/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2752\n",
            "Epoch 131/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2755\n",
            "Epoch 132/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2754\n",
            "Epoch 133/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2755\n",
            "Epoch 134/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2755\n",
            "Epoch 135/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2756\n",
            "Epoch 136/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2756\n",
            "Epoch 137/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2758\n",
            "Epoch 138/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2757\n",
            "Epoch 139/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2758\n",
            "Epoch 140/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2759\n",
            "Epoch 141/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2760\n",
            "Epoch 142/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2760\n",
            "Epoch 143/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2760\n",
            "Epoch 144/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2761\n",
            "Epoch 145/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2761\n",
            "Epoch 146/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2762\n",
            "Epoch 147/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2762\n",
            "Epoch 148/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2763\n",
            "Epoch 149/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2762\n",
            "Epoch 150/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2764\n",
            "Epoch 151/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2764\n",
            "Epoch 152/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2763\n",
            "Epoch 153/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2765\n",
            "Epoch 154/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2766\n",
            "Epoch 155/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2765\n",
            "Epoch 156/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2767\n",
            "Epoch 157/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2766\n",
            "Epoch 158/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2767\n",
            "Epoch 159/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2767\n",
            "Epoch 160/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2769\n",
            "Epoch 161/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2768\n",
            "Epoch 162/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2769\n",
            "Epoch 163/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2769\n",
            "Epoch 164/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2769\n",
            "Epoch 165/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2770\n",
            "Epoch 166/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2771\n",
            "Epoch 167/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2772\n",
            "Epoch 168/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2771\n",
            "Epoch 169/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2773\n",
            "Epoch 170/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2771\n",
            "Epoch 171/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2773\n",
            "Epoch 172/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2773\n",
            "Epoch 173/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2773\n",
            "Epoch 174/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2775\n",
            "Epoch 175/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2775\n",
            "Epoch 176/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2776\n",
            "Epoch 177/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2776\n",
            "Epoch 178/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2775\n",
            "Epoch 179/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2776\n",
            "Epoch 180/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2776\n",
            "Epoch 181/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2776\n",
            "Epoch 182/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2777\n",
            "Epoch 183/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2777\n",
            "Epoch 184/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2777\n",
            "Epoch 185/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2780\n",
            "Epoch 186/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2780\n",
            "Epoch 187/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2780\n",
            "Epoch 188/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2780\n",
            "Epoch 189/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2780\n",
            "Epoch 190/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2780\n",
            "Epoch 191/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2780\n",
            "Epoch 192/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2782\n",
            "Epoch 193/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2781\n",
            "Epoch 194/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2781\n",
            "Epoch 195/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2783\n",
            "Epoch 196/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2784\n",
            "Epoch 197/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2782\n",
            "Epoch 198/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2785\n",
            "Epoch 199/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2784\n",
            "Epoch 200/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2784\n",
            "Epoch 201/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1687 - val_loss: 0.2784\n",
            "Epoch 202/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2784\n",
            "Epoch 203/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2786\n",
            "Epoch 204/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2785\n",
            "Epoch 205/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2786\n",
            "Epoch 206/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2786\n",
            "Epoch 207/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2786\n",
            "Epoch 208/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2786\n",
            "Epoch 209/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2787\n",
            "Epoch 210/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2787\n",
            "Epoch 211/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2787\n",
            "Epoch 212/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2788\n",
            "Epoch 213/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2789\n",
            "Epoch 214/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1687 - val_loss: 0.2788\n",
            "Epoch 215/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2789\n",
            "Epoch 216/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2789\n",
            "Epoch 217/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2790\n",
            "Epoch 218/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2790\n",
            "Epoch 219/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2790\n",
            "Epoch 220/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2792\n",
            "Epoch 221/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2793\n",
            "Epoch 222/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2791\n",
            "Epoch 223/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2792\n",
            "Epoch 224/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2792\n",
            "Epoch 225/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2793\n",
            "Epoch 226/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2793\n",
            "Epoch 227/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2792\n",
            "Epoch 228/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2793\n",
            "Epoch 229/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2793\n",
            "Epoch 230/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2793\n",
            "Epoch 231/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2793\n",
            "Epoch 232/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2795\n",
            "Epoch 233/500\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1676 - val_loss: 0.2795\n",
            "Epoch 234/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2795\n",
            "Epoch 235/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2795\n",
            "Epoch 236/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2797\n",
            "Epoch 237/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2796\n",
            "Epoch 238/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2796\n",
            "Epoch 239/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2796\n",
            "Epoch 240/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2798\n",
            "Epoch 241/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2798\n",
            "Epoch 242/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2799\n",
            "Epoch 243/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2797\n",
            "Epoch 244/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2799\n",
            "Epoch 245/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2797\n",
            "Epoch 246/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2798\n",
            "Epoch 247/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2798\n",
            "Epoch 248/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2798\n",
            "Epoch 249/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2799\n",
            "Epoch 250/500\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1669 - val_loss: 0.2799\n",
            "Epoch 251/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2800\n",
            "Epoch 252/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2801\n",
            "Epoch 253/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2801\n",
            "Epoch 254/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2800\n",
            "Epoch 255/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2801\n",
            "Epoch 256/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2801\n",
            "Epoch 257/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2801\n",
            "Epoch 258/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2801\n",
            "Epoch 259/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2802\n",
            "Epoch 260/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2802\n",
            "Epoch 261/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2803\n",
            "Epoch 262/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2803\n",
            "Epoch 263/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2803\n",
            "Epoch 264/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2802\n",
            "Epoch 265/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2804\n",
            "Epoch 266/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2804\n",
            "Epoch 267/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2804\n",
            "Epoch 268/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2804\n",
            "Epoch 269/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2805\n",
            "Epoch 270/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2803\n",
            "Epoch 271/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2805\n",
            "Epoch 272/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2805\n",
            "Epoch 273/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2804\n",
            "Epoch 274/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2807\n",
            "Epoch 275/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2806\n",
            "Epoch 276/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2806\n",
            "Epoch 277/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2807\n",
            "Epoch 278/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2805\n",
            "Epoch 279/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2806\n",
            "Epoch 280/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2806\n",
            "Epoch 281/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2806\n",
            "Epoch 282/500\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1661 - val_loss: 0.2808\n",
            "Epoch 283/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2806\n",
            "Epoch 284/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2807\n",
            "Epoch 285/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2808\n",
            "Epoch 286/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2808\n",
            "Epoch 287/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2809\n",
            "Epoch 288/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2810\n",
            "Epoch 289/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2809\n",
            "Epoch 290/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2809\n",
            "Epoch 291/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2810\n",
            "Epoch 292/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2809\n",
            "Epoch 293/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2809\n",
            "Epoch 294/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2810\n",
            "Epoch 295/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2810\n",
            "Epoch 296/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2810\n",
            "Epoch 297/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2811\n",
            "Epoch 298/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2811\n",
            "Epoch 299/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2811\n",
            "Epoch 300/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2812\n",
            "Epoch 301/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2811\n",
            "Epoch 302/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2812\n",
            "Epoch 303/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2812\n",
            "Epoch 304/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2813\n",
            "Epoch 305/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2811\n",
            "Epoch 306/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2812\n",
            "Epoch 307/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2812\n",
            "Epoch 308/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2813\n",
            "Epoch 309/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2813\n",
            "Epoch 310/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2814\n",
            "Epoch 311/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2815\n",
            "Epoch 312/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2814\n",
            "Epoch 313/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2815\n",
            "Epoch 314/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2813\n",
            "Epoch 315/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2814\n",
            "Epoch 316/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2814\n",
            "Epoch 317/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2815\n",
            "Epoch 318/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2815\n",
            "Epoch 319/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2815\n",
            "Epoch 320/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2815\n",
            "Epoch 321/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2815\n",
            "Epoch 322/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2817\n",
            "Epoch 323/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2816\n",
            "Epoch 324/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2815\n",
            "Epoch 325/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2815\n",
            "Epoch 326/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2816\n",
            "Epoch 327/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2816\n",
            "Epoch 328/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2817\n",
            "Epoch 329/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2817\n",
            "Epoch 330/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2818\n",
            "Epoch 331/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2817\n",
            "Epoch 332/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2816\n",
            "Epoch 333/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2818\n",
            "Epoch 334/500\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1652 - val_loss: 0.2817\n",
            "Epoch 335/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2818\n",
            "Epoch 336/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2818\n",
            "Epoch 337/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2818\n",
            "Epoch 338/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2818\n",
            "Epoch 339/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2819\n",
            "Epoch 340/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2819\n",
            "Epoch 341/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2819\n",
            "Epoch 342/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2819\n",
            "Epoch 343/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2820\n",
            "Epoch 344/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2820\n",
            "Epoch 345/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2820\n",
            "Epoch 346/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2820\n",
            "Epoch 347/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2820\n",
            "Epoch 348/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2820\n",
            "Epoch 349/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2821\n",
            "Epoch 350/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2821\n",
            "Epoch 351/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2821\n",
            "Epoch 352/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2821\n",
            "Epoch 353/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2822\n",
            "Epoch 354/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2821\n",
            "Epoch 355/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2821\n",
            "Epoch 356/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2822\n",
            "Epoch 357/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2821\n",
            "Epoch 358/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2823\n",
            "Epoch 359/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2822\n",
            "Epoch 360/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2822\n",
            "Epoch 361/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2822\n",
            "Epoch 362/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2823\n",
            "Epoch 363/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2823\n",
            "Epoch 364/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2823\n",
            "Epoch 365/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2823\n",
            "Epoch 366/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2823\n",
            "Epoch 367/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2824\n",
            "Epoch 368/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2823\n",
            "Epoch 369/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2824\n",
            "Epoch 370/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2824\n",
            "Epoch 371/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2825\n",
            "Epoch 372/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2824\n",
            "Epoch 373/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2825\n",
            "Epoch 374/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2825\n",
            "Epoch 375/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2824\n",
            "Epoch 376/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2826\n",
            "Epoch 377/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2826\n",
            "Epoch 378/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2825\n",
            "Epoch 379/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2826\n",
            "Epoch 380/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2825\n",
            "Epoch 381/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2825\n",
            "Epoch 382/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2825\n",
            "Epoch 383/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2826\n",
            "Epoch 384/500\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1639 - val_loss: 0.2826\n",
            "Epoch 385/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2826\n",
            "Epoch 386/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2826\n",
            "Epoch 387/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2827\n",
            "Epoch 388/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2827\n",
            "Epoch 389/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2827\n",
            "Epoch 390/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2826\n",
            "Epoch 391/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2826\n",
            "Epoch 392/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2827\n",
            "Epoch 393/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2828\n",
            "Epoch 394/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2827\n",
            "Epoch 395/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2828\n",
            "Epoch 396/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2828\n",
            "Epoch 397/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2830\n",
            "Epoch 398/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2828\n",
            "Epoch 399/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2828\n",
            "Epoch 400/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2829\n",
            "Epoch 401/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2829\n",
            "Epoch 402/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2829\n",
            "Epoch 403/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2827\n",
            "Epoch 404/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2829\n",
            "Epoch 405/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2829\n",
            "Epoch 406/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2829\n",
            "Epoch 407/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2830\n",
            "Epoch 408/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2829\n",
            "Epoch 409/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2830\n",
            "Epoch 410/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2830\n",
            "Epoch 411/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2829\n",
            "Epoch 412/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2829\n",
            "Epoch 413/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2830\n",
            "Epoch 414/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2831\n",
            "Epoch 415/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2830\n",
            "Epoch 416/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2831\n",
            "Epoch 417/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2831\n",
            "Epoch 418/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2832\n",
            "Epoch 419/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2831\n",
            "Epoch 420/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2832\n",
            "Epoch 421/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2831\n",
            "Epoch 422/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2832\n",
            "Epoch 423/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2832\n",
            "Epoch 424/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2833\n",
            "Epoch 425/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2831\n",
            "Epoch 426/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2832\n",
            "Epoch 427/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2831\n",
            "Epoch 428/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2833\n",
            "Epoch 429/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2833\n",
            "Epoch 430/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2832\n",
            "Epoch 431/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2833\n",
            "Epoch 432/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2832\n",
            "Epoch 433/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2832\n",
            "Epoch 434/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2832\n",
            "Epoch 435/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2833\n",
            "Epoch 436/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2834\n",
            "Epoch 437/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2833\n",
            "Epoch 438/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2833\n",
            "Epoch 439/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2834\n",
            "Epoch 440/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2835\n",
            "Epoch 441/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2834\n",
            "Epoch 442/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2835\n",
            "Epoch 443/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2836\n",
            "Epoch 444/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2834\n",
            "Epoch 445/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2835\n",
            "Epoch 446/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2835\n",
            "Epoch 447/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2834\n",
            "Epoch 448/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2835\n",
            "Epoch 449/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2834\n",
            "Epoch 450/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2835\n",
            "Epoch 451/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2835\n",
            "Epoch 452/500\n",
            "531/531 [==============================] - 2s 5ms/step - loss: 0.1634 - val_loss: 0.2836\n",
            "Epoch 453/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2835\n",
            "Epoch 454/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2835\n",
            "Epoch 455/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2836\n",
            "Epoch 456/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2835\n",
            "Epoch 457/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2836\n",
            "Epoch 458/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2835\n",
            "Epoch 459/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2836\n",
            "Epoch 460/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2836\n",
            "Epoch 461/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2836\n",
            "Epoch 462/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2837\n",
            "Epoch 463/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2838\n",
            "Epoch 464/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2837\n",
            "Epoch 465/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2837\n",
            "Epoch 466/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2837\n",
            "Epoch 467/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2838\n",
            "Epoch 468/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2838\n",
            "Epoch 469/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2838\n",
            "Epoch 470/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2838\n",
            "Epoch 471/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2838\n",
            "Epoch 472/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2837\n",
            "Epoch 473/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2839\n",
            "Epoch 474/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2839\n",
            "Epoch 475/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2837\n",
            "Epoch 476/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2838\n",
            "Epoch 477/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2839\n",
            "Epoch 478/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2839\n",
            "Epoch 479/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2837\n",
            "Epoch 480/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2838\n",
            "Epoch 481/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2841\n",
            "Epoch 482/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2839\n",
            "Epoch 483/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2838\n",
            "Epoch 484/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2839\n",
            "Epoch 485/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2841\n",
            "Epoch 486/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2840\n",
            "Epoch 487/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2840\n",
            "Epoch 488/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2840\n",
            "Epoch 489/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2840\n",
            "Epoch 490/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2840\n",
            "Epoch 491/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2840\n",
            "Epoch 492/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2839\n",
            "Epoch 493/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2841\n",
            "Epoch 494/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2839\n",
            "Epoch 495/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2839\n",
            "Epoch 496/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2841\n",
            "Epoch 497/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2841\n",
            "Epoch 498/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2841\n",
            "Epoch 499/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2841\n",
            "Epoch 500/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2841\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 500, 64, 0.16280469298362732, 0.2840763330459595, 0.026911166, 4.079782469853333, 0.24274816160377513)\n",
            "Epoch 1/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.3797 - val_loss: 0.3409\n",
            "Epoch 2/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2968 - val_loss: 0.3051\n",
            "Epoch 3/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2765 - val_loss: 0.2910\n",
            "Epoch 4/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2691 - val_loss: 0.2862\n",
            "Epoch 5/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2657 - val_loss: 0.2840\n",
            "Epoch 6/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2640 - val_loss: 0.2830\n",
            "Epoch 7/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2621 - val_loss: 0.2821\n",
            "Epoch 8/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2612 - val_loss: 0.2813\n",
            "Epoch 9/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2595 - val_loss: 0.2808\n",
            "Epoch 10/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2579 - val_loss: 0.2800\n",
            "Epoch 11/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2554 - val_loss: 0.2794\n",
            "Epoch 12/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2532 - val_loss: 0.2785\n",
            "Epoch 13/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2512 - val_loss: 0.2777\n",
            "Epoch 14/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2486 - val_loss: 0.2772\n",
            "Epoch 15/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2466 - val_loss: 0.2765\n",
            "Epoch 16/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2437 - val_loss: 0.2759\n",
            "Epoch 17/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2411 - val_loss: 0.2753\n",
            "Epoch 18/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2392 - val_loss: 0.2747\n",
            "Epoch 19/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2371 - val_loss: 0.2743\n",
            "Epoch 20/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2352 - val_loss: 0.2739\n",
            "Epoch 21/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2328 - val_loss: 0.2731\n",
            "Epoch 22/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2306 - val_loss: 0.2726\n",
            "Epoch 23/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2288 - val_loss: 0.2722\n",
            "Epoch 24/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2269 - val_loss: 0.2718\n",
            "Epoch 25/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2254 - val_loss: 0.2716\n",
            "Epoch 26/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2235 - val_loss: 0.2712\n",
            "Epoch 27/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2217 - val_loss: 0.2711\n",
            "Epoch 28/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2204 - val_loss: 0.2708\n",
            "Epoch 29/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2190 - val_loss: 0.2707\n",
            "Epoch 30/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2173 - val_loss: 0.2703\n",
            "Epoch 31/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2158 - val_loss: 0.2701\n",
            "Epoch 32/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2146 - val_loss: 0.2701\n",
            "Epoch 33/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2132 - val_loss: 0.2699\n",
            "Epoch 34/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2119 - val_loss: 0.2697\n",
            "Epoch 35/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2107 - val_loss: 0.2697\n",
            "Epoch 36/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2095 - val_loss: 0.2695\n",
            "Epoch 37/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2081 - val_loss: 0.2697\n",
            "Epoch 38/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2073 - val_loss: 0.2693\n",
            "Epoch 39/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2063 - val_loss: 0.2694\n",
            "Epoch 40/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2058 - val_loss: 0.2694\n",
            "Epoch 41/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2045 - val_loss: 0.2693\n",
            "Epoch 42/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2036 - val_loss: 0.2693\n",
            "Epoch 43/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2026 - val_loss: 0.2693\n",
            "Epoch 44/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2016 - val_loss: 0.2693\n",
            "Epoch 45/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.2006 - val_loss: 0.2693\n",
            "Epoch 46/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1998 - val_loss: 0.2693\n",
            "Epoch 47/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1990 - val_loss: 0.2694\n",
            "Epoch 48/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1984 - val_loss: 0.2694\n",
            "Epoch 49/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1979 - val_loss: 0.2692\n",
            "Epoch 50/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1971 - val_loss: 0.2695\n",
            "Epoch 51/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1960 - val_loss: 0.2695\n",
            "Epoch 52/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1958 - val_loss: 0.2696\n",
            "Epoch 53/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1950 - val_loss: 0.2695\n",
            "Epoch 54/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1946 - val_loss: 0.2695\n",
            "Epoch 55/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1940 - val_loss: 0.2697\n",
            "Epoch 56/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1932 - val_loss: 0.2697\n",
            "Epoch 57/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1927 - val_loss: 0.2699\n",
            "Epoch 58/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1917 - val_loss: 0.2697\n",
            "Epoch 59/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1913 - val_loss: 0.2699\n",
            "Epoch 60/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1910 - val_loss: 0.2699\n",
            "Epoch 61/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1904 - val_loss: 0.2700\n",
            "Epoch 62/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1901 - val_loss: 0.2700\n",
            "Epoch 63/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1892 - val_loss: 0.2700\n",
            "Epoch 64/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1895 - val_loss: 0.2701\n",
            "Epoch 65/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1885 - val_loss: 0.2704\n",
            "Epoch 66/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1881 - val_loss: 0.2703\n",
            "Epoch 67/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1876 - val_loss: 0.2705\n",
            "Epoch 68/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1871 - val_loss: 0.2704\n",
            "Epoch 69/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1869 - val_loss: 0.2705\n",
            "Epoch 70/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1862 - val_loss: 0.2706\n",
            "Epoch 71/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1861 - val_loss: 0.2707\n",
            "Epoch 72/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1856 - val_loss: 0.2707\n",
            "Epoch 73/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1854 - val_loss: 0.2709\n",
            "Epoch 74/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1848 - val_loss: 0.2710\n",
            "Epoch 75/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1843 - val_loss: 0.2710\n",
            "Epoch 76/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1837 - val_loss: 0.2711\n",
            "Epoch 77/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1843 - val_loss: 0.2712\n",
            "Epoch 78/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1841 - val_loss: 0.2712\n",
            "Epoch 79/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1836 - val_loss: 0.2712\n",
            "Epoch 80/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1828 - val_loss: 0.2714\n",
            "Epoch 81/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1827 - val_loss: 0.2714\n",
            "Epoch 82/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1822 - val_loss: 0.2714\n",
            "Epoch 83/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1820 - val_loss: 0.2715\n",
            "Epoch 84/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1817 - val_loss: 0.2717\n",
            "Epoch 85/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1815 - val_loss: 0.2719\n",
            "Epoch 86/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1815 - val_loss: 0.2719\n",
            "Epoch 87/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1811 - val_loss: 0.2720\n",
            "Epoch 88/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1810 - val_loss: 0.2720\n",
            "Epoch 89/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1806 - val_loss: 0.2720\n",
            "Epoch 90/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1806 - val_loss: 0.2720\n",
            "Epoch 91/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1801 - val_loss: 0.2721\n",
            "Epoch 92/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1802 - val_loss: 0.2723\n",
            "Epoch 93/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2723\n",
            "Epoch 94/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2723\n",
            "Epoch 95/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1795 - val_loss: 0.2725\n",
            "Epoch 96/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1789 - val_loss: 0.2726\n",
            "Epoch 97/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1786 - val_loss: 0.2727\n",
            "Epoch 98/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1785 - val_loss: 0.2727\n",
            "Epoch 99/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2727\n",
            "Epoch 100/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1785 - val_loss: 0.2728\n",
            "Epoch 101/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1783 - val_loss: 0.2728\n",
            "Epoch 102/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1777 - val_loss: 0.2729\n",
            "Epoch 103/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1779 - val_loss: 0.2730\n",
            "Epoch 104/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2731\n",
            "Epoch 105/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1771 - val_loss: 0.2731\n",
            "Epoch 106/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2732\n",
            "Epoch 107/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2731\n",
            "Epoch 108/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2733\n",
            "Epoch 109/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2734\n",
            "Epoch 110/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1768 - val_loss: 0.2735\n",
            "Epoch 111/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2735\n",
            "Epoch 112/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1763 - val_loss: 0.2737\n",
            "Epoch 113/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1759 - val_loss: 0.2736\n",
            "Epoch 114/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2737\n",
            "Epoch 115/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1756 - val_loss: 0.2738\n",
            "Epoch 116/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1758 - val_loss: 0.2738\n",
            "Epoch 117/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2739\n",
            "Epoch 118/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2740\n",
            "Epoch 119/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2741\n",
            "Epoch 120/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2741\n",
            "Epoch 121/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2741\n",
            "Epoch 122/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2741\n",
            "Epoch 123/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2744\n",
            "Epoch 124/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2745\n",
            "Epoch 125/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2744\n",
            "Epoch 126/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2744\n",
            "Epoch 127/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2745\n",
            "Epoch 128/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2745\n",
            "Epoch 129/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2747\n",
            "Epoch 130/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2747\n",
            "Epoch 131/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2748\n",
            "Epoch 132/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2748\n",
            "Epoch 133/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1735 - val_loss: 0.2749\n",
            "Epoch 134/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2749\n",
            "Epoch 135/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2750\n",
            "Epoch 136/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2750\n",
            "Epoch 137/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2751\n",
            "Epoch 138/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2751\n",
            "Epoch 139/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2751\n",
            "Epoch 140/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2752\n",
            "Epoch 141/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2753\n",
            "Epoch 142/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2753\n",
            "Epoch 143/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2755\n",
            "Epoch 144/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1725 - val_loss: 0.2755\n",
            "Epoch 145/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2754\n",
            "Epoch 146/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2755\n",
            "Epoch 147/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2755\n",
            "Epoch 148/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2756\n",
            "Epoch 149/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2757\n",
            "Epoch 150/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2758\n",
            "Epoch 151/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2759\n",
            "Epoch 152/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2759\n",
            "Epoch 153/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2758\n",
            "Epoch 154/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2760\n",
            "Epoch 155/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2759\n",
            "Epoch 156/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2760\n",
            "Epoch 157/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2759\n",
            "Epoch 158/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2761\n",
            "Epoch 159/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2761\n",
            "Epoch 160/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2761\n",
            "Epoch 161/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1709 - val_loss: 0.2762\n",
            "Epoch 162/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2763\n",
            "Epoch 163/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2763\n",
            "Epoch 164/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2763\n",
            "Epoch 165/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2764\n",
            "Epoch 166/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2764\n",
            "Epoch 167/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2765\n",
            "Epoch 168/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2765\n",
            "Epoch 169/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2765\n",
            "Epoch 170/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2766\n",
            "Epoch 171/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2766\n",
            "Epoch 172/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2768\n",
            "Epoch 173/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2768\n",
            "Epoch 174/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2768\n",
            "Epoch 175/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2768\n",
            "Epoch 176/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2769\n",
            "Epoch 177/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2769\n",
            "Epoch 178/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2771\n",
            "Epoch 179/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2769\n",
            "Epoch 180/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2771\n",
            "Epoch 181/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2772\n",
            "Epoch 182/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2772\n",
            "Epoch 183/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2771\n",
            "Epoch 184/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2771\n",
            "Epoch 185/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2772\n",
            "Epoch 186/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2773\n",
            "Epoch 187/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2773\n",
            "Epoch 188/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2773\n",
            "Epoch 189/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2773\n",
            "Epoch 190/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2774\n",
            "Epoch 191/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2774\n",
            "Epoch 192/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2775\n",
            "Epoch 193/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2776\n",
            "Epoch 194/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2777\n",
            "Epoch 195/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2777\n",
            "Epoch 196/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2775\n",
            "Epoch 197/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2775\n",
            "Epoch 198/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2779\n",
            "Epoch 199/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2778\n",
            "Epoch 200/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2777\n",
            "Epoch 201/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2778\n",
            "Epoch 202/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2778\n",
            "Epoch 203/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2778\n",
            "Epoch 204/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2779\n",
            "Epoch 205/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2779\n",
            "Epoch 206/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2780\n",
            "Epoch 207/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2779\n",
            "Epoch 208/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2781\n",
            "Epoch 209/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2782\n",
            "Epoch 210/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2782\n",
            "Epoch 211/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2781\n",
            "Epoch 212/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2782\n",
            "Epoch 213/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2782\n",
            "Epoch 214/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2783\n",
            "Epoch 215/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2782\n",
            "Epoch 216/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2784\n",
            "Epoch 217/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2783\n",
            "Epoch 218/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2783\n",
            "Epoch 219/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2784\n",
            "Epoch 220/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2785\n",
            "Epoch 221/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2785\n",
            "Epoch 222/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2786\n",
            "Epoch 223/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2786\n",
            "Epoch 224/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2786\n",
            "Epoch 225/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2786\n",
            "Epoch 226/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2788\n",
            "Epoch 227/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2787\n",
            "Epoch 228/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2786\n",
            "Epoch 229/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2787\n",
            "Epoch 230/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2788\n",
            "Epoch 231/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2787\n",
            "Epoch 232/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2788\n",
            "Epoch 233/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2788\n",
            "Epoch 234/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2789\n",
            "Epoch 235/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2788\n",
            "Epoch 236/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2790\n",
            "Epoch 237/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2789\n",
            "Epoch 238/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2790\n",
            "Epoch 239/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2790\n",
            "Epoch 240/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2790\n",
            "Epoch 241/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2790\n",
            "Epoch 242/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2791\n",
            "Epoch 243/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2791\n",
            "Epoch 244/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2792\n",
            "Epoch 245/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2792\n",
            "Epoch 246/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2792\n",
            "Epoch 247/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2792\n",
            "Epoch 248/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2792\n",
            "Epoch 249/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2793\n",
            "Epoch 250/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2794\n",
            "Epoch 251/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2793\n",
            "Epoch 252/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2794\n",
            "Epoch 253/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2794\n",
            "Epoch 254/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2793\n",
            "Epoch 255/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2794\n",
            "Epoch 256/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2795\n",
            "Epoch 257/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2795\n",
            "Epoch 258/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2796\n",
            "Epoch 259/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2796\n",
            "Epoch 260/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2795\n",
            "Epoch 261/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2796\n",
            "Epoch 262/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2798\n",
            "Epoch 263/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2798\n",
            "Epoch 264/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2796\n",
            "Epoch 265/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2796\n",
            "Epoch 266/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2798\n",
            "Epoch 267/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2798\n",
            "Epoch 268/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2798\n",
            "Epoch 269/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2798\n",
            "Epoch 270/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2797\n",
            "Epoch 271/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2797\n",
            "Epoch 272/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2800\n",
            "Epoch 273/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2799\n",
            "Epoch 274/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2799\n",
            "Epoch 275/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2798\n",
            "Epoch 276/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2799\n",
            "Epoch 277/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2800\n",
            "Epoch 278/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2800\n",
            "Epoch 279/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2799\n",
            "Epoch 280/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2801\n",
            "Epoch 281/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2800\n",
            "Epoch 282/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2800\n",
            "Epoch 283/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2801\n",
            "Epoch 284/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2801\n",
            "Epoch 285/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2802\n",
            "Epoch 286/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2802\n",
            "Epoch 287/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2802\n",
            "Epoch 288/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2802\n",
            "Epoch 289/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2801\n",
            "Epoch 290/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2803\n",
            "Epoch 291/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2803\n",
            "Epoch 292/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2803\n",
            "Epoch 293/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2803\n",
            "Epoch 294/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2804\n",
            "Epoch 295/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2804\n",
            "Epoch 296/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2804\n",
            "Epoch 297/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2803\n",
            "Epoch 298/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2806\n",
            "Epoch 299/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2805\n",
            "Epoch 300/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2805\n",
            "Epoch 301/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2806\n",
            "Epoch 302/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2806\n",
            "Epoch 303/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2806\n",
            "Epoch 304/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2806\n",
            "Epoch 305/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2805\n",
            "Epoch 306/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2806\n",
            "Epoch 307/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2806\n",
            "Epoch 308/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2806\n",
            "Epoch 309/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2806\n",
            "Epoch 310/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2808\n",
            "Epoch 311/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2807\n",
            "Epoch 312/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2808\n",
            "Epoch 313/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2808\n",
            "Epoch 314/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2808\n",
            "Epoch 315/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2807\n",
            "Epoch 316/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2809\n",
            "Epoch 317/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2809\n",
            "Epoch 318/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2809\n",
            "Epoch 319/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2807\n",
            "Epoch 320/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2808\n",
            "Epoch 321/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2808\n",
            "Epoch 322/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2809\n",
            "Epoch 323/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2810\n",
            "Epoch 324/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2809\n",
            "Epoch 325/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2809\n",
            "Epoch 326/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2810\n",
            "Epoch 327/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2810\n",
            "Epoch 328/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2810\n",
            "Epoch 329/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2810\n",
            "Epoch 330/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2810\n",
            "Epoch 331/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2810\n",
            "Epoch 332/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2812\n",
            "Epoch 333/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2811\n",
            "Epoch 334/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2811\n",
            "Epoch 335/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2811\n",
            "Epoch 336/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2811\n",
            "Epoch 337/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2812\n",
            "Epoch 338/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2811\n",
            "Epoch 339/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2813\n",
            "Epoch 340/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2812\n",
            "Epoch 341/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2812\n",
            "Epoch 342/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2813\n",
            "Epoch 343/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2813\n",
            "Epoch 344/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2813\n",
            "Epoch 345/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2813\n",
            "Epoch 346/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2814\n",
            "Epoch 347/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2814\n",
            "Epoch 348/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2815\n",
            "Epoch 349/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2815\n",
            "Epoch 350/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2814\n",
            "Epoch 351/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2815\n",
            "Epoch 352/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2814\n",
            "Epoch 353/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2817\n",
            "Epoch 354/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2815\n",
            "Epoch 355/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2816\n",
            "Epoch 356/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2815\n",
            "Epoch 357/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2816\n",
            "Epoch 358/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2816\n",
            "Epoch 359/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2817\n",
            "Epoch 360/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2815\n",
            "Epoch 361/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2815\n",
            "Epoch 362/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2816\n",
            "Epoch 363/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2819\n",
            "Epoch 364/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2817\n",
            "Epoch 365/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2817\n",
            "Epoch 366/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2817\n",
            "Epoch 367/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2818\n",
            "Epoch 368/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2817\n",
            "Epoch 369/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2817\n",
            "Epoch 370/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2818\n",
            "Epoch 371/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2818\n",
            "Epoch 372/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2818\n",
            "Epoch 373/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2818\n",
            "Epoch 374/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2819\n",
            "Epoch 375/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2820\n",
            "Epoch 376/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2819\n",
            "Epoch 377/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2818\n",
            "Epoch 378/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2819\n",
            "Epoch 379/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2819\n",
            "Epoch 380/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2819\n",
            "Epoch 381/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2819\n",
            "Epoch 382/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2819\n",
            "Epoch 383/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2820\n",
            "Epoch 384/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2821\n",
            "Epoch 385/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2821\n",
            "Epoch 386/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2819\n",
            "Epoch 387/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2820\n",
            "Epoch 388/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2820\n",
            "Epoch 389/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2821\n",
            "Epoch 390/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2821\n",
            "Epoch 391/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2821\n",
            "Epoch 392/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2822\n",
            "Epoch 393/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2821\n",
            "Epoch 394/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2822\n",
            "Epoch 395/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2822\n",
            "Epoch 396/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2823\n",
            "Epoch 397/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2822\n",
            "Epoch 398/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2822\n",
            "Epoch 399/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2822\n",
            "Epoch 400/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2821\n",
            "Epoch 401/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2822\n",
            "Epoch 402/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2823\n",
            "Epoch 403/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2824\n",
            "Epoch 404/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2823\n",
            "Epoch 405/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2823\n",
            "Epoch 406/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2822\n",
            "Epoch 407/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2824\n",
            "Epoch 408/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2824\n",
            "Epoch 409/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2824\n",
            "Epoch 410/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2823\n",
            "Epoch 411/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2824\n",
            "Epoch 412/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2824\n",
            "Epoch 413/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2823\n",
            "Epoch 414/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2824\n",
            "Epoch 415/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2824\n",
            "Epoch 416/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2826\n",
            "Epoch 417/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2824\n",
            "Epoch 418/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2825\n",
            "Epoch 419/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2825\n",
            "Epoch 420/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2825\n",
            "Epoch 421/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2825\n",
            "Epoch 422/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2825\n",
            "Epoch 423/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2824\n",
            "Epoch 424/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2827\n",
            "Epoch 425/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2825\n",
            "Epoch 426/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2826\n",
            "Epoch 427/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2826\n",
            "Epoch 428/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2826\n",
            "Epoch 429/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2827\n",
            "Epoch 430/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2825\n",
            "Epoch 431/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2826\n",
            "Epoch 432/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2827\n",
            "Epoch 433/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2826\n",
            "Epoch 434/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2827\n",
            "Epoch 435/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2828\n",
            "Epoch 436/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2826\n",
            "Epoch 437/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2828\n",
            "Epoch 438/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2827\n",
            "Epoch 439/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2828\n",
            "Epoch 440/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2828\n",
            "Epoch 441/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2828\n",
            "Epoch 442/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2828\n",
            "Epoch 443/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2828\n",
            "Epoch 444/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2829\n",
            "Epoch 445/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2829\n",
            "Epoch 446/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2828\n",
            "Epoch 447/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2828\n",
            "Epoch 448/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2829\n",
            "Epoch 449/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2829\n",
            "Epoch 450/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2828\n",
            "Epoch 451/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2829\n",
            "Epoch 452/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2829\n",
            "Epoch 453/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2828\n",
            "Epoch 454/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2829\n",
            "Epoch 455/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2831\n",
            "Epoch 456/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2830\n",
            "Epoch 457/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2829\n",
            "Epoch 458/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2831\n",
            "Epoch 459/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2829\n",
            "Epoch 460/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2830\n",
            "Epoch 461/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2830\n",
            "Epoch 462/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2831\n",
            "Epoch 463/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2829\n",
            "Epoch 464/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2831\n",
            "Epoch 465/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2831\n",
            "Epoch 466/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2831\n",
            "Epoch 467/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2831\n",
            "Epoch 468/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2830\n",
            "Epoch 469/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2832\n",
            "Epoch 470/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2833\n",
            "Epoch 471/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1625 - val_loss: 0.2833\n",
            "Epoch 472/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2831\n",
            "Epoch 473/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1631 - val_loss: 0.2830\n",
            "Epoch 474/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2833\n",
            "Epoch 475/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2832\n",
            "Epoch 476/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2832\n",
            "Epoch 477/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2833\n",
            "Epoch 478/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2832\n",
            "Epoch 479/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2833\n",
            "Epoch 480/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2833\n",
            "Epoch 481/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2832\n",
            "Epoch 482/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2833\n",
            "Epoch 483/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2833\n",
            "Epoch 484/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2834\n",
            "Epoch 485/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2833\n",
            "Epoch 486/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2833\n",
            "Epoch 487/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2835\n",
            "Epoch 488/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2833\n",
            "Epoch 489/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2834\n",
            "Epoch 490/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2834\n",
            "Epoch 491/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2834\n",
            "Epoch 492/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2836\n",
            "Epoch 493/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2834\n",
            "Epoch 494/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2834\n",
            "Epoch 495/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2835\n",
            "Epoch 496/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2834\n",
            "Epoch 497/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1629 - val_loss: 0.2835\n",
            "Epoch 498/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1626 - val_loss: 0.2834\n",
            "Epoch 499/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1627 - val_loss: 0.2836\n",
            "Epoch 500/500\n",
            "530/530 [==============================] - 3s 5ms/step - loss: 0.1630 - val_loss: 0.2835\n",
            "37/37 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (0.1, 500, 64, 0.16297492384910583, 0.2835470139980316, 0.025642982, 4.000544717310868, 0.18865723393176592)\n",
            "Epoch 1/500\n",
            "532/532 [==============================] - 4s 6ms/step - loss: 0.3767 - val_loss: 0.3426\n",
            "Epoch 2/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2946 - val_loss: 0.3072\n",
            "Epoch 3/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2748 - val_loss: 0.2938\n",
            "Epoch 4/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2674 - val_loss: 0.2887\n",
            "Epoch 5/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2640 - val_loss: 0.2864\n",
            "Epoch 6/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2624 - val_loss: 0.2851\n",
            "Epoch 7/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2608 - val_loss: 0.2845\n",
            "Epoch 8/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2599 - val_loss: 0.2835\n",
            "Epoch 9/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2580 - val_loss: 0.2827\n",
            "Epoch 10/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2564 - val_loss: 0.2822\n",
            "Epoch 11/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2540 - val_loss: 0.2812\n",
            "Epoch 12/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2522 - val_loss: 0.2804\n",
            "Epoch 13/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2499 - val_loss: 0.2796\n",
            "Epoch 14/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2478 - val_loss: 0.2790\n",
            "Epoch 15/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2455 - val_loss: 0.2784\n",
            "Epoch 16/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2428 - val_loss: 0.2776\n",
            "Epoch 17/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2410 - val_loss: 0.2769\n",
            "Epoch 18/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2388 - val_loss: 0.2764\n",
            "Epoch 19/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2366 - val_loss: 0.2759\n",
            "Epoch 20/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2346 - val_loss: 0.2753\n",
            "Epoch 21/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2324 - val_loss: 0.2749\n",
            "Epoch 22/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2304 - val_loss: 0.2745\n",
            "Epoch 23/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2285 - val_loss: 0.2739\n",
            "Epoch 24/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2270 - val_loss: 0.2738\n",
            "Epoch 25/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2251 - val_loss: 0.2733\n",
            "Epoch 26/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2231 - val_loss: 0.2731\n",
            "Epoch 27/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2214 - val_loss: 0.2729\n",
            "Epoch 28/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2206 - val_loss: 0.2725\n",
            "Epoch 29/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2188 - val_loss: 0.2723\n",
            "Epoch 30/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2175 - val_loss: 0.2720\n",
            "Epoch 31/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2162 - val_loss: 0.2720\n",
            "Epoch 32/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2145 - val_loss: 0.2718\n",
            "Epoch 33/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2131 - val_loss: 0.2716\n",
            "Epoch 34/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2119 - val_loss: 0.2715\n",
            "Epoch 35/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2107 - val_loss: 0.2713\n",
            "Epoch 36/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2096 - val_loss: 0.2717\n",
            "Epoch 37/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2088 - val_loss: 0.2711\n",
            "Epoch 38/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2075 - val_loss: 0.2712\n",
            "Epoch 39/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2063 - val_loss: 0.2713\n",
            "Epoch 40/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2057 - val_loss: 0.2712\n",
            "Epoch 41/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2045 - val_loss: 0.2711\n",
            "Epoch 42/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2040 - val_loss: 0.2710\n",
            "Epoch 43/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2026 - val_loss: 0.2710\n",
            "Epoch 44/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2017 - val_loss: 0.2710\n",
            "Epoch 45/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.2007 - val_loss: 0.2711\n",
            "Epoch 46/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1999 - val_loss: 0.2710\n",
            "Epoch 47/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1995 - val_loss: 0.2712\n",
            "Epoch 48/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1984 - val_loss: 0.2711\n",
            "Epoch 49/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1982 - val_loss: 0.2711\n",
            "Epoch 50/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1976 - val_loss: 0.2711\n",
            "Epoch 51/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1966 - val_loss: 0.2711\n",
            "Epoch 52/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1961 - val_loss: 0.2713\n",
            "Epoch 53/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1950 - val_loss: 0.2716\n",
            "Epoch 54/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1945 - val_loss: 0.2714\n",
            "Epoch 55/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1944 - val_loss: 0.2712\n",
            "Epoch 56/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1935 - val_loss: 0.2713\n",
            "Epoch 57/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1925 - val_loss: 0.2714\n",
            "Epoch 58/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1924 - val_loss: 0.2715\n",
            "Epoch 59/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1919 - val_loss: 0.2715\n",
            "Epoch 60/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1916 - val_loss: 0.2716\n",
            "Epoch 61/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1911 - val_loss: 0.2716\n",
            "Epoch 62/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1902 - val_loss: 0.2716\n",
            "Epoch 63/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1899 - val_loss: 0.2717\n",
            "Epoch 64/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1897 - val_loss: 0.2717\n",
            "Epoch 65/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1890 - val_loss: 0.2719\n",
            "Epoch 66/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1886 - val_loss: 0.2721\n",
            "Epoch 67/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1880 - val_loss: 0.2720\n",
            "Epoch 68/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1882 - val_loss: 0.2721\n",
            "Epoch 69/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1875 - val_loss: 0.2724\n",
            "Epoch 70/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1871 - val_loss: 0.2723\n",
            "Epoch 71/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1866 - val_loss: 0.2722\n",
            "Epoch 72/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1859 - val_loss: 0.2724\n",
            "Epoch 73/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1858 - val_loss: 0.2724\n",
            "Epoch 74/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1853 - val_loss: 0.2723\n",
            "Epoch 75/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1857 - val_loss: 0.2725\n",
            "Epoch 76/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1848 - val_loss: 0.2725\n",
            "Epoch 77/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1842 - val_loss: 0.2726\n",
            "Epoch 78/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1846 - val_loss: 0.2727\n",
            "Epoch 79/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1837 - val_loss: 0.2729\n",
            "Epoch 80/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1834 - val_loss: 0.2728\n",
            "Epoch 81/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1831 - val_loss: 0.2729\n",
            "Epoch 82/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1830 - val_loss: 0.2731\n",
            "Epoch 83/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1828 - val_loss: 0.2731\n",
            "Epoch 84/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1824 - val_loss: 0.2733\n",
            "Epoch 85/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1826 - val_loss: 0.2732\n",
            "Epoch 86/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1820 - val_loss: 0.2736\n",
            "Epoch 87/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1818 - val_loss: 0.2733\n",
            "Epoch 88/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1815 - val_loss: 0.2734\n",
            "Epoch 89/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1813 - val_loss: 0.2735\n",
            "Epoch 90/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1809 - val_loss: 0.2735\n",
            "Epoch 91/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1808 - val_loss: 0.2737\n",
            "Epoch 92/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1807 - val_loss: 0.2737\n",
            "Epoch 93/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1801 - val_loss: 0.2738\n",
            "Epoch 94/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2737\n",
            "Epoch 95/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2739\n",
            "Epoch 96/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1796 - val_loss: 0.2740\n",
            "Epoch 97/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2739\n",
            "Epoch 98/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1793 - val_loss: 0.2742\n",
            "Epoch 99/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1787 - val_loss: 0.2742\n",
            "Epoch 100/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1791 - val_loss: 0.2744\n",
            "Epoch 101/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2745\n",
            "Epoch 102/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1786 - val_loss: 0.2744\n",
            "Epoch 103/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1780 - val_loss: 0.2743\n",
            "Epoch 104/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1779 - val_loss: 0.2746\n",
            "Epoch 105/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1778 - val_loss: 0.2746\n",
            "Epoch 106/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1779 - val_loss: 0.2746\n",
            "Epoch 107/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1773 - val_loss: 0.2747\n",
            "Epoch 108/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.2748\n",
            "Epoch 109/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2749\n",
            "Epoch 110/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1768 - val_loss: 0.2748\n",
            "Epoch 111/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2749\n",
            "Epoch 112/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2752\n",
            "Epoch 113/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1767 - val_loss: 0.2752\n",
            "Epoch 114/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2751\n",
            "Epoch 115/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2752\n",
            "Epoch 116/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1759 - val_loss: 0.2753\n",
            "Epoch 117/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2752\n",
            "Epoch 118/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2754\n",
            "Epoch 119/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1754 - val_loss: 0.2754\n",
            "Epoch 120/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1756 - val_loss: 0.2755\n",
            "Epoch 121/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1756 - val_loss: 0.2756\n",
            "Epoch 122/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1756 - val_loss: 0.2756\n",
            "Epoch 123/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1753 - val_loss: 0.2758\n",
            "Epoch 124/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1751 - val_loss: 0.2758\n",
            "Epoch 125/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2757\n",
            "Epoch 126/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2758\n",
            "Epoch 127/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1745 - val_loss: 0.2759\n",
            "Epoch 128/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1748 - val_loss: 0.2758\n",
            "Epoch 129/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1748 - val_loss: 0.2760\n",
            "Epoch 130/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2762\n",
            "Epoch 131/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2762\n",
            "Epoch 132/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2761\n",
            "Epoch 133/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1743 - val_loss: 0.2762\n",
            "Epoch 134/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2762\n",
            "Epoch 135/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2763\n",
            "Epoch 136/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1734 - val_loss: 0.2764\n",
            "Epoch 137/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2763\n",
            "Epoch 138/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2765\n",
            "Epoch 139/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2765\n",
            "Epoch 140/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1736 - val_loss: 0.2765\n",
            "Epoch 141/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1732 - val_loss: 0.2767\n",
            "Epoch 142/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2767\n",
            "Epoch 143/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2767\n",
            "Epoch 144/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1730 - val_loss: 0.2768\n",
            "Epoch 145/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2768\n",
            "Epoch 146/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2768\n",
            "Epoch 147/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2769\n",
            "Epoch 148/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2770\n",
            "Epoch 149/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.2771\n",
            "Epoch 150/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2769\n",
            "Epoch 151/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2771\n",
            "Epoch 152/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2771\n",
            "Epoch 153/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2772\n",
            "Epoch 154/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1718 - val_loss: 0.2772\n",
            "Epoch 155/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2773\n",
            "Epoch 156/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2772\n",
            "Epoch 157/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2773\n",
            "Epoch 158/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2774\n",
            "Epoch 159/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1720 - val_loss: 0.2775\n",
            "Epoch 160/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2775\n",
            "Epoch 161/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2776\n",
            "Epoch 162/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1713 - val_loss: 0.2776\n",
            "Epoch 163/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1718 - val_loss: 0.2775\n",
            "Epoch 164/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2776\n",
            "Epoch 165/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2777\n",
            "Epoch 166/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2777\n",
            "Epoch 167/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2779\n",
            "Epoch 168/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2778\n",
            "Epoch 169/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2778\n",
            "Epoch 170/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2780\n",
            "Epoch 171/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2780\n",
            "Epoch 172/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2780\n",
            "Epoch 173/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2781\n",
            "Epoch 174/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2780\n",
            "Epoch 175/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2782\n",
            "Epoch 176/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2783\n",
            "Epoch 177/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2781\n",
            "Epoch 178/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2783\n",
            "Epoch 179/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2783\n",
            "Epoch 180/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2782\n",
            "Epoch 181/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1705 - val_loss: 0.2783\n",
            "Epoch 182/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2784\n",
            "Epoch 183/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2784\n",
            "Epoch 184/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2784\n",
            "Epoch 185/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2786\n",
            "Epoch 186/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2786\n",
            "Epoch 187/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2786\n",
            "Epoch 188/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2785\n",
            "Epoch 189/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2787\n",
            "Epoch 190/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2787\n",
            "Epoch 191/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2787\n",
            "Epoch 192/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2787\n",
            "Epoch 193/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2788\n",
            "Epoch 194/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2789\n",
            "Epoch 195/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1699 - val_loss: 0.2789\n",
            "Epoch 196/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2789\n",
            "Epoch 197/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2789\n",
            "Epoch 198/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2790\n",
            "Epoch 199/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2792\n",
            "Epoch 200/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2790\n",
            "Epoch 201/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2792\n",
            "Epoch 202/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2792\n",
            "Epoch 203/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2792\n",
            "Epoch 204/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2792\n",
            "Epoch 205/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2793\n",
            "Epoch 206/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2792\n",
            "Epoch 207/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2793\n",
            "Epoch 208/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2793\n",
            "Epoch 209/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2794\n",
            "Epoch 210/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2794\n",
            "Epoch 211/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2795\n",
            "Epoch 212/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2794\n",
            "Epoch 213/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2794\n",
            "Epoch 214/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2796\n",
            "Epoch 215/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2795\n",
            "Epoch 216/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1687 - val_loss: 0.2796\n",
            "Epoch 217/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2795\n",
            "Epoch 218/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2796\n",
            "Epoch 219/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2796\n",
            "Epoch 220/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2797\n",
            "Epoch 221/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2797\n",
            "Epoch 222/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2798\n",
            "Epoch 223/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2798\n",
            "Epoch 224/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2797\n",
            "Epoch 225/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2799\n",
            "Epoch 226/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2798\n",
            "Epoch 227/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2799\n",
            "Epoch 228/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2799\n",
            "Epoch 229/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2800\n",
            "Epoch 230/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2799\n",
            "Epoch 231/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2800\n",
            "Epoch 232/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2800\n",
            "Epoch 233/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2801\n",
            "Epoch 234/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2801\n",
            "Epoch 235/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2801\n",
            "Epoch 236/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2802\n",
            "Epoch 237/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2802\n",
            "Epoch 238/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2802\n",
            "Epoch 239/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2802\n",
            "Epoch 240/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2802\n",
            "Epoch 241/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2802\n",
            "Epoch 242/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2804\n",
            "Epoch 243/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2804\n",
            "Epoch 244/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2803\n",
            "Epoch 245/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2804\n",
            "Epoch 246/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2804\n",
            "Epoch 247/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2804\n",
            "Epoch 248/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2804\n",
            "Epoch 249/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2805\n",
            "Epoch 250/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2805\n",
            "Epoch 251/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2806\n",
            "Epoch 252/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2806\n",
            "Epoch 253/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2806\n",
            "Epoch 254/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2806\n",
            "Epoch 255/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2807\n",
            "Epoch 256/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2808\n",
            "Epoch 257/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2807\n",
            "Epoch 258/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2808\n",
            "Epoch 259/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2808\n",
            "Epoch 260/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2809\n",
            "Epoch 261/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2808\n",
            "Epoch 262/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2807\n",
            "Epoch 263/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2809\n",
            "Epoch 264/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2809\n",
            "Epoch 265/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2809\n",
            "Epoch 266/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2809\n",
            "Epoch 267/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2809\n",
            "Epoch 268/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2809\n",
            "Epoch 269/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2809\n",
            "Epoch 270/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2811\n",
            "Epoch 271/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2811\n",
            "Epoch 272/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2810\n",
            "Epoch 273/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2810\n",
            "Epoch 274/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2811\n",
            "Epoch 275/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2810\n",
            "Epoch 276/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2811\n",
            "Epoch 277/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2812\n",
            "Epoch 278/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2812\n",
            "Epoch 279/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1668 - val_loss: 0.2812\n",
            "Epoch 280/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2814\n",
            "Epoch 281/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2813\n",
            "Epoch 282/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2814\n",
            "Epoch 283/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2815\n",
            "Epoch 284/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2813\n",
            "Epoch 285/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2813\n",
            "Epoch 286/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2814\n",
            "Epoch 287/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2814\n",
            "Epoch 288/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2813\n",
            "Epoch 289/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2814\n",
            "Epoch 290/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2814\n",
            "Epoch 291/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2815\n",
            "Epoch 292/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2816\n",
            "Epoch 293/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2815\n",
            "Epoch 294/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2815\n",
            "Epoch 295/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2816\n",
            "Epoch 296/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2815\n",
            "Epoch 297/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2816\n",
            "Epoch 298/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2817\n",
            "Epoch 299/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2816\n",
            "Epoch 300/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2816\n",
            "Epoch 301/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2817\n",
            "Epoch 302/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2817\n",
            "Epoch 303/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2817\n",
            "Epoch 304/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2817\n",
            "Epoch 305/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2817\n",
            "Epoch 306/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2817\n",
            "Epoch 307/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2817\n",
            "Epoch 308/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2819\n",
            "Epoch 309/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2818\n",
            "Epoch 310/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2818\n",
            "Epoch 311/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2819\n",
            "Epoch 312/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2819\n",
            "Epoch 313/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2819\n",
            "Epoch 314/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2819\n",
            "Epoch 315/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2819\n",
            "Epoch 316/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2821\n",
            "Epoch 317/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2820\n",
            "Epoch 318/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2821\n",
            "Epoch 319/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2821\n",
            "Epoch 320/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1658 - val_loss: 0.2821\n",
            "Epoch 321/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2821\n",
            "Epoch 322/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2821\n",
            "Epoch 323/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2821\n",
            "Epoch 324/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2821\n",
            "Epoch 325/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2821\n",
            "Epoch 326/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2822\n",
            "Epoch 327/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2823\n",
            "Epoch 328/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2822\n",
            "Epoch 329/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2823\n",
            "Epoch 330/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2822\n",
            "Epoch 331/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2823\n",
            "Epoch 332/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2822\n",
            "Epoch 333/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2822\n",
            "Epoch 334/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2823\n",
            "Epoch 335/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2823\n",
            "Epoch 336/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2823\n",
            "Epoch 337/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2823\n",
            "Epoch 338/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2824\n",
            "Epoch 339/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2827\n",
            "Epoch 340/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2824\n",
            "Epoch 341/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2824\n",
            "Epoch 342/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2824\n",
            "Epoch 343/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2824\n",
            "Epoch 344/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2825\n",
            "Epoch 345/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2827\n",
            "Epoch 346/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2826\n",
            "Epoch 347/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2826\n",
            "Epoch 348/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2824\n",
            "Epoch 349/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2826\n",
            "Epoch 350/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2825\n",
            "Epoch 351/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2826\n",
            "Epoch 352/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2827\n",
            "Epoch 353/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2826\n",
            "Epoch 354/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2827\n",
            "Epoch 355/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2827\n",
            "Epoch 356/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2827\n",
            "Epoch 357/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1647 - val_loss: 0.2828\n",
            "Epoch 358/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2828\n",
            "Epoch 359/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2827\n",
            "Epoch 360/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2829\n",
            "Epoch 361/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2828\n",
            "Epoch 362/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2827\n",
            "Epoch 363/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1651 - val_loss: 0.2828\n",
            "Epoch 364/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2827\n",
            "Epoch 365/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2828\n",
            "Epoch 366/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2830\n",
            "Epoch 367/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2829\n",
            "Epoch 368/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2829\n",
            "Epoch 369/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2828\n",
            "Epoch 370/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2829\n",
            "Epoch 371/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2829\n",
            "Epoch 372/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2831\n",
            "Epoch 373/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2831\n",
            "Epoch 374/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2830\n",
            "Epoch 375/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2830\n",
            "Epoch 376/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2830\n",
            "Epoch 377/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2830\n",
            "Epoch 378/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2831\n",
            "Epoch 379/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2831\n",
            "Epoch 380/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2831\n",
            "Epoch 381/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2831\n",
            "Epoch 382/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2832\n",
            "Epoch 383/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2833\n",
            "Epoch 384/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2830\n",
            "Epoch 385/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2832\n",
            "Epoch 386/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2832\n",
            "Epoch 387/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2831\n",
            "Epoch 388/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2832\n",
            "Epoch 389/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2831\n",
            "Epoch 390/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2832\n",
            "Epoch 391/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2833\n",
            "Epoch 392/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2833\n",
            "Epoch 393/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2832\n",
            "Epoch 394/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2833\n",
            "Epoch 395/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2832\n",
            "Epoch 396/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2833\n",
            "Epoch 397/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2833\n",
            "Epoch 398/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2833\n",
            "Epoch 399/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2834\n",
            "Epoch 400/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2834\n",
            "Epoch 401/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2834\n",
            "Epoch 402/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2834\n",
            "Epoch 403/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2834\n",
            "Epoch 404/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2836\n",
            "Epoch 405/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2836\n",
            "Epoch 406/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2836\n",
            "Epoch 407/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2835\n",
            "Epoch 408/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2834\n",
            "Epoch 409/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2835\n",
            "Epoch 410/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2835\n",
            "Epoch 411/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1641 - val_loss: 0.2836\n",
            "Epoch 412/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2837\n",
            "Epoch 413/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2837\n",
            "Epoch 414/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2836\n",
            "Epoch 415/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1644 - val_loss: 0.2836\n",
            "Epoch 416/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2835\n",
            "Epoch 417/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2836\n",
            "Epoch 418/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2837\n",
            "Epoch 419/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2837\n",
            "Epoch 420/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1640 - val_loss: 0.2836\n",
            "Epoch 421/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2838\n",
            "Epoch 422/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2837\n",
            "Epoch 423/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2838\n",
            "Epoch 424/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2836\n",
            "Epoch 425/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1640 - val_loss: 0.2837\n",
            "Epoch 426/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2836\n",
            "Epoch 427/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2838\n",
            "Epoch 428/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2839\n",
            "Epoch 429/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1646 - val_loss: 0.2838\n",
            "Epoch 430/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1640 - val_loss: 0.2838\n",
            "Epoch 431/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2839\n",
            "Epoch 432/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2839\n",
            "Epoch 433/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1641 - val_loss: 0.2838\n",
            "Epoch 434/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1640 - val_loss: 0.2838\n",
            "Epoch 435/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2838\n",
            "Epoch 436/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2838\n",
            "Epoch 437/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2840\n",
            "Epoch 438/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1638 - val_loss: 0.2839\n",
            "Epoch 439/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1642 - val_loss: 0.2840\n",
            "Epoch 440/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2839\n",
            "Epoch 441/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2839\n",
            "Epoch 442/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2840\n",
            "Epoch 443/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1640 - val_loss: 0.2839\n",
            "Epoch 444/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2840\n",
            "Epoch 445/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2839\n",
            "Epoch 446/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2840\n",
            "Epoch 447/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2841\n",
            "Epoch 448/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2840\n",
            "Epoch 449/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2841\n",
            "Epoch 450/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2842\n",
            "Epoch 451/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2841\n",
            "Epoch 452/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1641 - val_loss: 0.2840\n",
            "Epoch 453/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2841\n",
            "Epoch 454/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2841\n",
            "Epoch 455/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2840\n",
            "Epoch 456/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2842\n",
            "Epoch 457/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1632 - val_loss: 0.2842\n",
            "Epoch 458/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2842\n",
            "Epoch 459/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2842\n",
            "Epoch 460/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2842\n",
            "Epoch 461/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1636 - val_loss: 0.2842\n",
            "Epoch 462/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2842\n",
            "Epoch 463/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2842\n",
            "Epoch 464/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2843\n",
            "Epoch 465/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2843\n",
            "Epoch 466/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1639 - val_loss: 0.2842\n",
            "Epoch 467/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2842\n",
            "Epoch 468/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2842\n",
            "Epoch 469/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2842\n",
            "Epoch 470/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1633 - val_loss: 0.2843\n",
            "Epoch 471/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1642 - val_loss: 0.2842\n",
            "Epoch 472/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2844\n",
            "Epoch 473/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2843\n",
            "Epoch 474/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2844\n",
            "Epoch 475/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2844\n",
            "Epoch 476/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2844\n",
            "Epoch 477/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2843\n",
            "Epoch 478/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2844\n",
            "Epoch 479/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2844\n",
            "Epoch 480/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2844\n",
            "Epoch 481/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2845\n",
            "Epoch 482/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2845\n",
            "Epoch 483/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2845\n",
            "Epoch 484/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2845\n",
            "Epoch 485/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2844\n",
            "Epoch 486/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2845\n",
            "Epoch 487/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2845\n",
            "Epoch 488/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2844\n",
            "Epoch 489/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2845\n",
            "Epoch 490/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2845\n",
            "Epoch 491/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2846\n",
            "Epoch 492/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2847\n",
            "Epoch 493/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2844\n",
            "Epoch 494/500\n",
            "532/532 [==============================] - 3s 6ms/step - loss: 0.1631 - val_loss: 0.2846\n",
            "Epoch 495/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1632 - val_loss: 0.2846\n",
            "Epoch 496/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.2846\n",
            "Epoch 497/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2846\n",
            "Epoch 498/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2848\n",
            "Epoch 499/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2846\n",
            "Epoch 500/500\n",
            "532/532 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2846\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 (0.1, 500, 64, 0.16339947283267975, 0.284610778093338, 0.029971626, 3.9892428451991515, 0.23966804298768216)\n",
            "Epoch 1/500\n",
            "531/531 [==============================] - 4s 5ms/step - loss: 0.3766 - val_loss: 0.3451\n",
            "Epoch 2/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2950 - val_loss: 0.3083\n",
            "Epoch 3/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2752 - val_loss: 0.2943\n",
            "Epoch 4/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2674 - val_loss: 0.2889\n",
            "Epoch 5/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2642 - val_loss: 0.2868\n",
            "Epoch 6/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2625 - val_loss: 0.2855\n",
            "Epoch 7/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2611 - val_loss: 0.2844\n",
            "Epoch 8/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2598 - val_loss: 0.2836\n",
            "Epoch 9/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2577 - val_loss: 0.2827\n",
            "Epoch 10/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2562 - val_loss: 0.2820\n",
            "Epoch 11/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2542 - val_loss: 0.2814\n",
            "Epoch 12/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2519 - val_loss: 0.2805\n",
            "Epoch 13/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2496 - val_loss: 0.2798\n",
            "Epoch 14/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2475 - val_loss: 0.2790\n",
            "Epoch 15/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2454 - val_loss: 0.2782\n",
            "Epoch 16/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2432 - val_loss: 0.2778\n",
            "Epoch 17/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2405 - val_loss: 0.2768\n",
            "Epoch 18/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2384 - val_loss: 0.2761\n",
            "Epoch 19/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2364 - val_loss: 0.2756\n",
            "Epoch 20/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2342 - val_loss: 0.2749\n",
            "Epoch 21/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2320 - val_loss: 0.2745\n",
            "Epoch 22/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2300 - val_loss: 0.2740\n",
            "Epoch 23/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2284 - val_loss: 0.2736\n",
            "Epoch 24/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2268 - val_loss: 0.2732\n",
            "Epoch 25/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2247 - val_loss: 0.2728\n",
            "Epoch 26/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2233 - val_loss: 0.2725\n",
            "Epoch 27/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2215 - val_loss: 0.2723\n",
            "Epoch 28/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2202 - val_loss: 0.2719\n",
            "Epoch 29/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2185 - val_loss: 0.2718\n",
            "Epoch 30/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2170 - val_loss: 0.2715\n",
            "Epoch 31/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2159 - val_loss: 0.2713\n",
            "Epoch 32/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2147 - val_loss: 0.2712\n",
            "Epoch 33/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2132 - val_loss: 0.2711\n",
            "Epoch 34/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2122 - val_loss: 0.2709\n",
            "Epoch 35/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2109 - val_loss: 0.2709\n",
            "Epoch 36/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2098 - val_loss: 0.2709\n",
            "Epoch 37/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2088 - val_loss: 0.2707\n",
            "Epoch 38/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2074 - val_loss: 0.2706\n",
            "Epoch 39/500\n",
            "531/531 [==============================] - 3s 6ms/step - loss: 0.2067 - val_loss: 0.2705\n",
            "Epoch 40/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2056 - val_loss: 0.2707\n",
            "Epoch 41/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2045 - val_loss: 0.2704\n",
            "Epoch 42/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2037 - val_loss: 0.2705\n",
            "Epoch 43/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2026 - val_loss: 0.2705\n",
            "Epoch 44/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2020 - val_loss: 0.2703\n",
            "Epoch 45/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2013 - val_loss: 0.2705\n",
            "Epoch 46/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.2005 - val_loss: 0.2705\n",
            "Epoch 47/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1996 - val_loss: 0.2703\n",
            "Epoch 48/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1989 - val_loss: 0.2703\n",
            "Epoch 49/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1980 - val_loss: 0.2704\n",
            "Epoch 50/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1974 - val_loss: 0.2705\n",
            "Epoch 51/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1970 - val_loss: 0.2704\n",
            "Epoch 52/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1963 - val_loss: 0.2706\n",
            "Epoch 53/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1952 - val_loss: 0.2707\n",
            "Epoch 54/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1949 - val_loss: 0.2706\n",
            "Epoch 55/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1942 - val_loss: 0.2709\n",
            "Epoch 56/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1936 - val_loss: 0.2707\n",
            "Epoch 57/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1932 - val_loss: 0.2710\n",
            "Epoch 58/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1925 - val_loss: 0.2709\n",
            "Epoch 59/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1918 - val_loss: 0.2708\n",
            "Epoch 60/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1915 - val_loss: 0.2710\n",
            "Epoch 61/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1910 - val_loss: 0.2711\n",
            "Epoch 62/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1905 - val_loss: 0.2711\n",
            "Epoch 63/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1899 - val_loss: 0.2712\n",
            "Epoch 64/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1900 - val_loss: 0.2712\n",
            "Epoch 65/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1892 - val_loss: 0.2712\n",
            "Epoch 66/500\n",
            "531/531 [==============================] - 3s 6ms/step - loss: 0.1887 - val_loss: 0.2713\n",
            "Epoch 67/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1885 - val_loss: 0.2714\n",
            "Epoch 68/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1878 - val_loss: 0.2713\n",
            "Epoch 69/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1876 - val_loss: 0.2715\n",
            "Epoch 70/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1871 - val_loss: 0.2714\n",
            "Epoch 71/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1870 - val_loss: 0.2716\n",
            "Epoch 72/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1866 - val_loss: 0.2717\n",
            "Epoch 73/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1862 - val_loss: 0.2723\n",
            "Epoch 74/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1859 - val_loss: 0.2717\n",
            "Epoch 75/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1851 - val_loss: 0.2719\n",
            "Epoch 76/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1852 - val_loss: 0.2717\n",
            "Epoch 77/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1848 - val_loss: 0.2721\n",
            "Epoch 78/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1842 - val_loss: 0.2720\n",
            "Epoch 79/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1842 - val_loss: 0.2721\n",
            "Epoch 80/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1844 - val_loss: 0.2722\n",
            "Epoch 81/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1834 - val_loss: 0.2722\n",
            "Epoch 82/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1834 - val_loss: 0.2722\n",
            "Epoch 83/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1829 - val_loss: 0.2723\n",
            "Epoch 84/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1823 - val_loss: 0.2724\n",
            "Epoch 85/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1823 - val_loss: 0.2725\n",
            "Epoch 86/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1820 - val_loss: 0.2726\n",
            "Epoch 87/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1816 - val_loss: 0.2726\n",
            "Epoch 88/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1816 - val_loss: 0.2727\n",
            "Epoch 89/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1809 - val_loss: 0.2730\n",
            "Epoch 90/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1812 - val_loss: 0.2731\n",
            "Epoch 91/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1809 - val_loss: 0.2728\n",
            "Epoch 92/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1805 - val_loss: 0.2731\n",
            "Epoch 93/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1805 - val_loss: 0.2730\n",
            "Epoch 94/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1804 - val_loss: 0.2731\n",
            "Epoch 95/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1797 - val_loss: 0.2731\n",
            "Epoch 96/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1795 - val_loss: 0.2732\n",
            "Epoch 97/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1794 - val_loss: 0.2732\n",
            "Epoch 98/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1798 - val_loss: 0.2735\n",
            "Epoch 99/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1788 - val_loss: 0.2734\n",
            "Epoch 100/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1786 - val_loss: 0.2734\n",
            "Epoch 101/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1787 - val_loss: 0.2734\n",
            "Epoch 102/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2735\n",
            "Epoch 103/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2736\n",
            "Epoch 104/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1784 - val_loss: 0.2736\n",
            "Epoch 105/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1780 - val_loss: 0.2736\n",
            "Epoch 106/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1780 - val_loss: 0.2738\n",
            "Epoch 107/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1778 - val_loss: 0.2738\n",
            "Epoch 108/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.2739\n",
            "Epoch 109/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1770 - val_loss: 0.2740\n",
            "Epoch 110/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1768 - val_loss: 0.2740\n",
            "Epoch 111/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.2741\n",
            "Epoch 112/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1770 - val_loss: 0.2740\n",
            "Epoch 113/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1765 - val_loss: 0.2742\n",
            "Epoch 114/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1768 - val_loss: 0.2743\n",
            "Epoch 115/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2744\n",
            "Epoch 116/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1762 - val_loss: 0.2743\n",
            "Epoch 117/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2744\n",
            "Epoch 118/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1756 - val_loss: 0.2745\n",
            "Epoch 119/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1758 - val_loss: 0.2746\n",
            "Epoch 120/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2747\n",
            "Epoch 121/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1759 - val_loss: 0.2747\n",
            "Epoch 122/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2747\n",
            "Epoch 123/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.2747\n",
            "Epoch 124/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1752 - val_loss: 0.2749\n",
            "Epoch 125/500\n",
            "531/531 [==============================] - 3s 6ms/step - loss: 0.1751 - val_loss: 0.2748\n",
            "Epoch 126/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1749 - val_loss: 0.2749\n",
            "Epoch 127/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2750\n",
            "Epoch 128/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2751\n",
            "Epoch 129/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.2750\n",
            "Epoch 130/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1748 - val_loss: 0.2752\n",
            "Epoch 131/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1746 - val_loss: 0.2751\n",
            "Epoch 132/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2751\n",
            "Epoch 133/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1742 - val_loss: 0.2754\n",
            "Epoch 134/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1741 - val_loss: 0.2753\n",
            "Epoch 135/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1741 - val_loss: 0.2753\n",
            "Epoch 136/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1739 - val_loss: 0.2754\n",
            "Epoch 137/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1744 - val_loss: 0.2753\n",
            "Epoch 138/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.2756\n",
            "Epoch 139/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1734 - val_loss: 0.2755\n",
            "Epoch 140/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1733 - val_loss: 0.2757\n",
            "Epoch 141/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1737 - val_loss: 0.2757\n",
            "Epoch 142/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1734 - val_loss: 0.2757\n",
            "Epoch 143/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.2758\n",
            "Epoch 144/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2759\n",
            "Epoch 145/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1729 - val_loss: 0.2759\n",
            "Epoch 146/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2759\n",
            "Epoch 147/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1728 - val_loss: 0.2760\n",
            "Epoch 148/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1727 - val_loss: 0.2760\n",
            "Epoch 149/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2762\n",
            "Epoch 150/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2762\n",
            "Epoch 151/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1722 - val_loss: 0.2762\n",
            "Epoch 152/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2762\n",
            "Epoch 153/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1726 - val_loss: 0.2763\n",
            "Epoch 154/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1723 - val_loss: 0.2763\n",
            "Epoch 155/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2764\n",
            "Epoch 156/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1721 - val_loss: 0.2764\n",
            "Epoch 157/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1722 - val_loss: 0.2764\n",
            "Epoch 158/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1715 - val_loss: 0.2765\n",
            "Epoch 159/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1718 - val_loss: 0.2766\n",
            "Epoch 160/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.2766\n",
            "Epoch 161/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1719 - val_loss: 0.2765\n",
            "Epoch 162/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2766\n",
            "Epoch 163/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2767\n",
            "Epoch 164/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1711 - val_loss: 0.2767\n",
            "Epoch 165/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2767\n",
            "Epoch 166/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1717 - val_loss: 0.2768\n",
            "Epoch 167/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2769\n",
            "Epoch 168/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2769\n",
            "Epoch 169/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2770\n",
            "Epoch 170/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1714 - val_loss: 0.2770\n",
            "Epoch 171/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.2769\n",
            "Epoch 172/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2770\n",
            "Epoch 173/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2771\n",
            "Epoch 174/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2771\n",
            "Epoch 175/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1710 - val_loss: 0.2772\n",
            "Epoch 176/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2773\n",
            "Epoch 177/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2772\n",
            "Epoch 178/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1707 - val_loss: 0.2773\n",
            "Epoch 179/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1708 - val_loss: 0.2773\n",
            "Epoch 180/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1706 - val_loss: 0.2774\n",
            "Epoch 181/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2774\n",
            "Epoch 182/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2774\n",
            "Epoch 183/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2774\n",
            "Epoch 184/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1701 - val_loss: 0.2775\n",
            "Epoch 185/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2776\n",
            "Epoch 186/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2778\n",
            "Epoch 187/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.2776\n",
            "Epoch 188/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1704 - val_loss: 0.2776\n",
            "Epoch 189/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2777\n",
            "Epoch 190/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.2777\n",
            "Epoch 191/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2778\n",
            "Epoch 192/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1698 - val_loss: 0.2778\n",
            "Epoch 193/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1696 - val_loss: 0.2779\n",
            "Epoch 194/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1700 - val_loss: 0.2780\n",
            "Epoch 195/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2779\n",
            "Epoch 196/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1697 - val_loss: 0.2780\n",
            "Epoch 197/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2780\n",
            "Epoch 198/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2779\n",
            "Epoch 199/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2780\n",
            "Epoch 200/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2781\n",
            "Epoch 201/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2780\n",
            "Epoch 202/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2781\n",
            "Epoch 203/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2782\n",
            "Epoch 204/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2782\n",
            "Epoch 205/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2782\n",
            "Epoch 206/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1694 - val_loss: 0.2783\n",
            "Epoch 207/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1692 - val_loss: 0.2783\n",
            "Epoch 208/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1693 - val_loss: 0.2783\n",
            "Epoch 209/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.2784\n",
            "Epoch 210/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2784\n",
            "Epoch 211/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2785\n",
            "Epoch 212/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.2786\n",
            "Epoch 213/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1690 - val_loss: 0.2786\n",
            "Epoch 214/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1691 - val_loss: 0.2785\n",
            "Epoch 215/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2785\n",
            "Epoch 216/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1689 - val_loss: 0.2788\n",
            "Epoch 217/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2785\n",
            "Epoch 218/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1688 - val_loss: 0.2786\n",
            "Epoch 219/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1687 - val_loss: 0.2787\n",
            "Epoch 220/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2786\n",
            "Epoch 221/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2788\n",
            "Epoch 222/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2787\n",
            "Epoch 223/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2788\n",
            "Epoch 224/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2788\n",
            "Epoch 225/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2787\n",
            "Epoch 226/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2791\n",
            "Epoch 227/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2789\n",
            "Epoch 228/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2788\n",
            "Epoch 229/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1682 - val_loss: 0.2790\n",
            "Epoch 230/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.2791\n",
            "Epoch 231/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2790\n",
            "Epoch 232/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1678 - val_loss: 0.2790\n",
            "Epoch 233/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2791\n",
            "Epoch 234/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1683 - val_loss: 0.2791\n",
            "Epoch 235/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1680 - val_loss: 0.2792\n",
            "Epoch 236/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1685 - val_loss: 0.2791\n",
            "Epoch 237/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2791\n",
            "Epoch 238/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2791\n",
            "Epoch 239/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2792\n",
            "Epoch 240/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1681 - val_loss: 0.2795\n",
            "Epoch 241/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2794\n",
            "Epoch 242/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1679 - val_loss: 0.2794\n",
            "Epoch 243/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.2793\n",
            "Epoch 244/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.2794\n",
            "Epoch 245/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2794\n",
            "Epoch 246/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2794\n",
            "Epoch 247/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2797\n",
            "Epoch 248/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2795\n",
            "Epoch 249/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2795\n",
            "Epoch 250/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2797\n",
            "Epoch 251/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2795\n",
            "Epoch 252/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2796\n",
            "Epoch 253/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2796\n",
            "Epoch 254/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1672 - val_loss: 0.2797\n",
            "Epoch 255/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2796\n",
            "Epoch 256/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2797\n",
            "Epoch 257/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1671 - val_loss: 0.2797\n",
            "Epoch 258/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2798\n",
            "Epoch 259/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1675 - val_loss: 0.2798\n",
            "Epoch 260/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2797\n",
            "Epoch 261/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2798\n",
            "Epoch 262/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1673 - val_loss: 0.2799\n",
            "Epoch 263/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2800\n",
            "Epoch 264/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2799\n",
            "Epoch 265/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2798\n",
            "Epoch 266/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2799\n",
            "Epoch 267/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2800\n",
            "Epoch 268/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2801\n",
            "Epoch 269/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2801\n",
            "Epoch 270/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2800\n",
            "Epoch 271/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2801\n",
            "Epoch 272/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2802\n",
            "Epoch 273/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2802\n",
            "Epoch 274/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2802\n",
            "Epoch 275/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1670 - val_loss: 0.2801\n",
            "Epoch 276/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2801\n",
            "Epoch 277/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2802\n",
            "Epoch 278/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.2801\n",
            "Epoch 279/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2801\n",
            "Epoch 280/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2803\n",
            "Epoch 281/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2805\n",
            "Epoch 282/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2804\n",
            "Epoch 283/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2803\n",
            "Epoch 284/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2803\n",
            "Epoch 285/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2804\n",
            "Epoch 286/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2805\n",
            "Epoch 287/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1667 - val_loss: 0.2804\n",
            "Epoch 288/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2804\n",
            "Epoch 289/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2805\n",
            "Epoch 290/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2804\n",
            "Epoch 291/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2805\n",
            "Epoch 292/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2808\n",
            "Epoch 293/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2806\n",
            "Epoch 294/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2806\n",
            "Epoch 295/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2806\n",
            "Epoch 296/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2805\n",
            "Epoch 297/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2806\n",
            "Epoch 298/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1665 - val_loss: 0.2806\n",
            "Epoch 299/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2808\n",
            "Epoch 300/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2806\n",
            "Epoch 301/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1666 - val_loss: 0.2808\n",
            "Epoch 302/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2808\n",
            "Epoch 303/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2807\n",
            "Epoch 304/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2811\n",
            "Epoch 305/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2807\n",
            "Epoch 306/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2809\n",
            "Epoch 307/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2809\n",
            "Epoch 308/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1663 - val_loss: 0.2809\n",
            "Epoch 309/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1664 - val_loss: 0.2808\n",
            "Epoch 310/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1658 - val_loss: 0.2809\n",
            "Epoch 311/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2808\n",
            "Epoch 312/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2808\n",
            "Epoch 313/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2810\n",
            "Epoch 314/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1661 - val_loss: 0.2809\n",
            "Epoch 315/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2811\n",
            "Epoch 316/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2811\n",
            "Epoch 317/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1662 - val_loss: 0.2811\n",
            "Epoch 318/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2811\n",
            "Epoch 319/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2811\n",
            "Epoch 320/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2811\n",
            "Epoch 321/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2813\n",
            "Epoch 322/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2811\n",
            "Epoch 323/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2811\n",
            "Epoch 324/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.2812\n",
            "Epoch 325/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2812\n",
            "Epoch 326/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2812\n",
            "Epoch 327/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2814\n",
            "Epoch 328/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2812\n",
            "Epoch 329/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2812\n",
            "Epoch 330/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2812\n",
            "Epoch 331/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2814\n",
            "Epoch 332/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2813\n",
            "Epoch 333/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2813\n",
            "Epoch 334/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2813\n",
            "Epoch 335/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2814\n",
            "Epoch 336/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1659 - val_loss: 0.2814\n",
            "Epoch 337/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2814\n",
            "Epoch 338/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2814\n",
            "Epoch 339/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2815\n",
            "Epoch 340/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2815\n",
            "Epoch 341/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2814\n",
            "Epoch 342/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1657 - val_loss: 0.2815\n",
            "Epoch 343/500\n",
            "531/531 [==============================] - 3s 6ms/step - loss: 0.1653 - val_loss: 0.2815\n",
            "Epoch 344/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2815\n",
            "Epoch 345/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1655 - val_loss: 0.2815\n",
            "Epoch 346/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2814\n",
            "Epoch 347/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2815\n",
            "Epoch 348/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2816\n",
            "Epoch 349/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2816\n",
            "Epoch 350/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2817\n",
            "Epoch 351/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2815\n",
            "Epoch 352/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2816\n",
            "Epoch 353/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2817\n",
            "Epoch 354/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2818\n",
            "Epoch 355/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2817\n",
            "Epoch 356/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2817\n",
            "Epoch 357/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2817\n",
            "Epoch 358/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1656 - val_loss: 0.2817\n",
            "Epoch 359/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2816\n",
            "Epoch 360/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2818\n",
            "Epoch 361/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2817\n",
            "Epoch 362/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1651 - val_loss: 0.2819\n",
            "Epoch 363/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1653 - val_loss: 0.2819\n",
            "Epoch 364/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1654 - val_loss: 0.2818\n",
            "Epoch 365/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2819\n",
            "Epoch 366/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2819\n",
            "Epoch 367/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.2819\n",
            "Epoch 368/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2818\n",
            "Epoch 369/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2818\n",
            "Epoch 370/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2819\n",
            "Epoch 371/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2820\n",
            "Epoch 372/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2820\n",
            "Epoch 373/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2820\n",
            "Epoch 374/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.2820\n",
            "Epoch 375/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2820\n",
            "Epoch 376/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2821\n",
            "Epoch 377/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2820\n",
            "Epoch 378/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2820\n",
            "Epoch 379/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2821\n",
            "Epoch 380/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2820\n",
            "Epoch 381/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2821\n",
            "Epoch 382/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2822\n",
            "Epoch 383/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2822\n",
            "Epoch 384/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2820\n",
            "Epoch 385/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2822\n",
            "Epoch 386/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2822\n",
            "Epoch 387/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2822\n",
            "Epoch 388/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2823\n",
            "Epoch 389/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1646 - val_loss: 0.2822\n",
            "Epoch 390/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2822\n",
            "Epoch 391/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2823\n",
            "Epoch 392/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2823\n",
            "Epoch 393/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2823\n",
            "Epoch 394/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2823\n",
            "Epoch 395/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2824\n",
            "Epoch 396/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2822\n",
            "Epoch 397/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2823\n",
            "Epoch 398/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2823\n",
            "Epoch 399/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2824\n",
            "Epoch 400/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1649 - val_loss: 0.2824\n",
            "Epoch 401/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2823\n",
            "Epoch 402/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2824\n",
            "Epoch 403/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2824\n",
            "Epoch 404/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2825\n",
            "Epoch 405/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2825\n",
            "Epoch 406/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2824\n",
            "Epoch 407/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2826\n",
            "Epoch 408/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2824\n",
            "Epoch 409/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2825\n",
            "Epoch 410/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2825\n",
            "Epoch 411/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2826\n",
            "Epoch 412/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2827\n",
            "Epoch 413/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2827\n",
            "Epoch 414/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2826\n",
            "Epoch 415/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2825\n",
            "Epoch 416/500\n",
            "531/531 [==============================] - 3s 6ms/step - loss: 0.1646 - val_loss: 0.2826\n",
            "Epoch 417/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2826\n",
            "Epoch 418/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1647 - val_loss: 0.2826\n",
            "Epoch 419/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2825\n",
            "Epoch 420/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2826\n",
            "Epoch 421/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2826\n",
            "Epoch 422/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2827\n",
            "Epoch 423/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2827\n",
            "Epoch 424/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2827\n",
            "Epoch 425/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1648 - val_loss: 0.2829\n",
            "Epoch 426/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2828\n",
            "Epoch 427/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2828\n",
            "Epoch 428/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2827\n",
            "Epoch 429/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2828\n",
            "Epoch 430/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2828\n",
            "Epoch 431/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.2827\n",
            "Epoch 432/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2827\n",
            "Epoch 433/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2828\n",
            "Epoch 434/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2829\n",
            "Epoch 435/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2828\n",
            "Epoch 436/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2830\n",
            "Epoch 437/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2828\n",
            "Epoch 438/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2829\n",
            "Epoch 439/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2827\n",
            "Epoch 440/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2829\n",
            "Epoch 441/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2829\n",
            "Epoch 442/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2828\n",
            "Epoch 443/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2831\n",
            "Epoch 444/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2829\n",
            "Epoch 445/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1645 - val_loss: 0.2829\n",
            "Epoch 446/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2829\n",
            "Epoch 447/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1642 - val_loss: 0.2829\n",
            "Epoch 448/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2829\n",
            "Epoch 449/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1643 - val_loss: 0.2830\n",
            "Epoch 450/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1641 - val_loss: 0.2831\n",
            "Epoch 451/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2832\n",
            "Epoch 452/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2831\n",
            "Epoch 453/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2831\n",
            "Epoch 454/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2831\n",
            "Epoch 455/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2831\n",
            "Epoch 456/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2831\n",
            "Epoch 457/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2831\n",
            "Epoch 458/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2830\n",
            "Epoch 459/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.2832\n",
            "Epoch 460/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2832\n",
            "Epoch 461/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2832\n",
            "Epoch 462/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2831\n",
            "Epoch 463/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2832\n",
            "Epoch 464/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2830\n",
            "Epoch 465/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2833\n",
            "Epoch 466/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2833\n",
            "Epoch 467/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2833\n",
            "Epoch 468/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2833\n",
            "Epoch 469/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2832\n",
            "Epoch 470/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2832\n",
            "Epoch 471/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2832\n",
            "Epoch 472/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2832\n",
            "Epoch 473/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2833\n",
            "Epoch 474/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1638 - val_loss: 0.2832\n",
            "Epoch 475/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2833\n",
            "Epoch 476/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1633 - val_loss: 0.2834\n",
            "Epoch 477/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2833\n",
            "Epoch 478/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2833\n",
            "Epoch 479/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2834\n",
            "Epoch 480/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2834\n",
            "Epoch 481/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2834\n",
            "Epoch 482/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2835\n",
            "Epoch 483/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2834\n",
            "Epoch 484/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.2834\n",
            "Epoch 485/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2835\n",
            "Epoch 486/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1639 - val_loss: 0.2834\n",
            "Epoch 487/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.2834\n",
            "Epoch 488/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2836\n",
            "Epoch 489/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1637 - val_loss: 0.2834\n",
            "Epoch 490/500\n",
            "531/531 [==============================] - 3s 5ms/step - loss: 0.1634 - val_loss: 0.2836\n",
            "Epoch 491/500\n",
            "525/531 [============================>.] - ETA: 0s - loss: 0.1631"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-070ac8787cfc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcsv_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/log_ae.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_ada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_ada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mX_val_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \"\"\"\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: /content/drive/MyDrive/MY_DATA/merged/log_ae.csv; Transport endpoint is not connected"
          ]
        }
      ],
      "source": [
        "l, cnt = [], 0\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=811)\n",
        "\n",
        "for lr in learning_rates: # one hyperparameter\n",
        "  for epoch in epochs:  # second hyperparameter\n",
        "    for batch_size in batch_sizes: # third hyperparameter\n",
        "      for train_index, val_index in kf.split(train_df.index): #validation\n",
        "        X_train = train_df.iloc[train_index, :]\n",
        "        y_tr = y_train.iloc[train_index, :]\n",
        "        X_valid = train_df.iloc[val_index, :]\n",
        "        y_valid = y_train.iloc[val_index, :]\n",
        "\n",
        "        pca = PCA(0.95).fit(X_train)\n",
        "        x_train = pca.transform(X_train)\n",
        "        x_val = pca.transform(X_valid)\n",
        "\n",
        "        mapp = LabelEncoder()\n",
        "\n",
        "        y_train_adasyn = mapp.fit_transform(y_tr[\"Label\"]).ravel()\n",
        "\n",
        "        ada = ADASYN(random_state=0, sampling_strategy='all')\n",
        "        x_train_ada, y_train_ada = ada.fit_resample(x_train, y_train_adasyn)\n",
        "        dims = [x_train_ada.shape[1], 1024, 64]\n",
        "        optimizer = SGD(learning_rate=lr, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "        model, encoder = autoencoder(dims, init=init)\n",
        "        model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "        csv_logger = CSVLogger(f'{path}/log_ae.csv', append=True, separator=';')\n",
        "\n",
        "        history = model.fit(x_train_ada, x_train_ada, batch_size=batch_size, epochs=epoch, validation_data=(x_val, x_val), callbacks=[csv_logger])\n",
        "        X_val_pred = encoder.predict(x_val, batch_size=batch_size)\n",
        "        \n",
        "        km = KMeans(n_clusters=10, random_state=42)\n",
        "        y_pred = km.fit_predict(X_val_pred)\n",
        "        ari = adjusted_rand_score(y_pred, y_valid[\"Label\"])\n",
        "\n",
        "        silhouetteScore = silhouette_score(X_val_pred, y_valid[\"Label\"], metric=\"euclidean\")\n",
        "        davies_bouldinScore = davies_bouldin_score(X_val_pred, y_valid[\"Label\"])\n",
        "\n",
        "        l.append((lr, epoch, batch_size, history.history[\"loss\"][-1], history.history[\"val_loss\"][-1], silhouetteScore, davies_bouldinScore, ari))\n",
        "        print(cnt, l[-1])\n",
        "        del model, encoder, history\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "      ae_df = pd.DataFrame.from_records(l, columns=[\"Learning rate\", \"epoch\", \"Batch size\", \"Loss\", \"Validation loss\", \"Silhouette\", \"DB score\", \"ARI\"])\n",
        "      ae_df.to_csv(f\"{path}/autoencoder_grid_search.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOtbpGbi5ZYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079d3f95-8c8d-46df-dd07-9e7dc6bb4744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(51, 8)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(f\"{path}/final_autoencoder_tunining_results_no_dropout_280323.csv\")\n",
        "\n",
        "df[\"Batch size\"] = df[\"Batch size\"].astype(str)\n",
        "df[\"Learning rate\"] = df[\"Learning rate\"].astype(str)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F3hLZxY-r5g3",
        "outputId": "cf4440aa-8d8c-4912-b628-d6c846d1d2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0 Learning rate  epoch Batch size      Loss  Validation loss  \\\n",
              "0            0           0.1    200         64  0.151553         0.344932   \n",
              "1            1           0.1    200        128  0.153167         0.341594   \n",
              "2            2           0.1    200        256  0.169428         0.336404   \n",
              "3            3           0.1    200        512  0.195097         0.337393   \n",
              "4            4           0.1    500         64  0.145487         0.353017   \n",
              "5            5           0.1    500        128  0.140490         0.353602   \n",
              "6            6           0.1    500        256  0.144590         0.347484   \n",
              "7            7           0.1    500        512  0.159682         0.338920   \n",
              "8            8           0.1   1000         64  0.142999         0.357691   \n",
              "9            9           0.1   1000        128  0.136013         0.362779   \n",
              "10          10           0.1   1000        256  0.135551         0.359841   \n",
              "11          11           0.1   1000        512  0.142078         0.350117   \n",
              "12          12           0.2    200         64  0.146355         0.351918   \n",
              "13          13           0.2    200        128  0.141596         0.351895   \n",
              "14          14           0.2    200        256  0.147021         0.344632   \n",
              "15          15           0.2    200        512  0.165640         0.336561   \n",
              "16          16           0.2    500         64  0.142368         0.358695   \n",
              "17          17           0.2    500        128  0.135025         0.363953   \n",
              "18          18           0.2    500        256  0.134193         0.362075   \n",
              "19          19           0.2    500        512  0.140083         0.350931   \n",
              "20          20           0.1    200         64  0.151553         0.344932   \n",
              "21          21           0.1    200        128  0.153167         0.341594   \n",
              "22          22           0.1    200        256  0.169428         0.336404   \n",
              "23          23           0.1    200        512  0.195097         0.337393   \n",
              "24          24           0.1    500         64  0.145487         0.353017   \n",
              "25          25           0.1    500        128  0.140490         0.353602   \n",
              "26          26           0.1    500        256  0.144590         0.347484   \n",
              "27          27           0.1    500        512  0.159682         0.338920   \n",
              "28          28           0.1   1000         64  0.142999         0.357691   \n",
              "29          29           0.1   1000        128  0.136013         0.362779   \n",
              "30          30           0.1   1000        256  0.135551         0.359841   \n",
              "31          31           0.1   1000        512  0.142078         0.350117   \n",
              "32          32           0.2    200         64  0.146355         0.351918   \n",
              "33          33           0.2    200        128  0.141596         0.351895   \n",
              "34          34           0.2    200        256  0.147021         0.344632   \n",
              "35          35           0.2    200        512  0.165640         0.336561   \n",
              "36          36           0.2    500         64  0.142368         0.358695   \n",
              "37          37           0.2    500        128  0.135025         0.363953   \n",
              "38          38           0.2    500        256  0.134193         0.362075   \n",
              "39          39           0.2    500        512  0.140083         0.350931   \n",
              "40          40           0.2    500        512  0.139880         0.350881   \n",
              "41          41           0.2   1000         64  0.140595         0.363311   \n",
              "42          42           0.2   1000        128  0.132584         0.371805   \n",
              "43          43           0.2   1000        256  0.129413         0.375034   \n",
              "44          44           0.2   1000        512  0.131053         0.367082   \n",
              "45           0           0.1    200       1024  0.215995         0.343508   \n",
              "46           1           0.1    500       1024  0.184725         0.336111   \n",
              "47           2           0.1   1000       1024  0.158255         0.339662   \n",
              "48           3           0.2    200       1024  0.193033         0.337055   \n",
              "49           4           0.2    500       1024  0.155962         0.339236   \n",
              "50           5           0.2   1000       1024  0.138306         0.352282   \n",
              "\n",
              "    Silhouette  DB score  \n",
              "0     0.029006  3.887175  \n",
              "1     0.031909  3.737434  \n",
              "2     0.030330  3.715341  \n",
              "3     0.030664  3.734398  \n",
              "4     0.027085  3.997392  \n",
              "5     0.029743  3.822829  \n",
              "6     0.032375  3.704416  \n",
              "7     0.031069  3.683062  \n",
              "8     0.026491  4.038314  \n",
              "9     0.028197  3.871561  \n",
              "10    0.031107  3.748146  \n",
              "11    0.032604  3.673862  \n",
              "12    0.024407  4.076494  \n",
              "13    0.026883  3.875106  \n",
              "14    0.030229  3.730811  \n",
              "15    0.030464  3.694858  \n",
              "16    0.024887  4.101184  \n",
              "17    0.024062  4.044115  \n",
              "18    0.026699  3.836038  \n",
              "19    0.031136  3.677031  \n",
              "20    0.029006  3.887175  \n",
              "21    0.031909  3.737434  \n",
              "22    0.030330  3.715341  \n",
              "23    0.030664  3.734398  \n",
              "24    0.027085  3.997392  \n",
              "25    0.029743  3.822829  \n",
              "26    0.032375  3.704416  \n",
              "27    0.031069  3.683062  \n",
              "28    0.026491  4.038314  \n",
              "29    0.028197  3.871561  \n",
              "30    0.031107  3.748146  \n",
              "31    0.032604  3.673862  \n",
              "32    0.024407  4.076494  \n",
              "33    0.026883  3.875106  \n",
              "34    0.030229  3.730811  \n",
              "35    0.030464  3.694858  \n",
              "36    0.024887  4.101184  \n",
              "37    0.024062  4.044115  \n",
              "38    0.026699  3.836038  \n",
              "39    0.031136  3.677031  \n",
              "40    0.031309  3.678975  \n",
              "41    0.026240  4.079932  \n",
              "42    0.023239  4.102121  \n",
              "43    0.024766  3.972408  \n",
              "44    0.028776  3.759147  \n",
              "45    0.031881  3.822553  \n",
              "46    0.030019  3.709065  \n",
              "47    0.031145  3.677402  \n",
              "48    0.030247  3.735013  \n",
              "49    0.031782  3.662619  \n",
              "50    0.032255  3.636954  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a532a6a-2891-4a5b-bfd6-3bd091d1b474\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Learning rate</th>\n",
              "      <th>epoch</th>\n",
              "      <th>Batch size</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Validation loss</th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>DB score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>64</td>\n",
              "      <td>0.151553</td>\n",
              "      <td>0.344932</td>\n",
              "      <td>0.029006</td>\n",
              "      <td>3.887175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>128</td>\n",
              "      <td>0.153167</td>\n",
              "      <td>0.341594</td>\n",
              "      <td>0.031909</td>\n",
              "      <td>3.737434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>256</td>\n",
              "      <td>0.169428</td>\n",
              "      <td>0.336404</td>\n",
              "      <td>0.030330</td>\n",
              "      <td>3.715341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>512</td>\n",
              "      <td>0.195097</td>\n",
              "      <td>0.337393</td>\n",
              "      <td>0.030664</td>\n",
              "      <td>3.734398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>64</td>\n",
              "      <td>0.145487</td>\n",
              "      <td>0.353017</td>\n",
              "      <td>0.027085</td>\n",
              "      <td>3.997392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>128</td>\n",
              "      <td>0.140490</td>\n",
              "      <td>0.353602</td>\n",
              "      <td>0.029743</td>\n",
              "      <td>3.822829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>256</td>\n",
              "      <td>0.144590</td>\n",
              "      <td>0.347484</td>\n",
              "      <td>0.032375</td>\n",
              "      <td>3.704416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>512</td>\n",
              "      <td>0.159682</td>\n",
              "      <td>0.338920</td>\n",
              "      <td>0.031069</td>\n",
              "      <td>3.683062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>64</td>\n",
              "      <td>0.142999</td>\n",
              "      <td>0.357691</td>\n",
              "      <td>0.026491</td>\n",
              "      <td>4.038314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>128</td>\n",
              "      <td>0.136013</td>\n",
              "      <td>0.362779</td>\n",
              "      <td>0.028197</td>\n",
              "      <td>3.871561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>256</td>\n",
              "      <td>0.135551</td>\n",
              "      <td>0.359841</td>\n",
              "      <td>0.031107</td>\n",
              "      <td>3.748146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>512</td>\n",
              "      <td>0.142078</td>\n",
              "      <td>0.350117</td>\n",
              "      <td>0.032604</td>\n",
              "      <td>3.673862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>64</td>\n",
              "      <td>0.146355</td>\n",
              "      <td>0.351918</td>\n",
              "      <td>0.024407</td>\n",
              "      <td>4.076494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>128</td>\n",
              "      <td>0.141596</td>\n",
              "      <td>0.351895</td>\n",
              "      <td>0.026883</td>\n",
              "      <td>3.875106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>256</td>\n",
              "      <td>0.147021</td>\n",
              "      <td>0.344632</td>\n",
              "      <td>0.030229</td>\n",
              "      <td>3.730811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>512</td>\n",
              "      <td>0.165640</td>\n",
              "      <td>0.336561</td>\n",
              "      <td>0.030464</td>\n",
              "      <td>3.694858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>64</td>\n",
              "      <td>0.142368</td>\n",
              "      <td>0.358695</td>\n",
              "      <td>0.024887</td>\n",
              "      <td>4.101184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>128</td>\n",
              "      <td>0.135025</td>\n",
              "      <td>0.363953</td>\n",
              "      <td>0.024062</td>\n",
              "      <td>4.044115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>256</td>\n",
              "      <td>0.134193</td>\n",
              "      <td>0.362075</td>\n",
              "      <td>0.026699</td>\n",
              "      <td>3.836038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>512</td>\n",
              "      <td>0.140083</td>\n",
              "      <td>0.350931</td>\n",
              "      <td>0.031136</td>\n",
              "      <td>3.677031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>64</td>\n",
              "      <td>0.151553</td>\n",
              "      <td>0.344932</td>\n",
              "      <td>0.029006</td>\n",
              "      <td>3.887175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>128</td>\n",
              "      <td>0.153167</td>\n",
              "      <td>0.341594</td>\n",
              "      <td>0.031909</td>\n",
              "      <td>3.737434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>256</td>\n",
              "      <td>0.169428</td>\n",
              "      <td>0.336404</td>\n",
              "      <td>0.030330</td>\n",
              "      <td>3.715341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>512</td>\n",
              "      <td>0.195097</td>\n",
              "      <td>0.337393</td>\n",
              "      <td>0.030664</td>\n",
              "      <td>3.734398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>64</td>\n",
              "      <td>0.145487</td>\n",
              "      <td>0.353017</td>\n",
              "      <td>0.027085</td>\n",
              "      <td>3.997392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>128</td>\n",
              "      <td>0.140490</td>\n",
              "      <td>0.353602</td>\n",
              "      <td>0.029743</td>\n",
              "      <td>3.822829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>256</td>\n",
              "      <td>0.144590</td>\n",
              "      <td>0.347484</td>\n",
              "      <td>0.032375</td>\n",
              "      <td>3.704416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>512</td>\n",
              "      <td>0.159682</td>\n",
              "      <td>0.338920</td>\n",
              "      <td>0.031069</td>\n",
              "      <td>3.683062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>64</td>\n",
              "      <td>0.142999</td>\n",
              "      <td>0.357691</td>\n",
              "      <td>0.026491</td>\n",
              "      <td>4.038314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>128</td>\n",
              "      <td>0.136013</td>\n",
              "      <td>0.362779</td>\n",
              "      <td>0.028197</td>\n",
              "      <td>3.871561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>256</td>\n",
              "      <td>0.135551</td>\n",
              "      <td>0.359841</td>\n",
              "      <td>0.031107</td>\n",
              "      <td>3.748146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>512</td>\n",
              "      <td>0.142078</td>\n",
              "      <td>0.350117</td>\n",
              "      <td>0.032604</td>\n",
              "      <td>3.673862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>64</td>\n",
              "      <td>0.146355</td>\n",
              "      <td>0.351918</td>\n",
              "      <td>0.024407</td>\n",
              "      <td>4.076494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>128</td>\n",
              "      <td>0.141596</td>\n",
              "      <td>0.351895</td>\n",
              "      <td>0.026883</td>\n",
              "      <td>3.875106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>256</td>\n",
              "      <td>0.147021</td>\n",
              "      <td>0.344632</td>\n",
              "      <td>0.030229</td>\n",
              "      <td>3.730811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>512</td>\n",
              "      <td>0.165640</td>\n",
              "      <td>0.336561</td>\n",
              "      <td>0.030464</td>\n",
              "      <td>3.694858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>64</td>\n",
              "      <td>0.142368</td>\n",
              "      <td>0.358695</td>\n",
              "      <td>0.024887</td>\n",
              "      <td>4.101184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>128</td>\n",
              "      <td>0.135025</td>\n",
              "      <td>0.363953</td>\n",
              "      <td>0.024062</td>\n",
              "      <td>4.044115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>256</td>\n",
              "      <td>0.134193</td>\n",
              "      <td>0.362075</td>\n",
              "      <td>0.026699</td>\n",
              "      <td>3.836038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>512</td>\n",
              "      <td>0.140083</td>\n",
              "      <td>0.350931</td>\n",
              "      <td>0.031136</td>\n",
              "      <td>3.677031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>40</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>512</td>\n",
              "      <td>0.139880</td>\n",
              "      <td>0.350881</td>\n",
              "      <td>0.031309</td>\n",
              "      <td>3.678975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1000</td>\n",
              "      <td>64</td>\n",
              "      <td>0.140595</td>\n",
              "      <td>0.363311</td>\n",
              "      <td>0.026240</td>\n",
              "      <td>4.079932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1000</td>\n",
              "      <td>128</td>\n",
              "      <td>0.132584</td>\n",
              "      <td>0.371805</td>\n",
              "      <td>0.023239</td>\n",
              "      <td>4.102121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1000</td>\n",
              "      <td>256</td>\n",
              "      <td>0.129413</td>\n",
              "      <td>0.375034</td>\n",
              "      <td>0.024766</td>\n",
              "      <td>3.972408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>44</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1000</td>\n",
              "      <td>512</td>\n",
              "      <td>0.131053</td>\n",
              "      <td>0.367082</td>\n",
              "      <td>0.028776</td>\n",
              "      <td>3.759147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>200</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.215995</td>\n",
              "      <td>0.343508</td>\n",
              "      <td>0.031881</td>\n",
              "      <td>3.822553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>500</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.184725</td>\n",
              "      <td>0.336111</td>\n",
              "      <td>0.030019</td>\n",
              "      <td>3.709065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.158255</td>\n",
              "      <td>0.339662</td>\n",
              "      <td>0.031145</td>\n",
              "      <td>3.677402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>200</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.193033</td>\n",
              "      <td>0.337055</td>\n",
              "      <td>0.030247</td>\n",
              "      <td>3.735013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.155962</td>\n",
              "      <td>0.339236</td>\n",
              "      <td>0.031782</td>\n",
              "      <td>3.662619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1000</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.138306</td>\n",
              "      <td>0.352282</td>\n",
              "      <td>0.032255</td>\n",
              "      <td>3.636954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a532a6a-2891-4a5b-bfd6-3bd091d1b474')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a532a6a-2891-4a5b-bfd6-3bd091d1b474 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a532a6a-2891-4a5b-bfd6-3bd091d1b474');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selection of best hyperparameters"
      ],
      "metadata": {
        "id": "JX4uhCeSXiD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below I visualized all hyperparameters in a graf that helped to identify the best results."
      ],
      "metadata": {
        "id": "bav_AlImYUZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnGPujrg90bU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "dc4dffc6-cac4-4eae-ac92-24452b3a65c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['128', '512', '256', '64', '1024']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "ax = sns.scatterplot(df, y=\"DB score\", x = \"Batch size\", hue=\"Learning rate\", style=\"epoch\", palette=\"Set2\", s=80)\n",
        "plt.legend(title=r\"$\\it{Hyperparameters}$\", fontsize=16)\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "labels = list(set(df[\"Batch size\"].tolist()))\n",
        "print(labels)\n",
        "ax.set_xticks(labels)\n",
        "ax.set_xlabel(\"Batch size\", fontsize=14)\n",
        "ax.set_ylabel(\"Davies-Bouldin score [-]\", fontsize=14)\n",
        "ax.set_xticklabels(labels, size = 14)\n",
        "ax.yaxis.set_tick_params(labelsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.scatterplot(df, y=\"Silhouette\", x = \"Batch size\", hue=\"Learning rate\", style=\"epoch\", palette=\"Set2\", s=80)\n",
        "plt.legend(title=r\"$\\it{Hyperparameters}$\", fontsize=16)\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "labels = list(set(df[\"Batch size\"].tolist()))\n",
        "ax.set_xticks(labels)\n",
        "ax.set_xlabel(\"Batch size\", fontsize=14)\n",
        "ax.set_ylabel(\"Silhuette score [-]\", fontsize=14)\n",
        "ax.set_xticklabels(labels, size = 14)\n",
        "ax.yaxis.set_tick_params(labelsize=14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "TyGzyMxLjopf",
        "outputId": "3d5f3209-4d03-49f7-aa11-be395541bdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(\"DB score\").head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FWsWzPNVkfCi",
        "outputId": "5e26e08e-f762-4fde-9844-20970728c13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Learning rate  epoch Batch size      Loss  Validation loss  Silhouette  \\\n",
              "5            0.2   1000       1024  0.138306         0.352282    0.032255   \n",
              "4            0.2    500       1024  0.155962         0.339236    0.031782   \n",
              "31           0.1   1000        512  0.142078         0.350117    0.032604   \n",
              "11           0.1   1000        512  0.142078         0.350117    0.032604   \n",
              "19           0.2    500        512  0.140083         0.350931    0.031136   \n",
              "\n",
              "    DB score  \n",
              "5   3.636954  \n",
              "4   3.662619  \n",
              "31  3.673862  \n",
              "11  3.673862  \n",
              "19  3.677031  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb3b9a33-3b8e-4b6b-bd25-de65ef297d42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learning rate</th>\n",
              "      <th>epoch</th>\n",
              "      <th>Batch size</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Validation loss</th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>DB score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.2</td>\n",
              "      <td>1000</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.138306</td>\n",
              "      <td>0.352282</td>\n",
              "      <td>0.032255</td>\n",
              "      <td>3.636954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.155962</td>\n",
              "      <td>0.339236</td>\n",
              "      <td>0.031782</td>\n",
              "      <td>3.662619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>512</td>\n",
              "      <td>0.142078</td>\n",
              "      <td>0.350117</td>\n",
              "      <td>0.032604</td>\n",
              "      <td>3.673862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.1</td>\n",
              "      <td>1000</td>\n",
              "      <td>512</td>\n",
              "      <td>0.142078</td>\n",
              "      <td>0.350117</td>\n",
              "      <td>0.032604</td>\n",
              "      <td>3.673862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.2</td>\n",
              "      <td>500</td>\n",
              "      <td>512</td>\n",
              "      <td>0.140083</td>\n",
              "      <td>0.350931</td>\n",
              "      <td>0.031136</td>\n",
              "      <td>3.677031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb3b9a33-3b8e-4b6b-bd25-de65ef297d42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb3b9a33-3b8e-4b6b-bd25-de65ef297d42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb3b9a33-3b8e-4b6b-bd25-de65ef297d42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(\"DB score\").head().to_csv(f\"{path}/temp.csv\", index=False)"
      ],
      "metadata": {
        "id": "yTmHLA53UXBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(df, y=\"Silhouette\", x = \"Validation loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "icCNhw7uSrAS",
        "outputId": "e2d40831-6af9-4c44-9aee-e5c125406635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Validation loss', ylabel='Silhouette'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(df, y=\"DB score\", x = \"Validation loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "_UpZ5qWzSrbg",
        "outputId": "bdbaf902-718e-4ca7-8756-85604f67e6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Validation loss', ylabel='DB score'>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
